<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Jesse&#39;s Blog</title>
  
  <subtitle>直落夜深花睡去，临风春华便思君。</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://jesse-csj.github.io/"/>
  <updated>2020-10-19T08:57:26.716Z</updated>
  <id>https://jesse-csj.github.io/</id>
  
  <author>
    <name>Jesse_jia</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>（读论文）推荐系统之ctr预估-GateNet模型解析</title>
    <link href="https://jesse-csj.github.io/2020/10/19/GateNet/"/>
    <id>https://jesse-csj.github.io/2020/10/19/GateNet/</id>
    <published>2020-10-19T02:03:03.000Z</published>
    <updated>2020-10-19T08:57:26.716Z</updated>
    
    <content type="html"><![CDATA[<p>由于秋招原因许久未更，现开始逐步看今天最近的论文。 今天分享的是微博＆腾讯关于<strong>CTR预估</strong>的一篇文章：GateNet:Gating-Enhanced Deep Network for Click-Through Rate Prediction.</p><p><strong>要点</strong>：在点击率预测中，当前的深度学习模型中基本都包括embedding layer和MLP hidden layers。另一方面，<strong>门控机制（gating mechanism）</strong>也广泛应用于许多研究领域，如计算机视觉和自然语言处理。目前已有一些研究证明了门控机制提高了非凸深度神经网络的可训练性，于是本文将门控机制与深度CTR模型相结合，并通过实验证明融合模型的性能取得了较大的提升。下面一起来看下细节。</p><p>原文：《<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2007.03519.pdf" target="_blank" rel="noopener">GateNet:Gating-Enhanced Deep Network for Click-Through Rate Prediction</a>》</p><a id="more"></a><p><br></p><h1 id="1、-引入"><a href="#1、-引入" class="headerlink" title="1、 引入"></a>1、 引入</h1><p><br><br>首先是常见的CTR和深度CTR模型，如FM,DeepFM,Wide＆Deep,DCN,xDeepFM等，之前都已经介绍过，还不了解的同学可以查看之前的文章，这里就不详细介绍了。接下来简单介绍下门机制。</p><h2 id="门机制（gating-mechanism）"><a href="#门机制（gating-mechanism）" class="headerlink" title="门机制（gating mechanism）"></a><strong>门机制（gating mechanism）</strong></h2><p>目前，门机制广泛用于CV和NLP，CV如Highway Network，它们利用转换门和进位门来分别表示通过转换输入和进位输出产生了多少输出。在NLP中，如LSTM，GRU，语言建模，序列对序列学习，他们利用门来防止梯度消失和解决长期依赖问题。</p><p>此外，在推荐系统中使用门来自动调整建模共享信息和建模任务特定信息之间的参数。应用门机制的另一个推荐系统是hierarchical gating network(HGN)，它应用特征级和实例级的门模块来自适应地控制哪些item潜在特征和哪些相关item可以被传递到下游层。具体的细节大家可以参阅上面提到这些的原论文。简单来理解，<strong>门控机制即相当于一个调节阀，可以控制流入的信息流量的流入程度。</strong></p><p>由于目前推荐系统中常用的深度学习模型基本都包括embedding layer和MLP hidden layers，本文就在此基础上将门机制和这两种layer相结合，产生<strong>Feature Embedding Gate和Hidden Gate</strong>，接下来对二者分别介绍模型细节。</p><p><br></p><h1 id="2、-模型"><a href="#2、-模型" class="headerlink" title="2、 模型"></a>2、 模型</h1><h2 id="2-1-Feature-Embedding-Gate"><a href="#2-1-Feature-Embedding-Gate" class="headerlink" title="2.1 Feature Embedding Gate"></a>2.1 Feature Embedding Gate</h2><p>Feature Embedding Gate主要是在嵌入层环节增加了门控机制，用于从特征中选择更为重要的特征（<strong>信息选择</strong>）。如果模型中带有Feature Embedding Gate，其网络结构如下图所示：</p><p><img src="https://pic3.zhimg.com/80/v2-37d58de7637338d39fea80aae24f2726_720w.jpg" alt></p><p>假设输入的离散特征经过Embedding layer得到E=[e1,e2,e3,…,ei,…ef]，其中f代表特征域的个数，ei 代表第i个域的embedding向量，长度为K。</p><p>接下来，E会通过Feature Embedding Gate进行转换。首先，对每一个embedding向量，通过下面的公式来计算门值 gi ，代表该嵌入向量的特征重要程度：</p><p><img src="https://pic1.zhimg.com/80/v2-6abb7cc1ae5276a09efbde60822facec_720w.jpg" alt></p><p>然后，将嵌入向量ei和门值 gi 计算哈达玛积，得到gei ，并得到最终的输出GE,如下公式：</p><p><img src="https://pic4.zhimg.com/80/v2-d9fba15a1f272a61da00759a4a34cfef_720w.jpg" alt></p><p><img src="https://pic3.zhimg.com/80/v2-9218f6574ad679c6292a9e44cda2ec92_720w.png" alt></p><p>上面只是对Feature Embedding Gate的一个简要的介绍，其具体的做法包括多种（后面会有实验对比效果），比如输出的 gi 是一个跟 ei 同样长度的向量，那么此时我们称门为bit-wise gate，如果输出的gi 是一个值，那么此时称门为vector-wise gate。分别如下：</p><p><img src="https://pic1.zhimg.com/80/v2-241be08455d687142b00dd3afd57d968_720w.jpg" alt></p><p>另一种就是是否所有的域都用同一个参数矩阵W，如果每个域的参数矩阵都不相同，那么被称之为field private，如果所有域的参数矩阵相同，则称之为field sharing。</p><p>后续会用实验对比这四种做法的效果。（<strong>插一句</strong>，有没有觉得和之前一篇新浪的FiBiNet的行文很像，特别是这个参数矩阵相同和不同的这个实验点完全一样，我又去翻了下，果然作者都有张俊林老师，哈哈~）</p><h2 id="2-2-Hidden-Gate"><a href="#2-2-Hidden-Gate" class="headerlink" title="2.2 Hidden Gate"></a>2.2 Hidden Gate</h2><p>同理，Hidden Gate就是在<strong>多层感知机环节</strong>增加了门控机制，用于选择更加重要的特征交互传递到更深层的网络。如果模型中带有Hidden Gate，其网络结构如下图所示：</p><p><img src="https://pic1.zhimg.com/80/v2-8449ba534a4179b1d328737944b493d0_720w.jpg" alt></p><p>假设 <strong>a⁽ˡ⁾</strong> 是第l层隐藏层的输出：</p><p><img src="https://pic4.zhimg.com/80/v2-0882a51f179c0268da020084841f6e03_720w.png" alt></p><p>接下来将 <strong>a⁽ˡ⁾</strong> 输入到hidden gate中，计算方式如下：</p><p><img src="https://pic3.zhimg.com/80/v2-f01171a7ecd0e264bf29eb3b51237836_720w.jpg" alt></p><p>损失函数采取交叉熵损失函数。其余部分均与普通DNN相同。<br><br></p><h1 id="3、-实验"><a href="#3、-实验" class="headerlink" title="3、 实验"></a>3、 实验</h1><p><br><br>首先来看下在网络中单独加入Feature Embedding Gate的效果：</p><p><img src="https://pic1.zhimg.com/80/v2-d426ab3372713cfe6b63264307837ea0_720w.jpg" alt></p><p>可以看到加入Feature Embedding Gate后，在多个模型以及多个数据集中都取得了更好的AUC。</p><p>接下来对比前文提到的四种不同做法，field private和field sharing，以及bit-wise gate和vector-wise gate哪种效果更好。实验结果如下：</p><p><img src="https://pic4.zhimg.com/80/v2-9729c4da3a49216d8581badaa068a57b_720w.jpg" alt></p><p><img src="https://pic2.zhimg.com/80/v2-32f0659ac628793c137b01fc74259dd9_720w.jpg" alt></p><p>可以看到，field private的结果是明显好于field sharing的，但在不同的数据集中，bit-wise gate和vector-wise gate表现有所差异。</p><p>然后是在MLP中加入Hidden Gate的效果，加入Hidden Gate后性能也有一定的提升：</p><p><img src="https://pic2.zhimg.com/80/v2-9de1ad9f8e76c820ac3c666629e8ffed_720w.jpg" alt></p><p>作者最后对两种gate进行结合，效果如下：</p><p><img src="https://pic2.zhimg.com/80/v2-fefe8cadc97074830c11178867bc7ef5_720w.jpg" alt></p><p>实验发现<strong>同时加入两种gate的效果其实还不如单独一种gate好</strong>，具体的原因还需要进一步验证，本文暂未提及。</p><p>本文介绍就到此结束，总的来说感觉像是使用了门控机制的两层attention来结合点击率预测，不是很有新意。在本文中门控机制介绍的较为简略，感兴趣的同学可以看一波LSTM之类的介绍。另外，最近不忙开始逐步恢复更新今年较新的论文啦~</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;由于秋招原因许久未更，现开始逐步看今天最近的论文。 今天分享的是微博＆腾讯关于&lt;strong&gt;CTR预估&lt;/strong&gt;的一篇文章：GateNet:Gating-Enhanced Deep Network for Click-Through Rate Prediction.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;要点&lt;/strong&gt;：在点击率预测中，当前的深度学习模型中基本都包括embedding layer和MLP hidden layers。另一方面，&lt;strong&gt;门控机制（gating mechanism）&lt;/strong&gt;也广泛应用于许多研究领域，如计算机视觉和自然语言处理。目前已有一些研究证明了门控机制提高了非凸深度神经网络的可训练性，于是本文将门控机制与深度CTR模型相结合，并通过实验证明融合模型的性能取得了较大的提升。下面一起来看下细节。&lt;/p&gt;
&lt;p&gt;原文：《&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2007.03519.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GateNet:Gating-Enhanced Deep Network for Click-Through Rate Prediction&lt;/a&gt;》&lt;/p&gt;
    
    </summary>
    
      <category term="ctr" scheme="https://jesse-csj.github.io/categories/ctr/"/>
    
    
      <category term="推荐系统" scheme="https://jesse-csj.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
      <category term="DNN" scheme="https://jesse-csj.github.io/tags/DNN/"/>
    
      <category term="GateNet" scheme="https://jesse-csj.github.io/tags/GateNet/"/>
    
  </entry>
  
  <entry>
    <title>（读论文）推荐系统之ctr预估-DIN模型解析</title>
    <link href="https://jesse-csj.github.io/2019/10/12/DIN/"/>
    <id>https://jesse-csj.github.io/2019/10/12/DIN/</id>
    <published>2019-10-12T08:03:03.000Z</published>
    <updated>2020-10-19T09:05:09.198Z</updated>
    
    <content type="html"><![CDATA[<p>本人才疏学浅，不足之处欢迎大家指出和交流。</p><p>今天要分享的是阿里针对<strong>电子商务领域</strong>的CTR预估的Deep Interest Network理论，由盖坤大神团队提出于2017年。该模型要点为：结合<strong>Attention机制</strong>，<strong>充分利用/挖掘用户历史行为数据中的信息，</strong>下面一起来看下。</p><p>原文：《Deep Interest Network for Click-Through Rate Prediction》<br><a id="more"></a><br>地址： <a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1706.06978.pdf" target="_blank" rel="noopener">https://www.ijcai.org/proceedings/2017/0435.pdf</a></p><p><br></p><h1 id="1、-背景"><a href="#1、-背景" class="headerlink" title="1、 背景"></a>1、 背景</h1><p><br><br>自Youtube 提出来基于 embedding 和 MLP 的基础模型后，近年来深度学习在CTR预估领域已经有了广泛的应用，常见的算法有如如Wide&amp;Deep，DeepFM等。在这些方法中，大规模稀疏输入首先将特征映射到某固定维度的嵌入向量，然后通过多个MLP，以了解特征之间的非线性关系。这样，无论候选广告是什么，用户特征都被压缩为固定长度的表示向量。</p><p>这种方法的优点在于：<strong>通过神经网络可以拟合高阶的非线性关系，同时减少了人工特征的工作量。</strong></p><p>但是，使用固定长度的矢量将成为瓶颈，这给嵌入和MLP方法从丰富的历史行为中有效捕捉用户的不同兴趣带来了困难。</p><p>于是文中提出了DIN网络来解决这个问题，该网络设计了一个<strong>局部激活单元</strong>来自适应地从和某个广告相关的历史行为中学习用户兴趣。这个表示向量会因广告而异，极大地提高了模型的表达能力。<br>提前来看下DIN具体的解决方案总结：</p><ol><li><strong>使用_用户兴趣分布_来表示用户多种多样的兴趣爱好</strong></li><li><strong>使用_Attention机制_来实现Local Activation</strong></li><li><strong>针对模型训练，提出了_Dice激活函数，自适应正则_，显著提升了模型性能与收敛速度</strong></li></ol><h2 id="1-1-数据特性"><a href="#1-1-数据特性" class="headerlink" title="1.1 数据特性"></a>1.1 数据特性</h2><p>在丰富的线上电商数据中，研究者们通过观察发现了用户行为数据中有两个很重要的特性：</p><p><strong>Diversity：</strong><br>用户在访问电商网站时会对多种商品都感兴趣。也就是用户的兴趣非常的广泛。</p><p><strong>Local Activation：</strong><br>用户是否会点击推荐给他的商品，仅仅取决于历史行为数据中的一小部分，而不是全部。</p><p>举个栗子：</p><p><img src="https://pic2.zhimg.com/80/v2-7ff1103b4871d07ad984d628b0d9b195_720w.jpg" alt></p><p><strong>Diversity</strong>：一个年轻的母亲，从她的历史行为中，我们可以看到她的兴趣非常广泛：羊毛衫、手提袋、耳环、童装、运动装等等。</p><p><strong>Local Activation</strong>：一个爱游泳的人，他之前购买过travel book、ice cream、potato chips、swimming cap。当前给他推荐的商品(或者说是广告)是goggle。那么他是否会点击这次广告，跟他之前是否购买过薯片、书籍、冰激凌没有任何相关性，而是与他之前购买过游泳帽有关。也就是说在这一次CTR预估中，部分历史数据(swimming cap)起了决定作用，而其他的基本都没起作用。</p><p>针对上面提到的用户行为中存在的两种特性，阿里将其运用于自身的推荐系统中，推出了深度兴趣网络DIN，接下来，我们就一起来看一下推荐系统和模型的一些实现细节。<br><br></p><h1 id="2、系统设计"><a href="#2、系统设计" class="headerlink" title="2、系统设计"></a>2、系统设计</h1><p><br><br><img src="https://pic4.zhimg.com/80/v2-79b3bedfc3564e179eda19a36f8ad39b_720w.jpg" alt></p><p>阿里的推荐系统工作流程就如上图所示，可以简单描述为：</p><ol><li>检查用户历史行为数据。</li><li>使用matching module产生候选ads。</li><li>通过ranking module得到候选ads的点击概率，并根据概率排序得到推荐列表。</li><li>记录下用户在当前展示广告下的反应(点击与否)。</li></ol><p>这是一个闭环的系统，对于用户行为数据(User Behavior Data)，系统自己生产并消费。</p><h2 id="2-1、特征设计"><a href="#2-1、特征设计" class="headerlink" title="2.1、特征设计"></a><strong>2.1、特征设计</strong></h2><p> 论文中将所涉及到的特征分为四个部分：<strong>用户特征、用户行为特征、广告特征、上下文特征</strong>，并<strong>没有进行特征组合/交叉特征</strong>。而是通过DNN去学习特征间的交互信息。具体如下：</p><p><img src="https://pic3.zhimg.com/80/v2-bff54141fb596c7e59032ce3b5c00e22_720w.jpg" alt></p><p>其中，只有用户行为特征是multi-hot的，即多值离散特征，原因就是一个用户会购买多个good_id,也会访问多个shop_id。针对这种特征，由于每个涉及到的非0值个数是不一样的，常见的做法就是将id转换成embedding之后，加一层pooling层，比如average-pooling，sum-pooling，max-pooling。DIN中使用的是weighted-sum，其实就是加权的sum-pooling，权重经过一个activation unit计算得到。即<code>Embedding -&gt; Pooling</code> + <code>Attention</code>，后面还会具体介绍。</p><h2 id="2-2、评价指标"><a href="#2-2、评价指标" class="headerlink" title="2.2、评价指标"></a>2.2、评价指标</h2><h3 id="模型使用的评价指标是GAUC，我们先来看一下GAUC的计算公式："><a href="#模型使用的评价指标是GAUC，我们先来看一下GAUC的计算公式：" class="headerlink" title="模型使用的评价指标是GAUC，我们先来看一下GAUC的计算公式："></a>模型使用的评价指标是GAUC，我们先来看一下GAUC的计算公式：</h3><p><img src="https://pic3.zhimg.com/80/v2-a39c279a6ecb4f91ad976a04f09cbc4e_720w.png" alt></p><p>AUC表示正样本得分比负样本得分高的概率。在CTR实际应用场景中，CTR预测常被用于对每个用户候选广告的排序。但是不同用户之间存在差异：有些用户本身就是点击率较高。以往的评价指标对样本不区分用户地进行AUC的计算。论文采用的GAUC实现了用户级别的AUC计算，<strong>在单个用户AUC的基础上，按照点击次数或展示次数进行加权平均，消除了用户偏差对模型的影响</strong>，更准确的描述了模型的表现效果，并且实践证明了GAUC相比于AUC更加稳定、可靠。<br><br></p><h1 id="3、DIN"><a href="#3、DIN" class="headerlink" title="3、DIN"></a>3、DIN</h1><h2 id="3-1、Base-Model"><a href="#3-1、Base-Model" class="headerlink" title="3.1、Base Model"></a>3.1、Base Model</h2><p>在介绍DIN之前，我们先来看一下一个base，结构如下：</p><p><img src="https://pic2.zhimg.com/80/v2-974c499958fd7e8691b42be0aef0c5dd_720w.png" alt></p><p><img src="https://pic3.zhimg.com/80/v2-dd4be3d1ad1e63a17e6567f45b78d0c2_720w.jpg" alt></p><p>可以看到，Base Model首先吧one-hot或multi-hot特征转换为特定长度的embedding，作为模型的输入，然后经过一个DNN的part，得到最终的预估值。特别地，针对multi-hot的特征，做了一次element-wise+的操作，这里其实就是sum-pooling，这样，不管特征中有多少个非0值，经过转换之后的长度都是一样的。</p><p><strong>这里对multi-hot多值离散特征进行Pooling操作，就是对Diversity的建模。Base Model中还没有引入Attention机制。</strong></p><h2 id="3-2、Deep-Interest-Network"><a href="#3-2、Deep-Interest-Network" class="headerlink" title="3.2、Deep Interest Network"></a>3.2、<strong>Deep Interest Network</strong></h2><p>仔细研究我们可以发现，Base Model有一个很大的问题，Pooling操作损失了很多信息，它对用户的历史行为是同等对待的，没有做任何处理，这显然是不合理的。一个很显然的例子，离现在越近的行为，越能反映你当前的兴趣。因此，对用户历史行为基于Attention机制进行一个加权，阿里提出了深度兴趣网络（Deep Interest Network)，先来看一下模型结构：</p><p><img src="https://pic2.zhimg.com/80/v2-dff7b3f35e4d6dec8d994436c2b0f719_720w.jpg" alt></p><p>首先给出结论，在这个模型中：</p><ol><li>Activation Unit实现Attention机制，对Local Activation建模</li><li>Pooling(weighted sum)对Diversity建模</li></ol><p>第二点比较好理解，接下来重点来理解下第一点。</p><p><strong>Attention</strong>：基于用户历史行为，充分挖掘用户兴趣和候选广告之间的关系。用户是否点击某个广告往往是基于他之前的部分兴趣，这是应用Attention机制的基础。Attention机制简单的理解就是对于不同的特征有不同的权重，这样某些特征就会主导这一次的预测，就好像模型对某些特征pay attention。但是，<strong>DIN中并不能直接用attention机制</strong>。因为<strong>对于不同的候选广告，用户兴趣表示(embedding vector)应该是不同的</strong>。也就是说用户针对不同的广告表现出不同的兴趣表示，即使是历史兴趣行为相同，但是各个行为的权重不同。这就是DIN针对Local Activation建模的含义。</p><p>attention在这里的实际意义是：每个Ad，都有一堆历史行为对其产生attention的影响分，这些attention分值与历史行为共同对这个Ad下的用户表示做加权。（可以将行为结构信息保留下来，并且提供了注意焦点）如下：</p><p>用户在某个广告下的Embedding表示Vu(Va)：</p><p><img src="https://pic4.zhimg.com/80/v2-a80dff84ded36a0d9290b49cabf2a85b_720w.jpg" alt></p><p>其中，Vu是所有behavior ids的加权和，表示的是用户兴趣；Va是候选广告的嵌入向量；<strong>wᵢ</strong>是候选广告影响着每个behavior id的权重，也就是Local Activation。<strong>wᵢ</strong>通过Activation Unit计算得出，这一块用函数去拟合，表示为g(Vi,Va)。</p><h2 id="3-3、-Dice激活函数"><a href="#3-3、-Dice激活函数" class="headerlink" title="3.3、 Dice激活函数"></a>3.3、 Dice激活函数</h2><h3 id="从Relu到PRelu："><a href="#从Relu到PRelu：" class="headerlink" title="从Relu到PRelu："></a><strong>从Relu到PRelu：</strong></h3><p>PReLU其实是ReLU的改良版，ReLU可以看作是<code>x*Max(x,0)</code>，相当于输出x经过了一个在0点的阶跃整流器。由于ReLU在x小于0的时候，梯度为0，可能导致网络停止更新，PReLU对整流器的左半部分形式进行了修改，使得x小于0时输出不为0。</p><p>研究表明，PReLU能提高准确率但是也稍微增加了过拟合的风险。PReLU形式如下：</p><p><img src="https://pic3.zhimg.com/80/v2-c3d6377a3fd62a5f650640301663444e_720w.jpg" alt></p><p>无论是ReLU还是PReLU突变点都在0，论文里认为，对于所有输入不应该都选择0点为突变点而是应该依赖于数据的。于是提出了一种<strong>data dependent</strong>的方法：<strong>Dice激活函数</strong>。形式如下：</p><p><img src="https://pic2.zhimg.com/80/v2-7c883ed0ce7130c631a9723f71e2b929_720w.jpg" alt></p><p>可以看出，pi是一个概率值，这个概率值决定着输出是取yi或者是alpha_i * yi，pi也起到了一个整流器的作用。<br>pi的计算分为两步：</p><ol><li>首先，对x进行均值归一化处理，这使得整流点是在数据的均值处，实现了data dependent的想法；</li><li>其次，经过一个sigmoid函数的计算，得到了一个0到1的概率值。巧合的是最近google提出的Swish函数形式为<code>x*sigmoid(x)</code> 在多个实验上证明了比ReLU函数<code>x*Max(x,0)</code>表现更优。</li></ol><p>另外，期望和方差使用每次训练的mini batch data直接计算，并类似于Momentum使用了_指数加权平均_：</p><p><img src="https://pic1.zhimg.com/80/v2-7367f3537b83c3f941669b58343b7ac4_720w.jpg" alt></p><p>alpha是一个超参数，推荐值为0.99。</p><h2 id="3-4、-自适应正则-Adaptive-Regularization"><a href="#3-4、-自适应正则-Adaptive-Regularization" class="headerlink" title="3.4、 自适应正则 Adaptive Regularization"></a>3.4、 自适应正则 Adaptive Regularization</h2><p>在深度ctr模型中，输入非常稀疏且维度高，模型稍微复杂导致参数非常多，通常的做法是加入L1、L2、Dropout等防止过拟合，论文中尝试后效果都不是很好。同时研究者们也发现用户数据符合长尾定律，也就是说很多的feature id只出现了几次，而一小部分feature id出现很多次。这在训练过程中增加了很多噪声，并且加重了过拟合。</p><p>对于这个问题一个简单的处理办法就是：人工的去掉出现次数比较少的feature id。缺点是：损失的信息不好评估；阈值的设定非常的粗糙。</p><p><strong>DIN给出的解决方案是：</strong></p><ol><li>针对feature id出现的频率，来自适应的调整他们正则化的强度；</li><li>对于出现频率高的，给与较小的正则化强度；</li><li>对于出现频率低的，给予较大的正则化强度。</li></ol><p><img src="https://pic1.zhimg.com/80/v2-ec2e6fd529f3dd0f0cede1f73e3b0da8_720w.jpg" alt></p><p><img src="https://pic2.zhimg.com/80/v2-0e64614ffeb4b940dcd8e1ec1d7ae5c1_720w.jpg" alt></p><p>其中B是mini batch样本，大小为b；ni是出现频率；Ii表示我们考虑特征非零的样本。</p><p>这样做的原因是，作者实践发现出现频率高的物品无论是在模型评估还是线上收入中都有较大影响。<br><br></p><h1 id="4、结果展示"><a href="#4、结果展示" class="headerlink" title="4、结果展示"></a>4、结果展示</h1><p><br><br>下图展示了用户兴趣分布：颜色越暖表示用户兴趣越高，可以看到用户的兴趣分布有多个峰。</p><p><img src="https://pic4.zhimg.com/80/v2-8c99874f15242d4770216e43f7d21f1f_720w.jpg" alt></p><p>利用候选的广告，反向激活历史兴趣。不同的历史兴趣爱好对于当前候选广告的权重不同，做到了local activation，如下图：</p><p><img src="https://pic3.zhimg.com/80/v2-8f486013316db4282e4b5b2f1349d156_720w.jpg" alt><br><br></p><h1 id="5、实验"><a href="#5、实验" class="headerlink" title="5、实验"></a>5、实验</h1><p><br><br><strong>Amazon Dataset</strong></p><p>作者选取了Electronics 子集 ，包含192,403 users, 63,001 goods, 801 categories and 1,689,188 samples。该数据集中的用户行为比较多，每个用户的商品有超过5个用户评论。特征包括goods_id, cate_id, user reviewed goods_id_list and cate_id_list。假设一个用户的行为是 <img src="https://www.zhihu.com/equation?tex=b_%7B1%7D%2Cb_%7B2%7D%2C...%2Cb_%7Bk%7D%2C...b_%7Bn%7D" alt="[公式]">，那么认为就是使用前面k个看过的物品预测第k+1个物品。Training dataset is generated with k = 1, 2, . . . , n 2 for each user，测试集是使用前面n-1个看过的物品预测最后一个物品。</p><p>对所有模型使用 SGD as the optimizer with exponential decay， 学习率从1开始衰减到0.1。min-batch大小为32。</p><p><strong>MovieLens Dataset</strong></p><p>contains 138,493 users, 27,278 movies, 21 categories and 20,000,263 samples。</p><p>为了让其适合ctr任务，作者将其转换为二分类数据。之前电影评分为0-5，作者将4、5标注为正样本，其余为负样本。根据userId来划分训练集和测试集。among all 138,493 users, of which 100,000 are randomly selected into training set (about 14,470,000 samples) and the rest 38,493 into the test set (about 5,530,000 samples).</p><p>任务变成:根据用户的历史行为，判断用户是否会对某个电影打分超过3分。Features include movie_id, movie_cate_id and user rated movie_id_list, movie_cate_id_list。optimizer, learning rate and mini-batch size 和上一个数据集相同。</p><p><strong>Alibaba Dataset</strong></p><p>取自阿里展示广告系统 线上流量，两周做训练集，接下来的一天为测试集。训练集和测试集大小分别为20亿和1.4亿。</p><p>For all the deep models, the dimensionality of embedding vector is 12 for the whole 16 groups of features. Layers of MLP issetby192⇥200⇥80⇥2.</p><p>由于数据量大，mini-batch size 为5000，Adam 为优化器。同时使用了指数衰减 in which learning rate starts at 0.001 and decay rate is set to 0.9.</p><p><img src="https://pic1.zhimg.com/80/v2-6d5711fbca0d33a8facc074ad65ab9fc_720w.jpg" alt></p><p>所有实验重复5次，平均指标作为最终指标。</p><p>1.DNN 均超过LR。<br>2.PNN和DeepFM 超过W&amp;D。<br>3.DIN效果最好,尤其在amazon上，作者归因于局部激活单元的作用。<br>4.Dice 是有效的。</p><p>其他相关实验其参阅原论文，本次就聊到这里啦。</p><p>实现DIN的一个Demo，感兴趣的童鞋可以看下我的<a href="https://link.zhihu.com/?target=https%3A//github.com/Jesse-csj/TensorFlow_Practice" target="_blank" rel="noopener">[github]</a>。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本人才疏学浅，不足之处欢迎大家指出和交流。&lt;/p&gt;
&lt;p&gt;今天要分享的是阿里针对&lt;strong&gt;电子商务领域&lt;/strong&gt;的CTR预估的Deep Interest Network理论，由盖坤大神团队提出于2017年。该模型要点为：结合&lt;strong&gt;Attention机制&lt;/strong&gt;，&lt;strong&gt;充分利用/挖掘用户历史行为数据中的信息，&lt;/strong&gt;下面一起来看下。&lt;/p&gt;
&lt;p&gt;原文：《Deep Interest Network for Click-Through Rate Prediction》&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="ctr" scheme="https://jesse-csj.github.io/categories/ctr/"/>
    
    
      <category term="推荐系统" scheme="https://jesse-csj.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
      <category term="Attention" scheme="https://jesse-csj.github.io/tags/Attention/"/>
    
      <category term="DNN" scheme="https://jesse-csj.github.io/tags/DNN/"/>
    
  </entry>
  
  <entry>
    <title>（读论文）推荐系统之ctr预估-MLR(LS-PLM)模型解析</title>
    <link href="https://jesse-csj.github.io/2019/08/12/MLR/"/>
    <id>https://jesse-csj.github.io/2019/08/12/MLR/</id>
    <published>2019-08-12T02:11:19.000Z</published>
    <updated>2020-10-19T09:07:59.410Z</updated>
    
    <content type="html"><![CDATA[<p>本人才疏学浅，不足之处欢迎大家指出和交流。</p><p>迟更几天，今天继续带来传统模型之MLR算法模型，这是一篇来自阿里盖坤团队的方案（LS-PLM），发表于2017年，但实际在2012年就已经提出并应用于实际业务中（膜拜ing），当时主流仍然是我们上一篇提到过的的LR模型，而本文作者创新性地提出了MLR(mixed logistic regression, 混合逻辑斯特回归)算法，引领了广告领域CTR预估算法的全新升级。总的来说，MLR算法创新地提出并实现了直接在原始空间学习特征之间的非线性关系，基于数据自动发掘可推广的模式，相比于人工来说效率和精度均有了大幅提升。下面我们一起来了解下细节。</p><p>原文：《Learning Piece-wise Linear Models from Large Scale Data for Ad Click Prediction》<br><a id="more"></a></p><p>地址：<a href="https://arxiv.org/abs/1704.05194" target="_blank" rel="noopener">https://arxiv.org/abs/1704.05194</a></p><p><br></p><h1 id="1、背景"><a href="#1、背景" class="headerlink" title="1、背景"></a>1、背景</h1><p><br><br>CTR预估(click-through-rate prediction)是广告行业比较常见的问题，根据用户的历史行为来判断用户对广告点击的可能性。在常见工业场景中，该问题的输入往往是数以万计的稀疏特征向量，在进行特征交叉后会维数会更高，比较常见的就是采用逻辑回归模型加一些正则化，因为逻辑回归模型计算开销小且容易实现并行。之前提到的facebook的一篇论文<a href="https://zhuanlan.zhihu.com/p/76794626" target="_blank" rel="noopener">（LR+GBDT）</a>中先用树模型做分类之后再加一个逻辑回归模型，最后得出效果出奇的好，应该也是工业界比较常用的方法，同时树模型的选择或者说是再构造特征的特性也逐渐被大家所关注。另一种比较有效的就是因子分解模型系列，包括FM及其的其他变种，它们的主要思想就是构造交叉特征或者是二阶的特征来一起进行训练。</p><p>这篇文章中，作者主要提出了一种piece-wise的线性模型，并且给出了其在大规模数据上的训练算法，称之为LS-PLM(Large Scale Piecewise Linear Model)，LS-PLM采用了分治的思想，先分成几个局部再用线性模型拟合，这两部都采用监督学习的方式，来优化总体的预测误差，总的来说有以下优势：</p><ul><li><p><strong>端到端的非线性学习</strong>： 从模型端自动挖掘数据中蕴藏的非线性模式，省去了大量的人工特征设计，这 使得MLR算法可以端到端地完成训练，在不同场景中的迁移和应用非常轻松。通过分区来达到拟合非线性函数的效果；</p></li><li><p><strong>可伸缩性（scalability）</strong>：与逻辑回归模型相似，都可以很好的处理复杂的样本与高维的特征，并且做到了分布式并行；</p></li><li><p><strong>稀疏性</strong>: 对于在线学习系统，模型的稀疏性较为重要，所以采用了$L_{1}$和\(L_{2,1}\)正则化，模型的学习和在线预测性能更好。当然，目标函数非凸非光滑为算法优带来了新的挑战。</p></li></ul><p><br></p><h1 id="2、MLR总览"><a href="#2、MLR总览" class="headerlink" title="2、MLR总览"></a>2、MLR总览</h1><p><br></p><p>MLR就像它的名字一样，由很多个LR模型组合而成。用分片线性模式来拟合高维空间的非线性模式，形式化表述如下：</p><img src="/2019/08/12/MLR/1.png"> <p>给定样本x，模型的预测p(y|x)分为两部分：首先根据<br>\(\sigma（u_{j}^{T}x）\)分割特征空间为m部分，其中m为给定的超参数，然后对于各部分计算\(\eta（w_{j}^{T}x）\)作为各部分的预测。函数g(⋅)确保了我们的模型满足概率函数的定义。当我们将softmax函数作为分割函数σ(x)，将sigmoid函数作为拟合函数η(x)的时候，该模型为：</p><img src="/2019/08/12/MLR/2.png"> <p>此时我们的混合模型可以看做一个FOE模型：</p><img src="/2019/08/12/MLR/3.png"> <p>目标损失函数为：</p><img src="/2019/08/12/MLR/4.png"> <p>论文中一个直观的例子，<strong>如下图，LR不能拟合非线性数据，MLR可以拟合非线性数据，因为划分-训练模式</strong>。</p><img src="/2019/08/12/MLR/5.png"> <p>这种菱形分界面（非线性数据）其实非常难学，但MLR在其中表现出色。通过控制分片数量m，可以平衡模型的拟合能力和过拟合。上图m=4。论文中m=12得到了不错的效果。</p><p>理论上来说，增大m可以带来无限制的非线性拟合能力，但是同样会增加计算、存储的开销，同时会带来过拟合的风险。具体如何选取m要结合实际情况取舍；</p><p>同时MLR还引入了<strong>结构化先验、分组稀疏、线性偏置、模型级联、增量训练、Common Feature Trick</strong>来提升模型性能。</p><p>针对MLR上面提到的各种特性，下面我们一一来介绍细节：</p><p><br></p><h1 id="3、模型细节"><a href="#3、模型细节" class="headerlink" title="3、模型细节"></a>3、模型细节</h1><p><br></p><h2 id="3-1-结构化先验"><a href="#3-1-结构化先验" class="headerlink" title="3.1 结构化先验"></a>3.1 结构化先验</h2><p>MLR中非常重要的就是如何划分原始特征空间。</p><p>通过引入结构化先验，我们使用用户特征来划分特征空间，使用广告特征来进行基分类器的训练，减小了模型的探索空间，收敛更容易。</p><p>同时，这也是符合我们认知的：不同的人群具有聚类特性，同一类人群具有类似的广告点击偏好。</p><h2 id="3-2-线性偏置"><a href="#3-2-线性偏置" class="headerlink" title="3.2 线性偏置"></a>3.2 线性偏置</h2><p>针对CTR预估问题中存在的两种偏置：</p><ul><li><p><strong>Position Bias</strong>：排名第1位和第5位的样本，点击率天然存在差异。宝贝展示的页面、位置影响点击率</p></li><li><p><strong>Sample Bias</strong>：PC和Mobile上的样本，点击率天然存在差异。</p></li></ul><p>在原来宝贝特征x的基础上，增加偏移向量y(场景、页数、位置等)。如果直接学习联合概率P(X,Y)面临问题：学习联合概率一定需要x和y的大部分组合，但是实际情况，并不是所有的x，y的组合都能有采样。针对这个问题，提出了带偏移MLR算法，形式化表述如下：</p><img src="/2019/08/12/MLR/6.png"> <p>而且，大规模非线性CTR预估和偏移变量的分解一起优化。并且，只需要很少的一些x，y组合就可以了。从论文给出的数据中，AUC提高了2-8个百分点。</p><h2 id="3-3-模型级联"><a href="#3-3-模型级联" class="headerlink" title="3.3 模型级联"></a>3.3 模型级联</h2><p>盖坤在PPT讲解到，MLR支持与LR的级联式训练。有点类似于Wide &amp; Deep，一些强Feature配置成级联形式能够提高模型的收敛性。例如典型的应用方法是：以统计反馈类特征构建第一层模型，输出FBctr级联到第二级大规模稀疏ID特征中去，能得到更好的提升效果。</p><img src="/2019/08/12/MLR/7.png"> <p>反馈特征常用的如反馈CTR，是指系统上线一段时间之后得到的历史CTR值。</p><h2 id="3-4-增量训练"><a href="#3-4-增量训练" class="headerlink" title="3.4 增量训练"></a>3.4 增量训练</h2><p>实验证明，MLR利用结构先验（用户特征进行聚类，广告特征进行分类）进行pretrain，然后再增量进行全空间参数寻优训练，会使得收敛步数更少，收敛更稳定。</p><img src="/2019/08/12/MLR/8.jpg"> <p><br></p><h1 id="4、一些trick"><a href="#4、一些trick" class="headerlink" title="4、一些trick"></a>4、一些trick</h1><p><br></p><p>论文的idea简单有效的，重点是工程中出来的论文，对工程实现上的优化细节都很详细，下面我们来看下：</p><h2 id="4-1-并行化"><a href="#4-1-并行化" class="headerlink" title="4.1 并行化"></a>4.1 并行化</h2><p>论文里的实现基于分布式，包括两个维度的并行化，模型并行化，数据并行化。每一个计算节点中都包含两种角色：<strong>Server Node, Worker Node</strong>，这样做的好处有两点：</p><ul><li><p>最大化利用CPU计算资源。之前大多数Server Node单独放到一台服务器上，造成CPU资源的极大浪费。</p></li><li><p>最大化利用Memory资源。</p></li></ul><img src="/2019/08/12/MLR/9.jpg"> <h2 id="4-2-Common-Feature-Trick"><a href="#4-2-Common-Feature-Trick" class="headerlink" title="4.2 Common Feature Trick"></a>4.2 Common Feature Trick</h2><img src="/2019/08/12/MLR/10.jpg"> <p>一个用户在一次pageview中会看到多个广告，每个广告都组成一条样本。所以这些样本之间很多特征都是重复的。这些特征包括：用户特征（年龄、性别等）、用户的历史访问信息（之前购买的物品、喜欢的店铺等）。那么我们对于向量内积的计算分成两部分：<strong>common和non-common parts:</strong></p><img src="/2019/08/12/MLR/11.jpg"> <p>利用<strong>Common Feature Trick</strong>可以从三个方面来优化并行化：</p><ul><li><p>对于有Common Feature的样本作为一组一起训练，并保证在存储在一个worker上</p></li><li><p>对于Common Feature仅仅保存一次，以便来节省内存</p></li><li><p>对于Common Feature的loss和梯度更新只需要一次即可</p></li></ul><p>下面是实验结果：</p><img src="/2019/08/12/MLR/12.jpg"> <p>可以看到Common Feature Trick效果还是非常明显的。</p><p><br></p><h1 id="5、总结"><a href="#5、总结" class="headerlink" title="5、总结"></a>5、总结</h1><p><br></p><p>文末说LS-PLM在2012年就应用于阿里巴巴的ctr预估，到2017年才发表论文。不出意料，现在的模型应该已经不再是MLR这么简单了。另外，从MLR和LR进行级联，以便加强强特征来看，MLR还是有很大的局限性。个人感觉模型理论上来说确实非常棒，利用分片线性来模型高维非线性，但是虽然取得了非常不错的成绩，但是带来的挑战也不小：比如初值问题、非凸问题的局部极值、虽然MLR比LR好，但不知道和全局最优相比还有多远；第二，在初值的Pre-train方面需要改进和优化模型函数等等；第三，目前规模化能力方面也需要能够吞吐更多特征和数据，比如采用更快的收敛算法等等；最后，整体的MLR算法的抽象能力也需进一步得到强化。</p><p><br><br><br><br>实现MLR的一个Demo，感兴趣的童鞋可以看下我的<a href="https://link.zhihu.com/?target=https%253A//github.com/Jesse-csj/TensorFlow_Practice" target="_blank" rel="noopener">[github]</a><br><br></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本人才疏学浅，不足之处欢迎大家指出和交流。&lt;/p&gt;
&lt;p&gt;迟更几天，今天继续带来传统模型之MLR算法模型，这是一篇来自阿里盖坤团队的方案（LS-PLM），发表于2017年，但实际在2012年就已经提出并应用于实际业务中（膜拜ing），当时主流仍然是我们上一篇提到过的的LR模型，而本文作者创新性地提出了MLR(mixed logistic regression, 混合逻辑斯特回归)算法，引领了广告领域CTR预估算法的全新升级。总的来说，MLR算法创新地提出并实现了直接在原始空间学习特征之间的非线性关系，基于数据自动发掘可推广的模式，相比于人工来说效率和精度均有了大幅提升。下面我们一起来了解下细节。&lt;/p&gt;
&lt;p&gt;原文：《Learning Piece-wise Linear Models from Large Scale Data for Ad Click Prediction》&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="ctr" scheme="https://jesse-csj.github.io/categories/ctr/"/>
    
    
      <category term="推荐系统" scheme="https://jesse-csj.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
      <category term="LR" scheme="https://jesse-csj.github.io/tags/LR/"/>
    
      <category term="MLR" scheme="https://jesse-csj.github.io/tags/MLR/"/>
    
  </entry>
  
  <entry>
    <title>（读论文）推荐系统之ctr预估-LR与GBDT+LR模型解析</title>
    <link href="https://jesse-csj.github.io/2019/08/05/LR/"/>
    <id>https://jesse-csj.github.io/2019/08/05/LR/</id>
    <published>2019-08-05T08:18:19.000Z</published>
    <updated>2020-10-19T09:09:24.154Z</updated>
    
    <content type="html"><![CDATA[<p>本人才疏学浅，不足之处欢迎大家指出和交流。</p><p>本系列接下来几天要分享的文章是关于线性模型的，在15年后深度模型在ctr预估领域百花齐放之时，仍觉得传统的LR模型有着基石的作用，于是有了今天的分享。话不多说，今天会介绍的是经典的LR模型及Facebook在2014年提出的GBDT+LR（重点）。当时深度学习几乎还没有应用到到计算广告领域，Facebook提出利用GBDT的叶节点编号作为非线性特征的表示，或者说是组合特征的一种方式，可以自动实现特征工程，下面我们一起来看看吧。</p><p>两篇原文：</p><ol><li><p><a href="http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=51313D6762FBE110533EDFDD7A47A8D9?doi=10.1.1.134.395&amp;rep=rep1&amp;type=pdf" target="_blank" rel="noopener">Predicting Clicks: Estimating the Click-Through Rate for New Ads</a></p></li><li><p><a href="http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=A54CCA7D4A8F05B6636C9D64316BCF96?doi=10.1.1.718.9050&amp;rep=rep1&amp;type=pdf" target="_blank" rel="noopener">Practical Lessons from Predicting Clicks on Ads at Facebook</a></p></li></ol><a id="more"></a><p><br></p><h1 id="1、背景"><a href="#1、背景" class="headerlink" title="1、背景"></a>1、背景</h1><p><br><br>这里是微软研究院在当时提出LR模型时的商业背景：搜索引擎主要靠商业广告收入，在广告位上面打广告，用户点击，之后广告商付费。在通用搜索引擎，通常广告位置是在搜索结果之前，或者在搜索结果右边，由此为查询选择正确的广告及其展示顺序会极大地影响用户看到并点击每个广告的概率。此排名对搜索引擎从广告中获得的收入产生了很大影响。此外，向用户展示他们喜欢点击的广告也会提高用户满意度。出于这些原因，能够准确估计系统中广告的点击率非常重要。（朴素的想法即是放用户可能点击的广告，并且放每次点击广告商付费多的广告）<br>于是就归结到点击率预估的<strong>模型选择和特征工程</strong>问题。<br><br></p><h1 id="2、LR-Model"><a href="#2、LR-Model" class="headerlink" title="2、LR Model"></a>2、LR Model</h1><h2 id="2-1-LR的数学基础"><a href="#2-1-LR的数学基础" class="headerlink" title="2.1 LR的数学基础"></a>2.1 LR的数学基础</h2><p><br><br>为何在2012年之前LR模型占据了计算广告领域的极大部分市场呢，我们可以从数学角度稍作分析：<br>逻辑回归（logistics regression）作为广义线性模型的一种，它的假设是因变量y服从<strong>伯努利分布</strong>。那么在点击率预估这个问题上，“点击”这个事件是否发生就是模型的因变量y。而用户是否点击广告这个问题是一个经典的掷偏心硬币（二分类）问题，因此CTR模型的因变量显然应该服从伯努利分布。所以采用LR作为CTR模型是符合“点击”这一事件的物理意义的。<br>在了解这一基础假设的情况下，再来看LR的数学形式就极具解释性了：<br><img src="/2019/08/05/LR/3.png"><br>其中x是输入向量，θ 是我们要学习的参数向量。结合CTR模型的问题来说，x就是输入的特征向量，h(x)就是我们最终希望得到的点击率。</p><h2 id="2-2、朴素的直觉和可解释性"><a href="#2-2、朴素的直觉和可解释性" class="headerlink" title="2.2、朴素的直觉和可解释性"></a>2.2、朴素的直觉和可解释性</h2><p><br><br>直观来讲，LR模型目标函数的形式就是各特征的加权和，最后再加以sigmoid函数。忽略其数学基础，仅靠我们的直觉认知也可以一定程度上得出使用LR作为CTR模型的合理性：<br><strong>使用各特征的加权和是为了综合不同特征对CTR的影响，而由于不同特征的重要程度不一样，所以为不同特征指定不同的权重来代表不同特征的重要程度。最后要加上sigmoid函数，是希望其值能够映射到0-1之间，使其符合CTR的概率意义。</strong></p><p>对LR的这一直观（或是主观）认识的另一好处就是模型具有极强的<strong>可解释性</strong>，算法工程师们可以轻易的解释哪些特征比较重要，在CTR模型的预测有偏差的时候，也可以轻易找到哪些因素影响了最后的结果。</p><h2 id="2-3、工程化的需要"><a href="#2-3、工程化的需要" class="headerlink" title="2.3、工程化的需要"></a>2.3、工程化的需要</h2><p><br><br>在工业界每天动辄TB级别的数据面前，模型的训练开销就异常重要了。在GPU尚未流行开来的2012年之前，LR模型也凭借其易于并行化、模型简单、训练开销小等特点占据着工程领域的主流。囿于工程团队的限制，即使其他复杂模型的效果有所提升，在没有明显beat LR之前，公司也不会贸然加大计算资源的投入升级CTR模型，这是LR持续流行的另一重要原因。<br><br></p><h1 id="3、GBDT-LR-Model"><a href="#3、GBDT-LR-Model" class="headerlink" title="3、GBDT+LR Model"></a>3、GBDT+LR Model</h1><p>这篇文章（原文2）主要介绍了CTR预估模型LR(Logistic Regression)+GBDT。当时深度学习还没有应用到计算广告领域，而在此之前为探索特征交叉而提出的FM（见本系列第一篇）和FFM（本系列忘写了…）虽然能够较好地解决数据稀疏性的问题，但他们仍停留在二阶交叉的情况。如果要继续提高特征交叉的维度，不可避免的会发生组合爆炸和计算复杂度过高等问题。在此基础上，2014年Facebook提出了基于GBDT+LR组合模型的解决方案。简而言之，Facebook提出了一种利用GBDT（Gradient Boosting Decision Tree）自动进行特征筛选和组合，进而生成新的离散特征向量，再把该特征向量当作LR模型输入，预估CTR的模型结构。随后Kaggle竞赛也有实践此思路，GBDT与LR融合开始引起了业界关注。</p><p>LR+GBDT相比于单纯的LR或者GBDT带来了较大的性能提升，论文中给出数据为3%，这在CTR预估领域确实非常不错。除此之外，Facebook还在在线学习、Data freshness、学习速率、树模型参数、特征重要度等方面进行了探索（本文不做讨论，有兴趣可参阅原文）。<strong>Online Learning</strong>是本文的一个重点，接下来我们会提到。</p><h2 id="3-1、GBDT"><a href="#3-1、GBDT" class="headerlink" title="3.1、GBDT"></a>3.1、GBDT</h2><p>GBDT 由多棵 CART 树组成，本质是多颗回归树组成的森林。每一个节点按贪心分裂，最终生成的树包含多层，这就相当于一个特征组合的过程。根据规则，样本一定会落在一个叶子节点上，将这个叶子节点记为1，其他节点设为0，得到一个向量。论文中如下图所示：有两棵树，第一棵树有三个叶子节点，第二棵树有两个叶子节点。如果一个样本落在第一棵树的第二个叶子，将它编码成 [0, 1, 0]。在第二棵树落到第一个叶子，编码成 [1, 0]。所以，输入到 LR 模型中的向量就是 [0, 1, 0, 1, 0]。</p><p>需要注意的一点是：利用 GBDT 模型进行自动特征组合和筛选是一个独立（独立于LR训练）的过程，于是乎这种方法的优点在于两个模型在训练过程中是独立进行的，不需要进行联合训练，自然也就不存在如何将LR的梯度回传到GBDT这类复杂的问题。<br><img src="/2019/08/05/LR/1.png"></p><p>然而，还有两个重要问题是为什么建树采用GBDT以及为什么要使用集成的决策树模型，而不是单棵的决策树模型呢？</p><p>对于第二个问题，我们可以认为一棵树的表达能力很弱，不足以表达多个有区分性的特征组合，多棵树的表达能力更强一些，可以更好的发现有效的特征和特征组合。</p><p>对于第一个问题，牵涉到GBDT原理问题，这里不做过多讨论，请参考两篇网上写的较好的文章：</p><ol><li><a href="https://www.cnblogs.com/peizhe123/p/5086128.html" target="_blank" rel="noopener">GBDT详解</a>。</li><li><a href="https://www.cnblogs.com/ModifyRong/p/7744987.html" target="_blank" rel="noopener">机器学习算法GBDT</a>。</li></ol><h2 id="3-2、GBDT-LR总结"><a href="#3-2、GBDT-LR总结" class="headerlink" title="3.2、GBDT+LR总结"></a>3.2、GBDT+LR总结</h2><p>由于决策树的结构特点，事实上，决策树的深度就决定了特征交叉的维度。如果决策树的深度为4，通过三次节点分裂，最终的叶节点实际上是进行了3阶特征组合后的结果，如此强的特征组合能力显然是FM系的模型不具备的。但由于GBDT容易产生过拟合，以及GBDT这种特征转换方式实际上丢失了大量特征的数值信息，因此我们不能简单说GBDT由于特征交叉的能力更强，效果就比FM或FFM好（事实上FFM是2015年提出的）。在模型的选择和调试上，永远都是多种因素综合作用的结果。</p><p>GBDT+LR比FM重要的意义在于，它大大推进了特征工程模型化这一重要趋势，某种意义上来说，之后深度学习的各类网络结构，以及embedding技术的应用，都是这一趋势的延续。</p><p><br></p><h1 id="4、总结（具体的对比实验和实现细节等请参阅原论文）"><a href="#4、总结（具体的对比实验和实现细节等请参阅原论文）" class="headerlink" title="4、总结（具体的对比实验和实现细节等请参阅原论文）"></a>4、总结（具体的对比实验和实现细节等请参阅原论文）</h1><p>本次简要介绍了传统的LR模型，其优点和缺点都非常突出，但不可否认的是其基石般的作用和地位。同时我们也介绍了GBDT+LR的改进方案以解决特征组合的问题，但GBDT也存在很容易过拟合的问题，接下来这段话摘自于知乎：</p><img src="/2019/08/05/LR/2.png"><p>这里是引用阿里的盖坤大神的回答，他认为GBDT只是对历史的一个记忆罢了，没有推广性，或者说泛化能力。 番外：盖坤大神的团队在2017年提出了两个重要的用于CTR预估的模型，MLR（实际开始应用于2012年）和DIN，后面的线性模型和深度模型也会更新这两篇，下篇再见吧~</p><p>实现GBDT+LR的一个Demo，感兴趣的童鞋可以看下我的<a href="[https://link.zhihu.com/?target=https%3A//github.com/Jesse-csj/TensorFlow_Practice](https://link.zhihu.com/?target=https%3A//github.com/Jesse-csj/TensorFlow_Practice">github</a>。<br><br><br><strong>巨人的肩膀</strong>：<a href="https://blog.csdn.net/shenziheng1/article/details/89737467" target="_blank" rel="noopener">https://blog.csdn.net/shenziheng1/article/details/89737467</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本人才疏学浅，不足之处欢迎大家指出和交流。&lt;/p&gt;
&lt;p&gt;本系列接下来几天要分享的文章是关于线性模型的，在15年后深度模型在ctr预估领域百花齐放之时，仍觉得传统的LR模型有着基石的作用，于是有了今天的分享。话不多说，今天会介绍的是经典的LR模型及Facebook在2014年提出的GBDT+LR（重点）。当时深度学习几乎还没有应用到到计算广告领域，Facebook提出利用GBDT的叶节点编号作为非线性特征的表示，或者说是组合特征的一种方式，可以自动实现特征工程，下面我们一起来看看吧。&lt;/p&gt;
&lt;p&gt;两篇原文：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=51313D6762FBE110533EDFDD7A47A8D9?doi=10.1.1.134.395&amp;amp;rep=rep1&amp;amp;type=pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Predicting Clicks: Estimating the Click-Through Rate for New Ads&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=A54CCA7D4A8F05B6636C9D64316BCF96?doi=10.1.1.718.9050&amp;amp;rep=rep1&amp;amp;type=pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Practical Lessons from Predicting Clicks on Ads at Facebook&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="ctr" scheme="https://jesse-csj.github.io/categories/ctr/"/>
    
    
      <category term="推荐系统" scheme="https://jesse-csj.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
      <category term="LR" scheme="https://jesse-csj.github.io/tags/LR/"/>
    
  </entry>
  
  <entry>
    <title>（读论文）推荐系统之ctr预估-XDeepFM模型解析</title>
    <link href="https://jesse-csj.github.io/2019/08/01/XDeepFM/"/>
    <id>https://jesse-csj.github.io/2019/08/01/XDeepFM/</id>
    <published>2019-08-01T08:18:19.000Z</published>
    <updated>2020-10-19T11:54:00.544Z</updated>
    
    <content type="html"><![CDATA[<p>本人才疏学浅，不足之处欢迎大家指出和交流。</p><p>今天要分享的是极深因子分解机模型（XDeepFM)。未闻其声先见其名：<br>可能我们现在有两个问题，一是为何它叫极深因子分解机（深在哪），二是和DeepFM有何关系？下面我们一起走进模型细节来看看吧。</p><p>原文：<a href="https://arxiv.org/pdf/1803.05170.pdf" target="_blank" rel="noopener">《xDeepFM: Combining Explicit and Implicit Feature Interactions for Recommender Systems》</a></p><a id="more"></a><p><br></p><h1 id="1、背景"><a href="#1、背景" class="headerlink" title="1、背景"></a>1、背景</h1><p><br><br>数据特点就不再说明了，有兴趣的同学翻下专栏前面的文章。首先看到XDeepFM这个名字很容易联想到17年华为提出的DeepFM。但正如上一篇所提到的，这篇论文其实是针对DCN进行改进的，感兴趣的同学可以先看下上一篇介绍DCN的文章。</p><p>DCN中我们可以看到，它的Cross层接在Embedding层之后，虽然可以显示自动构造高阶特征，但特征交互是发生在元素级（bit-wise）而非特征向量级（vector-wise）。这里先简单介绍下bit-wise与vector-wise；显式特征交互和隐式特征交互两组概念：</p><p><strong>bit-wise与vector-wise</strong>：假设隐向量的维度为3维，如果两个特征对应的向量分别为(a1,b1,c1)和(a2,b2,c2)的话，在进行交互时，交互的形式类似于f(w1∗a1∗a2,w2∗b1∗b2,w3∗c1∗c2) 的话，此时我们认为特征交互是发生在<strong>元素级（bit-wise）</strong>上。如果特征交互形式类似于f(w∗(a1∗a2,b1∗b2,c1∗c2)) 的话，那么我们认为特征交互是发生在<strong>特征向量级（vector-wise）</strong>。</p><p><strong>显式的特征交互和隐式的特征交互</strong>：以两个特征为例xi和xj，在经过一系列变换后，我们可以表示成 wij∗(xi∗xj) 的形式，就可以认为是显式特征交互，否则的话，是隐式的特征交互。</p><p>例如，Age Field对应嵌入向量<a1,b1,c1>，Occupation Field对应嵌入向量<a2,b2,c2>，在Cross层，a1,b1,c1,a2,b2,c2会拼接后直接作为输入，即它意识不到Field vector的概念。Cross 以嵌入向量中的单个bit为最细粒度，而FM是以向量为最细粒度学习相关性，即<strong>vector-wise</strong>。<strong>xDeepFM的动机，正是将FM的vector-wise的思想引入Cross部分</strong>。</a2,b2,c2></a1,b1,c1></p><p><br></p><h1 id="2、XDeepFM"><a href="#2、XDeepFM" class="headerlink" title="2、XDeepFM"></a>2、XDeepFM</h1><h2 id="2-1、DCN的不足"><a href="#2-1、DCN的不足" class="headerlink" title="2.1、DCN的不足"></a>2.1、DCN的不足</h2><p>xDeepFM主要是针对DCN的改进，论文中指出了DCN的缺点（为何我们前面说DCN的特征交叉是元素级别的呢，下面给出说明），提出了更有效的解决办法，作者称之为CIN结构。</p><p>我们来回顾下DCN的结构如下：</p><p>◆ 首先将数据进行embedding和stacking，输入到Cross网络和Deep网络中；然后将两个网络的输出融合得到输出。</p><p>◆ DCN中对Cross网络的设置有如下公式：</p><p><img src="https://pic1.zhimg.com/v2-0b40b7f0122cca04ef0ebd5c5aec8d84_b.png" alt></p><p>◆ 简化这个公式，去掉偏置项，可以得到：</p><p><img src="https://pic3.zhimg.com/v2-2641253221204f0844c32ecbda99bc86_b.png" alt></p><p>◆ 对于x1，有如下公式：</p><p><img src="https://pic3.zhimg.com/v2-e65fc90261fde9e5209cc1c656a866b2_b.png" alt></p><p>◆ 合并可得到：</p><p><img src="https://pic4.zhimg.com/v2-02a3bbaeea9e7c48433ee468b41d55df_b.png" alt></p><p>◆ 其中 <img src="https://www.zhihu.com/equation?tex=%5Calpha_%7B1%7D+%3D+x_%7B0%7D%5E%7BT%7Dw_%7Bk%7D+%2B1" alt="[公式]"> 实际上是关于 <strong>X₀</strong> 的一个线性回归函数，也就是得到的一个标量，不断递推到Xi+1有：</p><p><img src="https://pic2.zhimg.com/v2-80af14b5e10b084794b818c1e2d28a1d_b.png" alt></p><p>实际上，a(i+1)仍是一个标量，就相当于让 <strong>X₀</strong> 不断乘上一个数。</p><p>这一步我们就可以明显地看出了：DCN的<strong>特征交互是发生在元素级（bit-wise）而不是特征向量之间（vector-wise）</strong>，这一点违背了因子分解机的初衷。</p><h2 id="2-2、提出CIN（Compressed-Interaction-Network）"><a href="#2-2、提出CIN（Compressed-Interaction-Network）" class="headerlink" title="2.2、提出CIN（Compressed Interaction Network）"></a>2.2、提出CIN（Compressed Interaction Network）</h2><p><img src="https://pic1.zhimg.com/v2-d93a2ee9d429ea85c1bdc9ac9e269fe0_b.jpg" alt></p><p>于是，论文中提出了一种叫做CIN的结构，CIN应用向量维度，而不再是之前DCN模型的标量。如上图所示,CIN层的输入来自Embedding层，假设有m个field，每个field的<strong>embedding vector</strong>维度为D，则输入可表示为矩阵X₀（m*D）;</p><p>具体的来说，CIN的第K层的每个vector的计算公式为：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cbm%7BX%7D%5Ek_%7Bh%2C%2A%7D+%3D++%5Csum%5E%7BH_%7Bk-1%7D%7D_%7Bi%3D1%7D+++%5Csum%5E%7Bm%7D_%7Bj%3D1%7D+++%5Cbm%7BW%7D%5E%7Bk%2Ch%7D_%7Bij%7D++%28+%5Cbm%7BX%7D%5E%7Bk-1%7D_%7Bi%2C%2A%7D+++%5Ccirc+++%5Cbm%7BX%7D%5E%7B0%7D_%7Bj%2C%2A%7D++%29+%5Cin+%5Cmathbb%7BR%7D%5E%7B1%2AD%7D%2C+~~~~~~+where~~+1+%5Cle+h+%5Cle+H_k" alt="[公式]"></p><p>其中W矩阵表示第K层的第h个vector的权重矩阵，○ 表示<strong>哈达玛乘积</strong>，即逐元素乘，例如<img src="https://www.zhihu.com/equation?tex=%3Ca_1%2Cb_1%2Cc_1%3E+%5Ccirc+%3Ca_2%2Cb_2%2Cc_2%3E+%3D+%3Ca_1b_1%2Ca_2b_2%2Ca_3b_3%3E" alt="[公式]"></p><img src="/2019/08/01/XDeepFM/1.png"> <p>这样一看其实这种计算方式还是挺简单的，论文中还给出了从CNN角度来给出了分析：</p><p><img src="https://pic3.zhimg.com/v2-c7edac8ff4c07660477458059b326e0a_b.jpg" alt></p><img src="/2019/08/01/XDeepFM/2.png"> <p><img src="https://pic4.zhimg.com/v2-702c6821aeb85303528ff509fcd9bb07_b.jpg" alt></p><p>(3) 图（c）给出了CIN的整体结构，中间部分相当于是不同深度的图（b），对于生成的每个feature map都进行sum pooling，<strong>求和会将特征进行叠加</strong>。 从而实现输出单元可以得到不同阶数的特征交互模式。CIN的结构与RNN是很类似的，即每一层的状态是由前一层隐层的值与一个额外的输入数据计算所得。</p><h2 id="2-3、为何是CIN"><a href="#2-3、为何是CIN" class="headerlink" title="2.3、为何是CIN"></a>2.3、为何是CIN</h2><img src="/2019/08/01/XDeepFM/3.png"> <img src="/2019/08/01/XDeepFM/4.png"> <h2 id="2-4、复杂度分析："><a href="#2-4、复杂度分析：" class="headerlink" title="2.4、复杂度分析："></a>2.4、<strong>复杂度分析：</strong></h2><p>假设CIN和DNN每层神经元/向量个数都为H，网络深度为T 。那么CIN的参数空间复杂度为O(mTH²),普通的DNN为O(mDH+TH²),CIN的空间复杂度与输入维度 D 无关。</p><p>CIN的时间复杂度显然更高，按照上面分析的计算方式为O(mDTH²)，而DNN为O(mDH+TH²)，<strong>时间复杂度会是CIN的一个主要弱点</strong>。<br><br></p><h1 id="3、总结（具体的对比实验和实现细节等请参阅原论文）"><a href="#3、总结（具体的对比实验和实现细节等请参阅原论文）" class="headerlink" title="3、总结（具体的对比实验和实现细节等请参阅原论文）"></a>3、总结（具体的对比实验和实现细节等请参阅原论文）</h1><ol><li><p>xDeepFM将<strong>基于Field的vector-wise</strong>思想引入Cross，并且保留了Cross的优势，实验效果也提升明显。极深的意义就在于xDeepFm就真正做到了FM高阶交叉后的”Deep” Factorization Machine。</p></li><li><p>xDeepFM的时间复杂度较高，未来能持续改进它的性能，才能被应用在大规模计算任务中（工业界落地）。</p></li></ol><p>实现XDeepFM的源码，请参考原论文作者开源的代码：<a href="https://link.zhihu.com/?target=https%3A//github.com/Leavingseason/xDeepFM" target="_blank" rel="noopener">github</a></p><p><strong>巨人的肩膀：</strong><a href="https://zhuanlan.zhihu.com/p/57162373" target="_blank" rel="noopener">刺猬：xDeepFM：名副其实的 ”Deep” Factorization Machine</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本人才疏学浅，不足之处欢迎大家指出和交流。&lt;/p&gt;
&lt;p&gt;今天要分享的是极深因子分解机模型（XDeepFM)。未闻其声先见其名：&lt;br&gt;可能我们现在有两个问题，一是为何它叫极深因子分解机（深在哪），二是和DeepFM有何关系？下面我们一起走进模型细节来看看吧。&lt;/p&gt;
&lt;p&gt;原文：&lt;a href=&quot;https://arxiv.org/pdf/1803.05170.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;《xDeepFM: Combining Explicit and Implicit Feature Interactions for Recommender Systems》&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="ctr" scheme="https://jesse-csj.github.io/categories/ctr/"/>
    
    
      <category term="推荐系统" scheme="https://jesse-csj.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
      <category term="FM" scheme="https://jesse-csj.github.io/tags/FM/"/>
    
      <category term="Vector-Wise" scheme="https://jesse-csj.github.io/tags/Vector-Wise/"/>
    
  </entry>
  
  <entry>
    <title>（读论文）推荐系统之ctr预估-DCN模型解析</title>
    <link href="https://jesse-csj.github.io/2019/07/30/DCN/"/>
    <id>https://jesse-csj.github.io/2019/07/30/DCN/</id>
    <published>2019-07-30T11:19:37.000Z</published>
    <updated>2019-08-27T09:03:47.586Z</updated>
    
    <content type="html"><![CDATA[<p>本人才疏学浅，不足之处欢迎大家指出和交流。</p><p>今天要分享的是2017年斯坦福与Google联合提出的DCN模型，和明天要分享的XDeepFM是配套的，同时这篇论文是Google 对 Wide &amp; Deep工作的一个后续研究。本人写文章的顺序尽量严格按照时间顺序来的~话不多说，来看看今天分享的深度模型（串行结构）有哪些创新之处吧。</p><p>原文：《Deep &amp; Cross Network for Ad Click Predictions》<br><a id="more"></a></p><p>地址：<a href="https://arxiv.org/pdf/1708.05123.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1708.05123.pdf</a></p><h1 id="1、背景及相关工作"><a href="#1、背景及相关工作" class="headerlink" title="1、背景及相关工作"></a>1、背景及相关工作</h1><p><br></p><p>（每篇文章都重复下这些背景哈，希望大家别看烦，尽量换种方式）：传统的CTR预估模型需要大量的人工特征工程，耗时耗力；引入DNN之后，依靠神经网络强大的学习能力，可以一定程度上实现自动学习特征组合。但是DNN的缺点在于隐式的学习特征组合带来的不可解释性，以及低效率的学习(并不是所有的特征组合都是有用的)。这时交叉网络应运而生，同时联合DNN，发挥两者的共同优势。传统的CTR预估模型需要大量的人工特征工程，耗时耗力；引入DNN之后，依靠神经网络强大的学习能力，可以一定程度上实现自动学习特征组合。但是DNN的缺点在于隐式的学习特征组合带来的不可解释性，以及低效率的学习(并不是所有的特征组合都是有用的)。这时交叉网络应运而生，同时联合DNN，发挥两者的共同优势。</p><p><strong>相关工作：</strong>由于数据集规模和维数的急剧增加，之前已经提出了许多方法：</p><p>最开始FM使用隐向量的内积来建模组合特征；</p><p>FFM在此基础上引入field的概念，针对不同的field使用不同的隐向量。但是，这两者都是针对低阶（二阶，高阶会产生非常大的计算成本）的特征组合进行建模的；  随着DNN在计算机视觉、自然语言处理、语音识别等领域取得重要进展，DNN几乎无限的表达能力被广泛的研究。同样也尝试被用来解决web产品中输入数据高维高稀疏的问题。DNN可以对高维组合特征进行建模，但是DNN的不可解释性让DNN是否是目前最高效的针对此类问题的建模方式成为了一个问题；  另一方面，在Kaggle上的很多比赛中，大部分的获胜方案都是使用的人工特征工程，构造低阶的组合特征，这些特征意义明确且高效。而DNN学习到的特征都是隐式的、高度非线性的高阶组合特征，含义非常难以解释。这揭示了一个模型能够比通用的DNN设计更能够有效地学习的有界度特征的相互作用，那是否能设计一种DNN的特定网络结构来改善DNN，使得其学习起来更加高效呢？</p><p>Wide＆Deep是其中一个探索的例子，它以交叉特征作为一个线性模型的输入，与一个DNN模型一起训练，然而，W&amp;D网络的成功取决于正确的交叉特征的选择（仍依赖人工特征工程），这是一个至今还没有明确有效的方法解决的指数问题。</p><p>于是提出DCN进行进一步探索，将Wide部分替换为由特殊网络结构实现的Cross，<strong>自动构造有限高阶的交叉特征</strong>，并学习对应权重，告别了繁琐的人工叉乘。下面一起来看下细节：</p><h1 id="2、DEEP-amp-CROSS-NETWORK"><a href="#2、DEEP-amp-CROSS-NETWORK" class="headerlink" title="2、DEEP &amp; CROSS NETWORK"></a>2、DEEP &amp; CROSS NETWORK</h1><img src="/2019/07/30/DCN/1.png"> <p>DCN整体模型的架构图如上：底层是Embedding and stacking layer，然后是并行的Cross Network和Deep Network，最后是Combination Layer把Cross NetworkDeep Network的结果stack得到Output。</p><h2 id="2-1-Embedding-and-stacking-layer"><a href="#2-1-Embedding-and-stacking-layer" class="headerlink" title="2.1 Embedding and stacking layer"></a>2.1 Embedding and stacking layer</h2><p>DCN底层的两个功能是Embed和Stack。</p><p><strong>Embed</strong>：<br>在web-scale的推荐系统比如CTR预估中，输入的大部分特征都是类别型特征，通常的处理办</p><p>法就是编码为one-hot向量，对于实际应用中维度会非常高且稀疏，因此使用：<br><img src="/2019/07/30/DCN/2.png"><br>来将这些离散特征转换成实数值的稠密向量。</p><p><strong>Stack</strong>：<br>处理完了类别型特征，还有连续型特征需要处理。所以我们把连续型特征规范化之后，和嵌入向量stacking（堆叠）到一起形成一个向量，就得到了原始的输入：<br><img src="/2019/07/30/DCN/3.png"></p><h2 id="2-2-Cross-network"><a href="#2-2-Cross-network" class="headerlink" title="2.2 Cross network"></a>2.2 Cross network</h2><p>Cross Network是这个模型的核心，它被设计来<strong>高效地应用</strong>显式的交叉特征，关键在于如何高效地进行<strong>feature crossing</strong>。对于每层的计算，使用下述公式：<br><img src="/2019/07/30/DCN/4.png"></p><p>其中xl和xl+1 分别是第l层和第l+1层cross layer的输出（的列向量），wl和bl是这两层之间的连接参数。注意上式中所有的变量均是<strong>列向量</strong>，W也是列向量，并不是矩阵。</p><p><strong>理解</strong>：<br>这其实应用了残差网络的思想，<strong>xl+1 = f(xl, wl, bl) + xl：</strong>每一层的输出，都是上一层的输出加上<strong>feature crossing</strong> <strong>f</strong>。而<strong>f</strong>就是在<strong>拟合该层输出和上一层输出的残差（</strong>xl+1​−xl​<strong>）。残差网络有很多优点，其中一点是处理梯度退化/消失的问题，使神经网络可以“更深”.</strong>一层交叉层的可视化如下图所示：<br><img src="/2019/07/30/DCN/5.png"></p><p><strong>High-degree Interaction Across Features:</strong><br>Cross Network特殊的网络结构使得cross feature的阶数随着layer depth的增加而增加。相对于输入x0来说，一个l层的cross network的cross feature的阶数为l+1。</p><p><strong>复杂度分析：</strong><br>假设一共有Lc层cross layer，起始输入x0的维度为d。那么整个cross network的参数个数为:<br><img src="/2019/07/30/DCN/6.png"><br>因为每一层的W和b都是d维的。从上式可以发现，复杂度是输入维度d的线性函数，所以相比于deep network，cross network引入的复杂度微不足道。这样就保证了DCN的复杂度和DNN是一个级别的。论文中分析Cross Network的这种效率是因为x0 * xT的秩为1，使得我们不用计算并存储整个的矩阵就可以得到所有的cross terms。<br>但是，正是因为cross network的参数比较少导致它的表达能力受限，为了能够学习高阶非线性的组合特征，DCN并行的引入了Deep Network。</p><h2 id="2-3-Deep-network"><a href="#2-3-Deep-network" class="headerlink" title="2.3 Deep network"></a>2.3 Deep network</h2><p>深度网络就是一个全连接的前馈神经网络，每个深度层具有如下公式:<br><img src="/2019/07/30/DCN/7.png"><br>分析计算一下参数的数量来估计下复杂度。假设输入x0维度为d，一共有Lc层神经网络，每一层的神经元个数都是m个。那么总的参数或者复杂度为：<br><img src="/2019/07/30/DCN/8.png"></p><h2 id="2-4-Combination-layer"><a href="#2-4-Combination-layer" class="headerlink" title="2.4 Combination layer"></a>2.4 Combination layer</h2><p>Combination Layer把Cross Network和Deep Network的输出拼接起来，然后经过一个加权求和后得到logits，然后输入到标准的逻辑回归函数得到最终的预测概率。形式化如下：<br><img src="/2019/07/30/DCN/9.png"></p><p>p是最终的预测概率；XL1是d维的，表示Cross Network的最终输出；hL2是m维的，表示Deep Network的最终输出；Wlogits是Combination Layer的权重；最后经过sigmoid函数，得到最终预测概率。</p><p>损失函数使用带正则项的log loss，形式化如下：<br><img src="/2019/07/30/DCN/10.png"></p><p>此外，Cross Network和Deep Network，DCN是一起训练Cross Network和Deep Network的，这样网络可以知道另外一个网络的存在。</p><h1 id="3、CROSS-NETWORK-ANALYSIS"><a href="#3、CROSS-NETWORK-ANALYSIS" class="headerlink" title="3、CROSS NETWORK ANALYSIS"></a>3、CROSS NETWORK ANALYSIS</h1><p>本节是为了解释DCN的高效性,从三个角度：</p><p><strong>1、polynomial approximation；2、generalization to FMs；3、efficientprojection。</strong><br>（具体的分析请大家参阅原论文：这部分是非常重要的一部分，是论文中解释DCN为何高效的理论部分，然而本人还没有完全看懂，等之后再仔细看下再更新-  。-）</p><h1 id="4、总结（具体的对比实验和实现细节等请参阅原论文）"><a href="#4、总结（具体的对比实验和实现细节等请参阅原论文）" class="headerlink" title="4、总结（具体的对比实验和实现细节等请参阅原论文）"></a>4、总结（具体的对比实验和实现细节等请参阅原论文）</h1><p><br></p><p>DCN模型的特点：</p><p>1. 在cross network中，在每一层都应用feature crossing。高效的学习了bounded degree组合特征。不需要人工特征工程。</p><p>2. 网络结构简单且高效。多项式复杂度由layer depth决定。</p><p>3. 相比于DNN，DCN的logloss更低，而且参数的数量将近少了一个数量级。</p><p>4. 但是经过对cross network的分析如下，最终得到的输出就相当于X0 不断乘以一个数（标量），而且它们的特征交互是发生在元素级（bit-wise）。这种处理方式可能是存在问题的。<br><img src="/2019/07/30/DCN/11.png"></p><p>（当然，指出这些问题的就是我们下一篇论文XDeepFM啦，我们下篇再见）<br><br><br>实现DCN的一个Demo，感兴趣的童鞋可以看下我的<a href="https://link.zhihu.com/?target=https%3A//github.com/Jesse-csj/TensorFlow_Practice" target="_blank" rel="noopener">github</a>。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本人才疏学浅，不足之处欢迎大家指出和交流。&lt;/p&gt;
&lt;p&gt;今天要分享的是2017年斯坦福与Google联合提出的DCN模型，和明天要分享的XDeepFM是配套的，同时这篇论文是Google 对 Wide &amp;amp; Deep工作的一个后续研究。本人写文章的顺序尽量严格按照时间顺序来的~话不多说，来看看今天分享的深度模型（串行结构）有哪些创新之处吧。&lt;/p&gt;
&lt;p&gt;原文：《Deep &amp;amp; Cross Network for Ad Click Predictions》&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="ctr" scheme="https://jesse-csj.github.io/categories/ctr/"/>
    
    
      <category term="推荐系统" scheme="https://jesse-csj.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
      <category term="DNN" scheme="https://jesse-csj.github.io/tags/DNN/"/>
    
      <category term="Cross network" scheme="https://jesse-csj.github.io/tags/Cross-network/"/>
    
  </entry>
  
  <entry>
    <title>（读论文）推荐系统之ctr预估-AFM模型解析</title>
    <link href="https://jesse-csj.github.io/2019/07/29/AFM/"/>
    <id>https://jesse-csj.github.io/2019/07/29/AFM/</id>
    <published>2019-07-29T14:08:11.000Z</published>
    <updated>2020-10-19T08:40:04.594Z</updated>
    
    <content type="html"><![CDATA[<p>本人才疏学浅，不足之处欢迎大家指出和交流。</p><p>因个人原因最近都没更新，今天补上一篇FM家族的论文（AFM），接下来会将阿里的几个模型进行汇总下分开来分享，希望大家一起学习呀。</p><p>话不多说，今天要分享的是一个Attentional Factorization Machine模型，是17年FM家族的成员。它和NFM是同一个作者，其在FM上的改进，最大的特点就是使用一个<strong>attention network</strong>来学习不同组合特征的重要性。下面我们一起来看下。</p><p>原文：《Attentional Factorization Machines: Learning the Weight of Feature Interactions via Attention Network》<br><a id="more"></a></p><p>地址： <a href="https://www.ijcai.org/proceedings/2017/0435.pdf" target="_blank" rel="noopener">https://www.ijcai.org/proceedings/2017/0435.pdf</a></p><p><br></p><h1 id="1、Introduction"><a href="#1、Introduction" class="headerlink" title="1、Introduction"></a>1、Introduction</h1><p><br></p><p>首先介绍对于监督学习问题，类别特征作为输入，一般采用One-hot编码，所以需要引入特征交互来做出更精确的预测；但是如果直接以product的方式来显示交互，对于稀疏输入数据集，只能观察到一些交叉特征；所以提出FM，利用隐变量来做内积实现交互，但是FM也存在问题，也就是所有交互特征的权重是一样的，但是在实际中，对于预测性较低的特征，其对应权重也较低，所以AFM就是基于这个思想；</p><p><br></p><h1 id="2、FM"><a href="#2、FM" class="headerlink" title="2、FM"></a>2、FM</h1><p><br></p><p>FM全称Factorization Machine（详情可见之前的文章FM），通过隐向量内积来对每一对特征组合进行建模。</p><p>提出FM仍存在有下面两个问题：</p><ul><li><p>一个特征针对其他不同特征都使用同一个隐向量。所以有了FFM用来解决这个问题。</p></li><li><p>所有组合特征的权重w都有着相同的权重1。AFM就是用来解决这个问题的。</p></li></ul><p>在一次预测中，并不是所有的特征都有用的，但是FM对于所有的组合特征都使用相同的权重。AFM就是从这个角度进行优化的，针对不同的特征组合使用不同的权重。这也使得模型的可解释性更强，方便后续针对重要的特征组合进行深入研究。</p><p>虽然多层神经网络已经被证明可以有效的学习高阶特征组合。但是DNN的缺点也很明显（参数多，可解释性较弱，这里AFM未采用，智者见智吧）</p><p><br></p><h1 id="3、AFM"><a href="#3、AFM" class="headerlink" title="3、AFM"></a>3、AFM</h1><h2 id="3-1-Model"><a href="#3-1-Model" class="headerlink" title="3.1 Model"></a>3.1 Model</h2><p><br></p><p>AFM的模型结构如下：</p><img src="/2019/07/29/AFM/1.png"><p>可以看到Sparse Input和Embedding Layer和FM中的是相同的，Embedding Layer把输入特征中非零部分特征embed成一个dense vector。剩下的三层为重点，如下：</p><h3 id="Pair-wise-Interaction-Layer"><a href="#Pair-wise-Interaction-Layer" class="headerlink" title="Pair-wise Interaction Layer:"></a><strong>Pair-wise Interaction Layer:</strong></h3><p>这一层主要是对组合特征进行建模，原来的m个嵌入向量，通过element-wise product（两两做内积）操作得到了m(m-1)/2个组合向量，这些向量的维度和嵌入向量的维度相同均为k。形式化如下：</p><img src="/2019/07/29/AFM/2.png"><p>也就是说Pair-wise Interaction Layer的输入是所有嵌入向量，输出也是一组向量。输出是任意两个嵌入向量的element-wise product。任意两个嵌入向量都组合得到一个Interacted vector，所以m个嵌入向量得到m(m-1)/2个向量。</p><p>如果不考虑Attention机制，在Pair-wise Interaction Layer之后直接得到最终输出，可以形式化如下：</p><img src="/2019/07/29/AFM/3.png"><p>其中p和b分别是权重矩阵和偏置。当p全为1的时候，我们发现这就是FM。这个只是说明AFM的表达能力是在FM之上的，实际的情况中后面还使用了Attention机制。NFM中的Bilinear Interaction Layer也是把任意两个嵌入向量做element-wise product，然后进行sum pooling（求和池化，同NFM）操作。</p><h3 id="Attention-based-Pooling-Layer"><a href="#Attention-based-Pooling-Layer" class="headerlink" title="Attention-based Pooling Layer:"></a><strong>Attention-based Pooling Layer:</strong></h3><p>Attention机制的核心思想在于：当把不同的部分压缩在一起的时候，让不同的部分的贡献程度不一样。AFM通过在Interacted vector后增加一个weighted sum来实现Attention机制。形式化如下：</p><img src="/2019/07/29/AFM/4.png"><p>这里aij是交互特征的Attention score，表示不同的组合特征对于最终的预测的贡献程度。可以看到：</p><ol><li><p>Attention-based Pooling Layer的输入是Pair-wise Interaction Layer的输出。它包含m(m-1)/2个向量，每个向量的维度是k。（k是嵌入向量的维度，m是Embedding Layer中嵌入向量的个数）</p></li><li><p>Attention-based Pooling Layer的输出是一个k维向量。它对Interacted vector使用Attention score进行了weighted sum pooling（加权求和池化）操作。</p></li></ol><p><strong>但Attention score的学习是一个问题</strong>。一个常规的想法就是随着最小化loss来学习，但是存在一个问题是：对于训练集中从来没有一起出现过的特征组合的Attention score无法学习。</p><p><strong>为了解决泛化问题</strong>，引入多层感知机（MLP），这里称为Attention network，其形式化定义如下：<br><img src="/2019/07/29/AFM/5.png"></p><p>可以看到，Attention network实际上是一个one layer MLP，激活函数使用ReLU，网络大小用attention factor表示，就是神经元的个数。它的输入是两个嵌入向量element-wise product之后的结果(interacted vector，用来在嵌入空间中对组合特征进行编码)；它的输出是组合特征对应的Attention score（aij）。最后，使用softmax对得到的Attention score进行规范化。</p><p><strong>总结一下，AFM模型总形式化如下：</strong></p><img src="/2019/07/29/AFM/6.png"><p>前面一部分是线性部分；后面一部分对每两个嵌入向量进行element-wise product得到Interacted vector；然后使用Attention机制得到每个组合特征的Attention score，并用这个score来进行weighted sum pooling；最后将这个k维的向量通过权重矩阵的预测结果。</p><h2 id="3-2-Learning"><a href="#3-2-Learning" class="headerlink" title="3.2 Learning"></a>3.2 Learning</h2><p><br></p><p>AFM可以用于不同的任务：回归、分类、排序等，一般对于回归问题是平方损失，二分类是Logloss，本文使用平方损失函数，且使用SGD算法来最优化模型参数；</p><p>防止过拟合： 防止过拟合常用的方法是Dropout或者L2 L1正则化。AFM的做法是：</p><ol><li><p>在Pair-wise Interaction Layer的输出使用Dropout</p></li><li><p>在Attention Network中使用L2正则化</p></li></ol><p>Attention Network是一个one layer MLP。不使用Dropout是因为，作者发现如果同时在interaction layer和Attention Network中使用Dropout会使得训练不稳定，并且降低性能。</p><p>所以，AFM的loss函数更新为：</p><img src="/2019/07/29/AFM/7.png"><p><br></p><h1 id="4、Experiments"><a href="#4、Experiments" class="headerlink" title="4、Experiments"></a>4、Experiments</h1><p><br></p><p>数据集：MovieLens和Frappe；</p><ul><li><p>Frappe，9万条app使用日志，常用于context-aware推荐，上下文变量都是类别变量，天气、城市、时间等。独热编码得到5382特征。</p></li><li><p>MovieLens，66万电影的tag，用于标签推荐。UserID、movieID和tag转化得到90445特征。</p></li></ul><p>数据集划分：70% 训练, 20%验证， 10% 测试；</p><p>使用RMSE作为性能衡量，且和LibFM、HOFM、Wide&amp;Deep和DeepCross进行性能对（Embedding大小256维）；这里用FM Embedding向量预训练AFM会导致更好的性能表现（相比于随机初始化）；</p><img src="/2019/07/29/AFM/8.png"><img src="/2019/07/29/AFM/9.png"><p><br></p><h1 id="5、总结："><a href="#5、总结：" class="headerlink" title="5、总结："></a>5、总结：</h1><p><br></p><p>AFM是在FM的基础上改进的。相比于其他的DNN模型，比如Wide&amp;Deep，DeepCross都是通过MLP来隐式学习组合特征。这些Deep Methods都缺乏解释性，因为并不知道各个组合特征的情况。相比之下，FM通过两个隐向量内积来学习组合特征，解释性就比较好。</p><p>通过直接扩展FM，AFM引入Attention机制来学习不同组合特征的权重，即保证了模型的可解释性又提高了模型性能。</p><p>但是，DNN的另一个作用是提取高阶组合特征，而AFM由于最后的加权累加，二次项并没有进行更深的网络去学习高阶交叉特征，这应该是缺点之一。<br><br></p><p>实现AFM的一个Demo，感兴趣的童鞋可以看下我的<a href="https://github.com/Jesse-csj/TensorFlow_Practice" target="_blank" rel="noopener">[github]</a>。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本人才疏学浅，不足之处欢迎大家指出和交流。&lt;/p&gt;
&lt;p&gt;因个人原因最近都没更新，今天补上一篇FM家族的论文（AFM），接下来会将阿里的几个模型进行汇总下分开来分享，希望大家一起学习呀。&lt;/p&gt;
&lt;p&gt;话不多说，今天要分享的是一个Attentional Factorization Machine模型，是17年FM家族的成员。它和NFM是同一个作者，其在FM上的改进，最大的特点就是使用一个&lt;strong&gt;attention network&lt;/strong&gt;来学习不同组合特征的重要性。下面我们一起来看下。&lt;/p&gt;
&lt;p&gt;原文：《Attentional Factorization Machines: Learning the Weight of Feature Interactions via Attention Network》&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="ctr" scheme="https://jesse-csj.github.io/categories/ctr/"/>
    
    
      <category term="推荐系统" scheme="https://jesse-csj.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
      <category term="FM" scheme="https://jesse-csj.github.io/tags/FM/"/>
    
      <category term="Attention" scheme="https://jesse-csj.github.io/tags/Attention/"/>
    
  </entry>
  
  <entry>
    <title>（读论文）推荐系统之ctr预估-NFM模型解析</title>
    <link href="https://jesse-csj.github.io/2019/07/29/NFM/"/>
    <id>https://jesse-csj.github.io/2019/07/29/NFM/</id>
    <published>2019-07-29T12:18:19.000Z</published>
    <updated>2019-08-27T09:01:32.303Z</updated>
    
    <content type="html"><![CDATA[<p>本人才疏学浅，不足之处欢迎大家指出和交流。</p><p>周末没更新呀…今天要分享的是另一个Deep模型NFM(串行结构)。NFM也是用FM+DNN来对问题建模的，相比于之前提到的Wide&amp;Deep(Google)、DeepFM(华为+哈工大)、PNN(上交)和之后会分享的的DCN(Google)、DIN(阿里)等，NFM有什么优点呢，下面就走进模型我们一起来看看吧。</p><p>原文：《Neural Factorization Machines for Sparse Predictive Analytics》<br><a id="more"></a></p><p>地址：<a href="https://arxiv.org/pdf/1708.05027.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1708.05027.pdf</a></p><h1 id="1、问题由来"><a href="#1、问题由来" class="headerlink" title="1、问题由来"></a>1、问题由来</h1><p><br></p><p>老生常谈，再聊数据特点：对于广告中的大量的类别特征，特征组合也是非常多的。传统的做法是通过人工特征工程或者利用决策树来进行特征选择，选择出比较重要的特征。但是这样的做法都有一个缺点，就是：<strong>无法学习训练集中没有出现的特征组合</strong>。</p><p>最近几年，Embedding-based方法开始成为主流，通过把高维稀疏的输入_embed_到低维度的稠密的隐向量空间中，模型可以学习到训练集中没有出现过的特征组合。</p><p>Embedding-based大致可以分为两类：</p><p><strong>1.factorization machine-based linear models</strong></p><p><strong>2.neural network-based non-linear models</strong><a href="#_msocom_1"></a><br>（具体就不再展开了）</p><hr><p> <a href="#_msoanchor_1"></a>FM：以线性的方式学习二阶特征交互，对于捕获现实数据非线性和复杂的内在结构表达力不够；</p><p>深度网络：例如Wide&amp;Deep 和DeepCross，简单地拼接特征embedding向量不会考虑任何的特征之间的交互, 但是能够学习特征交互的非线性层的深层网络结构又很难训练优化；</p><p>而NFM摒弃了直接把嵌入向量拼接输入到神经网络的做法，在嵌入层之后增加了_Bi-Interaction_操作来对二阶组合特征进行建模。这使得low level的输入表达的信息更加的丰富，极大的提高了后面隐藏层学习高阶非线性组合特征的能力。</p><h1 id="2、NFM"><a href="#2、NFM" class="headerlink" title="2、NFM"></a>2、NFM</h1><h2 id="2-1-NFM-Model"><a href="#2-1-NFM-Model" class="headerlink" title="2.1 NFM Model"></a>2.1 NFM Model</h2><p>与FM（因式分解机）相似，NFM使用实值特征向量。给定一个稀疏向量x∈Rn作为输入，其中特征值为xi=0表示第i个特征不存在，NFM预估的目标为:<br><img src="/2019/07/29/NFM/1.png"><br>其中第一项和第二项是线性回归部分，与FM相似，FM模拟数据的全局偏差和特征权重。第三项f(x)是NFM的核心组成部分,用于建模特征交互。它是一个多层前馈神经网络。如图2所示，接下来，我们一层一层地阐述f(x)的设计。<br>模型整体结构图如下所示：<br><img src="/2019/07/29/NFM/2.png"></p><h3 id="2-1-1-Embedding-Layer"><a href="#2-1-1-Embedding-Layer" class="headerlink" title="2.1.1 Embedding Layer"></a>2.1.1 Embedding Layer</h3><p>和其他的DNN模型处理稀疏输入一样，Embedding将输入转换到低维度的稠密的嵌入空间中进行处理。这里做稍微不同的处理是，使用原始的特征值乘以Embedding vector，使得模型也可以处理real valued feature。</p><h3 id="2-1-2-Bi-Interaction-Layer"><a href="#2-1-2-Bi-Interaction-Layer" class="headerlink" title="2.1.2 Bi-Interaction Layer"></a>2.1.2 Bi-Interaction Layer</h3><p>Bi是Bi-linear的缩写，这一层其实是一个pooling层操作，它把很多个向量转换成一个向量，形式化如下：<br><img src="/2019/07/29/NFM/3.png"></p><p>fbi的输入是整个的嵌入向量，xi ，xj是特征取值，vi， vj是特征对应的嵌入向量。中间的操作表示对应位置相乘。所以原始的嵌入向量任意两个都进行组合，对应位置相乘结果得到一个新向量；然后把这些新向量相加，就得到了Bi-Interaction的输出。这个输出只有一个向量。</p><p>注：Bi-Interaction并没有引入额外的参数，而且它的计算复杂度也是线性的，参考FM的优化方法，化简如下：<br><img src="/2019/07/29/NFM/4.png"></p><h3 id="2-1-3-Hidden-Layer"><a href="#2-1-3-Hidden-Layer" class="headerlink" title="2.1.3 Hidden Layer"></a>2.1.3 Hidden Layer</h3><p>这个跟其他的模型基本一样，堆积隐藏层以期来学习高阶组合特征。一般选用constant的效果要好一些。</p><h3 id="2-1-4-Prediction-Layer"><a href="#2-1-4-Prediction-Layer" class="headerlink" title="2.1.4 Prediction Layer"></a>2.1.4 Prediction Layer</h3><p>最后一层隐藏层Zl到输出层最后预测结果形式化如下：<br><img src="/2019/07/29/NFM/5.png"><br>其中h是中间的网络参数。考虑到前面的各层隐藏层权重矩阵，f(x)形式化如下：<br><img src="/2019/07/29/NFM/6.png"></p><p>这里相比于FM其实多出的参数其实就是隐藏层的参数，所以说FM也可以看做是一个神经网络架构，就是去掉隐藏层的NFM。</p><h2 id="2-2-NFM-vs-Wide-amp-Deep、DeepCross"><a href="#2-2-NFM-vs-Wide-amp-Deep、DeepCross" class="headerlink" title="2.2 NFM vs Wide&amp;Deep、DeepCross"></a>2.2 NFM vs Wide&amp;Deep、DeepCross</h2><p><strong>实质：</strong></p><p>NFM最重要的区别就在于Bi-Interaction Layer。Wide&amp;Deep和DeepCross都是用拼接操作(concatenation)替换了Bi-Interaction。</p><p>Concatenation操作的最大缺点就是它并没有考虑任何的特征组合信息，所以就全部依赖后面的MLP去学习特征组合，但是很不幸，MLP的学习优化非常困难。</p><p>使用Bi-Interaction考虑到了二阶特征组合，使得输入的表示包含更多的信息，减轻了后面MLP部分的学习压力，所以可以用更简单的模型（实验中只一层隐层），取得更好的效果。</p><h1 id="3、总结（具体的对比实验和实现细节等请参阅原论文）"><a href="#3、总结（具体的对比实验和实现细节等请参阅原论文）" class="headerlink" title="3、总结（具体的对比实验和实现细节等请参阅原论文）"></a>3、总结（具体的对比实验和实现细节等请参阅原论文）</h1><p><br></p><p>NFM主要的特点如下：</p><p>1. NFM核心就是在NN中引入了Bilinear Interaction(Bi-Interaction) pooling操作。基于此，NN可以在low level就学习到包含更多信息的组合特征。</p><p>2. 通过deepen FM来学习高阶的非线性的组合特征。</p><p>3. NFM相比于上面提到的DNN模型，模型结构更浅、更简单(shallower structure)，但是性能更好，训练和调整参数更加容易。</p><p>所以，依旧是FM+DNN的组合套路，不同之处在于如何处理Embedding向量，这也是各个模型重点关注的地方。现在来看业界就如何用DNN来处理高维稀疏的数据并没有一个统一普适的方法，依旧在摸索中。</p><p>实现DeepFM的一个Demo，感兴趣的童鞋可以看下我的<a href="https://link.zhihu.com/?target=https%3A//github.com/Jesse-csj/TensorFlow_Practice" target="_blank" rel="noopener">github</a>。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本人才疏学浅，不足之处欢迎大家指出和交流。&lt;/p&gt;
&lt;p&gt;周末没更新呀…今天要分享的是另一个Deep模型NFM(串行结构)。NFM也是用FM+DNN来对问题建模的，相比于之前提到的Wide&amp;amp;Deep(Google)、DeepFM(华为+哈工大)、PNN(上交)和之后会分享的的DCN(Google)、DIN(阿里)等，NFM有什么优点呢，下面就走进模型我们一起来看看吧。&lt;/p&gt;
&lt;p&gt;原文：《Neural Factorization Machines for Sparse Predictive Analytics》&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="ctr" scheme="https://jesse-csj.github.io/categories/ctr/"/>
    
    
      <category term="推荐系统" scheme="https://jesse-csj.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
      <category term="FM" scheme="https://jesse-csj.github.io/tags/FM/"/>
    
      <category term="Bi-Interaction" scheme="https://jesse-csj.github.io/tags/Bi-Interaction/"/>
    
  </entry>
  
  <entry>
    <title>（读论文）推荐系统之ctr预估-DeepFM模型解析</title>
    <link href="https://jesse-csj.github.io/2019/07/24/DeepFM/"/>
    <id>https://jesse-csj.github.io/2019/07/24/DeepFM/</id>
    <published>2019-07-24T14:30:28.000Z</published>
    <updated>2019-08-27T09:00:49.937Z</updated>
    
    <content type="html"><![CDATA[<p>今天第二篇（最近更新的都是Deep模型，传统的线性模型会后面找个时间更新的哈）。本篇介绍华为的DeepFM模型 (2017年)，此模型在 Wide&amp;Deep 的基础上进行改进，成功解决了一些问题，具体的话下面一起来看下吧。</p><p>原文：《Deepfm: a factorization-machine based neural network for ctr prediction》<br><a id="more"></a></p><p>地址：<a href="http://www.ijcai.org/proceedings/2017/0239.pdf" target="_blank" rel="noopener">http://www.ijcai.org/proceedings/2017/0239.pdf</a></p><h1 id="1、问题由来"><a href="#1、问题由来" class="headerlink" title="1、问题由来"></a>1、问题由来</h1><h2 id="1-1、背景"><a href="#1-1、背景" class="headerlink" title="1.1、背景"></a>1.1、背景</h2><p>CTR 预估 数据特点：</p><ol><li><p>输入中包含类别型和连续型数据。类别型数据需要 one-hot, 连续型数据可以先离散化再 one-hot，也可以直接保留原值。</p></li><li><p>维度非常高。</p></li><li><p>数据非常稀疏。</p></li><li><p>特征按照 Field 分组。</p></li></ol><p>CTR 预估 重点 在于 学习组合特征。注意，组合特征包括二阶、三阶甚至更高阶的，阶数越高越复杂，越不容易学习。Google 的论文研究得出结论：高阶和低阶的组合特征都非常重要，同时学习到这两种组合特征的性能要比只考虑其中一种的性能要好。</p><p>那么关键问题转化成：如何高效的提取这些组合特征。一种办法就是引入领域知识人工进行特征工程。这样做的弊端是高阶组合特征非常难提取，会耗费极大的人力。而且，有些组合特征是隐藏在数据中的，即使是专家也不一定能提取出来，比如著名的“尿布与啤酒”问题。</p><p>在 DeepFM 提出之前，已有 LR（线性或广义线性模型后面更新），FM、FFM、FNN、PNN（以及三种变体：IPNN,OPNN,PNN*）,Wide&amp;Deep 模型，这些模型在 CTR 或者是推荐系统中被广泛使用。</p><h2 id="1-2、现有模型的问题"><a href="#1-2、现有模型的问题" class="headerlink" title="1.2、现有模型的问题"></a>1.2、现有模型的问题</h2><p><br></p><p>线性模型：最开始 CTR 或者是推荐系统领域，一些线性模型取得了不错的效果（线性模型LR简单、快速并且模型具有可解释，有着很好的拟合能力），但是LR模型是线性模型，表达能力有限，泛化能力较弱，需要做好特征工程，尤其需要交叉特征，才能取得一个良好的效果，然而在工业场景中，特征的数量会很多，可能达到成千上万，甚至数十万，这时特征工程就很难做，还不一定能取得更好的效果。</p><p>FM模型：线性模型差强人意，直接导致了 FM 模型应运而生（在 Kaggle 上打比赛提出来的，取得了第一名的成绩）。FM 通过隐向量 latent vector 做内积来表示组合特征，从理论上解决了低阶和高阶组合特征提取的问题。但是实际应用中受限于计算复杂度，一般也就只考虑到 2 阶交叉特征。后面又进行了改进，提出了 FFM，增加了 Field 的概念。</p><p>遇上深度学习：随着 DNN 在图像、语音、NLP 等领域取得突破，人们渐渐意识到 DNN 在特征表示上的天然优势。相继提出了使用 CNN 或 RNN 来做 CTR 预估的模型。但是，CNN 模型的缺点是：偏向于学习相邻特征的组合特征。  RNN 模型的缺点是：比较适用于有序列 (时序) 关系的数据。</p><p>FNN 的提出，应该算是一次非常不错的尝试：先使用预先训练好的 FM，得到隐向量，然后作为 DNN 的输入来训练模型。缺点在于：受限于 FM 预训练的效果。</p><p>随后提出了 PNN，PNN 为了捕获高阶组合特征，在embedding layer和first hidden layer之间增加了一个product layer。根据 product layer 使用内积、外积、混合分别衍生出IPNN, OPNN, PNN*三种类型。</p><p>但无论是 FNN 还是 PNN，他们都有一个绕不过去的缺点：对于低阶的组合特征，学习到的比较少。 而前面我们说过，低阶特征对于 CTR 也是非常重要的。</p><p>Google（上一篇） 意识到了这个问题，为了同时学习低阶和高阶组合特征，提出了 Wide&amp;Deep 模型。它混合了一个 线性模型（Wide part） 和 Deep 模型 (Deep part)。这两部分模型需要不同的输入，而 Wide part 部分的输入，依旧 依赖人工特征工程。</p><p>但是，这些模型普遍都存在两个问题：</p><ol><li><p>偏向于提取低阶或者高阶的组合特征。不能同时提取这两种类型的特征。</p></li><li><p>需要专业的领域知识来做特征工程。</p></li></ol><p>于是DeepFM 应运而生，成功解决了这两个问题，并做了一些改进，其 优点 如下：</p><ol><li><p>不需要预训练 FM 得到隐向量。</p></li><li><p>不需要人工特征工程。</p></li><li><p>能同时学习低阶和高阶的组合特征。</p></li><li><p>FM 模块和 Deep 模块共享 Feature Embedding 部分，可以更快的训练，以及更精确的训练学习。</p></li></ol><p>下面就一直来走进模型的细节。</p><h1 id="2、模型细节"><a href="#2、模型细节" class="headerlink" title="2、模型细节"></a>2、模型细节</h1><p><br></p><p>DeepFM主要做法如下：</p><ol><li><p>FM Component + Deep Component。FM 提取低阶组合特征，Deep 提取高阶组合特征。但是和 Wide&amp;Deep 不同的是，DeepFM 是端到端的训练，不需要人工特征工程。</p></li><li><p>共享 feature embedding。FM 和 Deep 共享输入和feature embedding不但使得训练更快，而且使得训练更加准确。相比之下，Wide&amp;Deep 中，input vector 非常大，里面包含了大量的人工设计的 pairwise 组合特征，增加了它的计算复杂度。</p></li></ol><p>模型整体结构图如下所示：<br><img src="/2019/07/24/DeepFM/1.png"></p><p>由上面网络结构图可以看到，DeepFM 包括 FM和 DNN两部分，所以模型最终的输出也由这两部分组成：</p><img src="/2019/07/24/DeepFM/2.png"><p>下面，把结构图进行拆分，分别来看这两部分。</p><h2 id="2-1、The-FM-Component"><a href="#2-1、The-FM-Component" class="headerlink" title="2.1、The FM Component"></a>2.1、The FM Component</h2><p>FM 部分的输出由两部分组成：一个 Addition Unit，多个 内积单元。</p><img src="/2019/07/24/DeepFM/4.png"><p>这里的 d 是输入 one-hot 之后的维度，我们一般称之为feature_size。对应的是 one-hot 之前的特征维度，我们称之为field_size。</p><img src="/2019/07/24/DeepFM/3.png"><p>架构图如上图所示：Addition Unit 反映的是 1 阶的特征。内积单元 反映的是 2 阶的组合特征对于预测结果的影响。</p><p>这里需要注意三点：</p><ol><li><p>这里的wij，也就是<vi,vj>，可以理解为DeepFM结构中计算embedding vector的权矩阵（并非是网上很多文章认为的vi是embedding vector）。</vi,vj></p></li><li><p>由于输入特征one-hot编码，所以embedding vector也就是输入层到Dense Embeddings层的权重。</p></li><li><p>Dense Embeddings层的神经元个数是由embedding vector和field_size共同确定，直观来说就是：神经元的个数为embedding vector*field_size。</p></li></ol><p><strong>FM Component 总结：</strong></p><ol><li><p>FM 模块实现了对于 1 阶和 2 阶组合特征的建模。</p></li><li><p>无须预训练。</p></li><li><p>没有人工特征工程。</p></li><li><p>embedding 矩阵的大小是：特征数量 * 嵌入维度。然后用一个 index 表示选择了哪个特征。</p></li></ol><p>需要训练的有两部分：</p><ol><li><p>input_vector 和 Addition Unit 相连的全连接层，也就是 1 阶的 Embedding 矩阵。</p></li><li><p>Sparse Feature 到 Dense Embedding 的 Embedding 矩阵，中间也是全连接的，要训练的是中间的权重矩阵，这个权重矩阵也就是隐向量 Vi。</p></li></ol><h2 id="2-2、The-Deep-Component"><a href="#2-2、The-Deep-Component" class="headerlink" title="2.2、The Deep Component"></a>2.2、The Deep Component</h2><p>Deep Component 架构图：</p><img src="/2019/07/24/DeepFM/5.png"><p>这里DNN的作用是构造高阶组合特征，网络里面黑色的线是全连接层，参数需要神经网络去学习。且有一个特点：DNN的输入也是embedding vector。所谓的权值共享指的就是这里。</p><p>关于DNN网络中的输入a处理方式采用前向传播，如下所示：</p><img src="/2019/07/24/DeepFM/6.png"><p>这里假设α(0)=(e1,e2,…em)表示 embedding层的输出，那么α(0)作为下一层 DNN隐藏层的输入，其前馈过程如下：</p><img src="/2019/07/24/DeepFM/7.png"><p>优点：</p><ol><li><p>模型可以从最原始的特征中，同时学习低阶和高阶组合特征</p></li><li><p>不再需要人工特征工程。Wide&amp;Deep 中低阶组合特征就是同过特征工程得到的。</p></li></ol><h1 id="3、总结（具体的对比实验和实现细节等请参阅原论文）"><a href="#3、总结（具体的对比实验和实现细节等请参阅原论文）" class="headerlink" title="3、总结（具体的对比实验和实现细节等请参阅原论文）"></a>3、总结（具体的对比实验和实现细节等请参阅原论文）</h1><p><br></p><p>DeepFM优点：</p><ol><li><p>没有用 FM 去预训练隐向量 Vi，并用 Vi去初始化神经网络。（相比之下 FNN 就需要预训练 FM 来初始化 DNN）。</p></li><li><p>FM 模块不是独立的，是跟整个模型一起训练学习得到的。（相比之下 Wide&amp;Deep 中的 Wide 和 Deep 部分是没有共享的）</p></li><li><p>不需要特征工程。（相比之下 Wide&amp;Deep 中的 Wide 部分需要特征工程）</p></li><li><p>训练效率高。（相比 PNN 没有那么多参数）</p></li></ol><p>其中最核心的：</p><ol><li><p>没有预训练（no pre-training）</p></li><li><p>共享 Feature Embedding，没有特征工程（no feature engineering）</p></li><li><p>同时学习低阶和高阶组合特征（capture both low-high-order interaction features）</p></li></ol><p>实现DeepFM的一个Demo，感兴趣的童鞋可以关注我的<a href="https://github.com/Jesse-csj/TensorFlow_Practice" target="_blank" rel="noopener">github</a>。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;今天第二篇（最近更新的都是Deep模型，传统的线性模型会后面找个时间更新的哈）。本篇介绍华为的DeepFM模型 (2017年)，此模型在 Wide&amp;amp;Deep 的基础上进行改进，成功解决了一些问题，具体的话下面一起来看下吧。&lt;/p&gt;
&lt;p&gt;原文：《Deepfm: a factorization-machine based neural network for ctr prediction》&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="ctr" scheme="https://jesse-csj.github.io/categories/ctr/"/>
    
    
      <category term="推荐系统" scheme="https://jesse-csj.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
      <category term="FM" scheme="https://jesse-csj.github.io/tags/FM/"/>
    
      <category term="DNN" scheme="https://jesse-csj.github.io/tags/DNN/"/>
    
  </entry>
  
  <entry>
    <title>（读论文）推荐系统之ctr预估-Wide＆Deep模型解析</title>
    <link href="https://jesse-csj.github.io/2019/07/24/Wide-Deep/"/>
    <id>https://jesse-csj.github.io/2019/07/24/Wide-Deep/</id>
    <published>2019-07-23T23:33:22.000Z</published>
    <updated>2019-08-27T09:01:17.240Z</updated>
    
    <content type="html"><![CDATA[<p>在读了FM和FNN/PNN的论文后，来学习一下16年的一篇Google的论文，文章将传统的LR和DNN组合构成一个wide&amp;deep模型（并行结构），既保留了LR的拟合能力，又具有DNN的泛化能力，并且不需要单独训练模型，可以方便模型的迭代，一起来看下吧。<br>原文：《Wide &amp; Deep Learning for Recommender Systems》<br><a id="more"></a></p><p>地址： <a href="https://arxiv.org/pdf/1606.07792.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1606.07792.pdf</a></p><h1 id="1、问题由来"><a href="#1、问题由来" class="headerlink" title="1、问题由来"></a><strong>1、问题由来</strong></h1><h2 id="1-1、背景"><a href="#1-1、背景" class="headerlink" title="1.1、背景"></a><strong>1.1、背景</strong></h2><p>本文提出时是针对推荐系统中应用的，当然也可以应用在ctr预估中。<br>首先介绍论文中通篇出现的两个名词：</p><ul><li>memorization（暂且翻译为记忆）：即从历史数据中发现item或者特征之间的相关性。</li><li>generalization（暂且翻译为泛化）：即相关性的传递，发现在历史数据中很少或者没有出现的新的特征组合。</li></ul><p>举个例子来解释下：在人类的认知学习过程中演化过程中，人类的大脑很复杂，它可以记忆(memorize)下每天发生的事情（麻雀可以飞，鸽子可以飞）然后泛化(generalize)这些知识到之前没有看到过的东西（有翅膀的动物都能飞）。<br>但是泛化的规则有时候不是特别的准确，有时候会出错（有翅膀的动物都能飞吗）。这时候就需要记忆(memorization)来修正泛化的规则(generalized rules)，叫做特例（企鹅有翅膀，但是不能飞）。这就是Memorization和Generalization的来由或者说含义。</p><h2 id="1-2、现有模型的问题"><a href="#1-2、现有模型的问题" class="headerlink" title="1.2、现有模型的问题"></a><strong>1.2、现有模型的问题</strong></h2><ul><li><p>线性模型LR简单、快速并且模型具有可解释，有着很好的拟合能力，但是LR模型是线性模型，表达能力有限，泛化能力较弱，需要做好特征工程，尤其需要交叉特征，才能取得一个良好的效果，然而在工业场景中，特征的数量会很多，可能达到成千上万，甚至数十万，这时特征工程就很难做，还不一定能取得更好的效果。 </p></li><li><p>DNN模型不需要做太精细的特征工程，就可以取得很好的效果，DNN可以自动交叉特征，学习到特征之间的相互作用，尤其是可以学到高阶特征交互，具有很好的泛化能力。另外，DNN通过增加embedding层，可以有效的解决稀疏数据特征的问题，防止特征爆炸。推荐系统中的泛化能力是很重要的，可以提高推荐物品的多样性，但是DNN在拟合数据上相比较LR会较弱。 </p></li><li><p>总结一下：</p><ol><li>线性模型无法学习到训练集中未出现的组合特征；</li><li>FM或DNN通过学习embedding vector虽然可以学习到训练集中未出现的组合特征，但是会过度泛化。</li></ol></li></ul><p>为了提高推荐系统的拟合性和泛化性，可以将LR和DNN结合起来，同时增强拟合能力和泛化能力，wide&amp;deep就是将LR和DNN组合起来，wide部分就是LR，deep部分就是DNN，将两者的结果组合进行输出。</p><h1 id="2、模型细节"><a href="#2、模型细节" class="headerlink" title="2、模型细节"></a><strong>2、模型细节</strong></h1><p>再简单介绍下两个名词的实现：<br><strong>Memorization：</strong>之前大规模稀疏输入的处理是：通过线性模型 + 特征交叉。所带来的Memorization以及记忆能力非常有效和可解释。但是Generalization（泛化能力）需要更多的人工特征工程。</p><p><strong>Generalization：</strong>相比之下，DNN几乎不需要特征工程。通过对低纬度的dense embedding进行组合可以学习到更深层次的隐藏特征。但是，缺点是有点over-generalize（过度泛化）。推荐系统中表现为：会给用户推荐不是那么相关的物品，尤其是user-item矩阵比较稀疏并且是high-rank（高秩矩阵）</p><p><strong>两者区别：</strong>Memorization趋向于更加保守，推荐用户之前有过行为的items。相比之下，generalization更加趋向于提高推荐系统的多样性（diversity）。</p><h2 id="2-1、Wide-和-Deep"><a href="#2-1、Wide-和-Deep" class="headerlink" title="2.1、Wide 和 Deep"></a><strong>2.1、Wide 和 Deep</strong></h2><p><strong>Wide &amp; Deep:</strong><br>Wide &amp; Deep包括两部分：线性模型 + DNN部分。结合上面两者的优点，平衡memorization和generalization。<br>原因：综合memorization和generalizatio的优点，服务于推荐系统。在本文的实验中相比于wide-only和deep-only的模型，wide &amp; deep提升显著。下图是模型整体结构：<br><img src="/2019/07/24/Wide-Deep/1.png"><br>可以看出，Wide也是一种特殊的神经网络，他的输入直接和输出相连，属于广义线性模型的范畴。Deep就是指Deep Neural Network，这个很好理解。Wide Linear Model用于memorization；Deep Neural Network用于generalization。<br>左侧是Wide-only，右侧是Deep-only，中间是Wide &amp; Deep。</p><h2 id="2-2、Cross-product-transformation"><a href="#2-2、Cross-product-transformation" class="headerlink" title="2.2、Cross-product transformation"></a><strong>2.2、Cross-product transformation</strong></h2><p>论文Wide中不断提到这样一种变换用来生成组合特征，这里很重要。它的定义如下：<br><img src="/2019/07/24/Wide-Deep/2.png"><br>其中k表示第k个组合特征。i表示输入X的第i维特征。C_ki表示这个第i维度特征是否要参与第k个组合特征的构造。d表示输入X的维度。到底有哪些维度特征要参与构造组合特征，这个是人工设定的（这也就是说需要人工特征工程），在公式中没有体现。</p><p>其实这么一个复杂的公式，就是我们之前一直在说的one-hot之后的组合特征：仅仅在输入样本X中的特征gender=female和特征language=en同时为1，新的组合特征AND(gender=female, language=en)才为1。所以只要把两个特征的值相乘就可以了。<br>（这样Cross-product transformation 可以在二值特征中学习到组合特征，并且为模型增加非线性）</p><h2 id="2-3、The-Wide-Component"><a href="#2-3、The-Wide-Component" class="headerlink" title="2.3、The Wide Component"></a>2.3、<strong>The Wide Component</strong></h2><p>如上面所说Wide Part其实是一个广义的线性模型。使用特征包括：</p><ul><li><p>raw input： 原始特征</p></li><li><p>cross-product transformation ：上面提到的组合特征</p></li></ul><p>用同一个例子来说明：你给model一个query（你想吃的美食），model返回给你一个美食，然后你购买/消费了这个推荐。 也就是说，推荐系统其实要学习的是这样一个条件概率： P(consumption | query, item)。<br>Wide Part可以对一些特例进行memorization。比如AND(query=”fried chicken”, item=”chicken fried rice”)虽然从字符角度来看很接近，但是实际上完全不同的东西，那么Wide就可以记住这个组合是不好的，是一个特例，下次当你再点炸鸡的时候，就不会推荐给你鸡肉炒米饭了。</p><h2 id="2-4、The-Deep-Component"><a href="#2-4、The-Deep-Component" class="headerlink" title="2.4、The Deep Component"></a>2.4、<strong>The Deep Component</strong></h2><p>如模型右边所示：Deep Part通过学习一个低纬度的dense representation（也叫做embedding vector）对于每一个query和item，来<strong>泛化</strong>给你推荐一些字符上看起来不那么相关，但是你可能也是需要的。比如说：你想要炸鸡，Embedding Space中，炸鸡和汉堡很接近，所以也会给你推荐汉堡。</p><p>Embedding vectors被随机初始化，并根据最终的loss来反向训练更新。这些低维度的dense embedding vectors被作为第一个隐藏层的输入。隐藏层的激活函数通常使用ReLU。</p><h1 id="3、模型训练"><a href="#3、模型训练" class="headerlink" title="3、模型训练"></a><strong>3、模型训练</strong></h1><p>训练中原始的稀疏特征，在两个组件中都会用到，比如query=”fried chicken” item=”chicken fried rice”:<br><img src="/2019/07/24/Wide-Deep/3.png"></p><p>在训练的时候，根据最终的loss计算出gradient，反向传播到Wide和Deep两部分中，分别训练自己的参数。也就是说，<strong>两个模块是一起训练的</strong>（也就是论文中的联合训练），注意这不是模型融合。</p><ul><li><p>Wide部分中的组合特征可以<strong>记住</strong>那些稀疏的，特定的rules</p></li><li><p>Deep部分通过Embedding来<strong>泛化</strong>推荐一些相似的items</p></li></ul><p>Wide模块通过组合特征可以很效率的学习一些特定的组合，但是这也导致了他并不能学习到训练集中没有出现的组合特征。所幸，Deep模块弥补了这个缺点。<br>另外，因为是一起训练的，wide和deep的size都减小了。wide组件只需要填补deep组件的不足就行了，所以需要比较少的cross-product feature transformations，而不是full-size wide Model。<br>具体的训练方法和实验请参考原论文。</p><h1 id="4、总结"><a href="#4、总结" class="headerlink" title="4、总结"></a><strong>4、总结</strong></h1><p>缺点：Wide部分还是需要人工特征工程。<br>优点：实现了对memorization和generalization的统一建模。能同时学习低阶和高阶组合特征</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在读了FM和FNN/PNN的论文后，来学习一下16年的一篇Google的论文，文章将传统的LR和DNN组合构成一个wide&amp;amp;deep模型（并行结构），既保留了LR的拟合能力，又具有DNN的泛化能力，并且不需要单独训练模型，可以方便模型的迭代，一起来看下吧。&lt;br&gt;原文：《Wide &amp;amp; Deep Learning for Recommender Systems》&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="ctr" scheme="https://jesse-csj.github.io/categories/ctr/"/>
    
    
      <category term="推荐系统" scheme="https://jesse-csj.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
      <category term="DNN" scheme="https://jesse-csj.github.io/tags/DNN/"/>
    
  </entry>
  
  <entry>
    <title>（读论文）推荐系统之ctr预估-PNN模型解析</title>
    <link href="https://jesse-csj.github.io/2019/07/22/PNN-new/"/>
    <id>https://jesse-csj.github.io/2019/07/22/PNN-new/</id>
    <published>2019-07-22T12:18:19.000Z</published>
    <updated>2019-08-27T09:38:22.272Z</updated>
    
    <content type="html"><![CDATA[<p>今天第二篇，还是之前的经典论文（PNN）还是基于DNN的深度模型用于预测点击率，不过相比于FNN提出了不少新的idea，一起来看下吧。</p><p>原论文：《 Product-based Neural Networks for User Response Prediction 》</p><a id="more"></a><p>地址：<a href="https://arxiv.org/pdf/1611.00144.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1611.00144.pdf</a></p><h1 id="1、原理"><a href="#1、原理" class="headerlink" title="1、原理"></a><strong>1、原理</strong></h1><p>给大家举例一个直观的场景：比如现在有一个凤凰网站，网站上面有一个迪斯尼广告，那我们现在想知道用户进入这个网站之后会不会有兴趣点击这个广告，类似这种用户点击率预测在信息检索领域就是一个非常核心的问题。普遍的做法就是通过不同的域来描述这个事件然后预测用户的点击行为，而这个域可以有很多。那么什么样的用户会点击这个广告呢？我们可能猜想：目前在上海的年轻的用户可能会有需求，如果今天是周五，看到这个广告，可能会点击这个广告为周末做活动参考。那可能的特征会是：[Weekday=Friday, occupation=Student, City=Shanghai]，当这些特征同时出现时，我们认为这个用户点击这个迪斯尼广告的概率会比较大。</p><p>传统的做法是应用One-Hot Binary的编码方式去处理这类数据，例如现在有三个域的数据X=[Weekday=Wednesday, Gender=Male, City=Shanghai],其中 Weekday有7个取值，我们就把它编译为7维的二进制向量，其中只有Wednesday是1，其他都是0，因为它只有一个特征值；Gender有两维，其中一维是1；如果有一万个城市的话，那City就有一万维，只有上海这个取值是1，其他是0。</p><p>那最终就会得到一个高维稀疏向量。但是这个数据集不能直接用神经网络训练：如果直接用One-Hot Binary进行编码，那输入特征至少有一百万，第一层至少需要500个节点，那么第一层我们就需要训练5亿个参数，那就需要20亿或是50亿的数据集，而要获得如此大的数据集基本上是很困难的事情。</p><p><strong>回顾FM、FNN模型</strong></p><p>因为上述原因需要将非常大的特征向量嵌入到低维向量空间中来减小模型复杂度，而FM（Factorisation machine）是很有效的embedding model：<br>                              <img src="/2019/07/22/PNN-new/1.jpg"></p><p>第一部分仍然为Logistic Regression，第二部分是通过两两向量之间的点积来判断特征向量之间和目标变量之间的关系。比如上述的迪斯尼广告，occupation=Student和City=Shanghai这两个向量之间的角度应该小于90，它们之间的点积应该大于0，说明和迪斯尼广告的点击率是正相关的。这种算法在推荐系统领域应用比较广泛。</p><p>那就基于这个模型来考虑神经网络模型，其实这个模型本质上就是一个三层网络：</p><img src="/2019/07/22/PNN-new/19.png"><p>它在第二层对向量做了乘积处理（比如上图蓝色节点直接为两个向量乘积，其连接边上没有参数需要学习），每个field都只会被映射到一个low-dimensional vector，field和field之间没有相互影响，那么第一层就被大量降维，之后就可以在此基础上应用神经网络模型。</p><p>如用FM算法对底层field进行embeddding，在此基础上面建模就是FNN(Factorisation-machinesupported Neural Networks)模型:</p><img src="/2019/07/22/PNN-new/20.png"><p>那现在进一步考虑FNN与一般的神经网络的区别是什么？大部分的神经网络模型对向量之间的处理都是采用加法操作，而FM 则是通过向量之间的乘法来衡量两者之间的关系。我们知道乘法关系其实相当于逻辑“且”的关系，拿上述例子来说，只有特征是学生而且在上海的人才有更大的概率去点击迪斯尼广告。但是加法仅相当于逻辑中“或”的关系，显然“且”比“或”更能严格区分目标变量。（加法就是正常拼接后作为输出，这里就是先做乘积再拼接作为DNN的输入）</p><p>所以我们接下来的工作就是对乘法关系建模。可以对两个向量做内积和外积的乘法操作，在此基础之上我们搭建的神经网络PNN：提出了一种product layer的思想，既基于乘法的运算来体现特征交叉的DNN网络结构，如下图：</p> <img src="/2019/07/22/PNN-new/2.jpg"><p>按照论文的思路，从上往下来看这个网络结构：</p><p>输出层 输出层很简单，将上一层的网络输出通过一个全链接层，经过sigmoid函数转换后映射到(0,1)的区间中，得到我们的点击率的预测值：</p><p>​                <img src="/2019/07/22/PNN-new/3.jpg"><br><strong>l2层</strong></p><p>根据l1层的输出，经一个全链接层 ，并使用relu进行激活，得到我们l2的输出结果：</p> <img src="/2019/07/22/PNN-new/4.jpg"><p><strong>l1层</strong> </p><p>l1层的输出由如下的公式计算：</p> <img src="/2019/07/22/PNN-new/5.jpg"><p>可以看到在得到l1层输出时，这里输入了三部分，分别是lz，lp 和 b1，b1是偏置项，这里可以先不管。lz和lp的计算就是PNN的重点所在了。</p><p><strong>Product Layer</strong></p><p>product思想来源于，在ctr预估中，认为特征之间的关系更多是一种and“且”的关系，而非add”或”的关系。例如，性别为男且喜欢游戏的人群，比起性别男和喜欢游戏的人群，前者的组合比后者更能体现特征交叉的意义。</p><p>product layer可以分成两个部分，一部分是线性部分lz，一部分是非线性部分lp。二者的形式如下：</p> <img src="/2019/07/22/PNN-new/6.jpg"><p>在这里，我们要使用到论文中所定义的一种运算方式，其实就是矩阵的点乘（对应位置相乘然后求和，最终得到的是一个标量）:</p> <img src="/2019/07/22/PNN-new/7.jpg"><p>这里先继续介绍网络结构，下一章中详细介绍Product Layer。</p><p><strong>Embedding Layer</strong></p><p>Embedding Layer跟DeepFM中相同，将每一个field的特征转换成同样长度的向量，这里用f来表示。</p><img src="/2019/07/22/PNN-new/18.png"><p><strong>损失函数</strong> </p><p>损失函数使用交叉熵：</p> <img src="/2019/07/22/PNN-new/8.jpg"><h1 id="2、Product-Layer详细介绍"><a href="#2、Product-Layer详细介绍" class="headerlink" title="2、Product Layer详细介绍"></a>2、Product Layer详细介绍</h1><p>前面提到了，product layer可以分成两个部分，一部分是线性部分lz，一部分是非线性部分lp。它们同维度，其具体形式如下：</p><p><img src="https://pic2.zhimg.com/80/v2-37e77e50b3a109ed916b6396046b55c0_hd.png" alt></p><p>其中z,p为信号向量，z为线性信号向量，p为二次信号向量,$W_{z}^{i},W_{p}^{i}$为权重矩阵(权重矩阵与z,p同维，经过定义的这种点乘运算后都得到一个标量作为DNN的输入）。</p><p>看上面的公式，我们首先需要知道z和p，这都是由我们的embedding层得到的，其中z是线性信号向量，因此我们直接用embedding层得到：</p> <img src="/2019/07/22/PNN-new/9.jpg"><p>论文中使用的等号加一个三角形，其实就是相等的意思，可以认为z就是embedding层的复制。</p><p>对于p来说，这里需要一个公式进行映射：</p> <img src="/2019/07/22/PNN-new/10.jpg"><p>不同的g函数的选择使得我们有了两种PNN的计算方法，一种叫做Inner PNN，简称IPNN，一种叫做Outer PNN，简称OPNN。</p><p>接下来分别来具体介绍这两种形式的PNN模型，由于涉及到复杂度的分析，所以我们这里先定义Embedding的大小为M，field的大小为N，而lz和lp的长度为D1。</p><h2 id="2-1-IPNN"><a href="#2-1-IPNN" class="headerlink" title="2.1 IPNN"></a>2.1 IPNN</h2><p>IPNN中p的计算方式如下，即使用内积来代表pij：</p> <img src="/2019/07/22/PNN-new/11.jpg"><p>所以，pij其实是一个数（标量），得到一个pij的时间复杂度为M，p的大小为N×N，因此计算得到p的时间复杂度为N×N×M。而再由p得到lp的时间复杂度是N×N×D1。因此 对于IPNN来说，总的时间复杂度为N×N(D1+M)。文章对这一结构进行了优化，可以看到，我们的p是一个对称矩阵，因此我们的权重也是一个对称矩阵，对这个对称矩阵进行如下的分解：</p> <img src="/2019/07/22/PNN-new/17.png"><p>因此：</p> <img src="/2019/07/22/PNN-new/12.jpg"><p>因此：</p> <img src="/2019/07/22/PNN-new/13.jpg"><h2 id="2-2-OPNN"><a href="#2-2-OPNN" class="headerlink" title="2.2 OPNN"></a>2.2 OPNN</h2><p>OPNN中p的计算方式如下：</p><p><img src="https://pic2.zhimg.com/80/v2-6d6e9313125ef15bfcbefa0b42596f86_hd.png" alt></p><p>此时pij为M×M的矩阵，计算一个pij的时间复杂度为M×M，而p是N×N×M×M的矩阵，因此计算p的事件复杂度为N×N×M×M。从而计算lp的时间复杂度变为D1×N×N×M×M。这个显然代价很高的。为了减少复杂度，论文使用了叠加的思想，它重新定义了p矩阵：</p><p>通过元素相乘的叠加，也就是先叠加N个field的Embedding向量，然后做乘法，可以大幅减少时间复杂度，定义p为：</p> <img src="/2019/07/22/PNN-new/14.jpg"><p>这里计算p的时间复杂度就变为了D1×M×(M+N)。</p><h1 id="3、Discussion"><a href="#3、Discussion" class="headerlink" title="3、Discussion"></a>3、Discussion</h1><p>和FNN相比，PNN多了一个product层，和FM相比，PNN多了隐层，并且输出不是简单的叠加；在训练部分，可以单独训练FNN或者FM部分作为初始化，然后BP算法应用整个网络，那么至少效果不会差于FNN和FM；</p><h1 id="4、Experiments"><a href="#4、Experiments" class="headerlink" title="4、Experiments"></a>4、Experiments</h1><p>使用Criteo和iPinYou的数据集，并用SGD算法比较了7种模型：LR、FM、FNN、CCPM、IPNN、OPNN、PNN（拼接内积和外积层），正则化部分（L2和Dropout）；</p><p>实验结果如下图所示：</p> <img src="/2019/07/22/PNN-new/15.jpg"><p>结果表明PNN提升还是蛮大的；这里介绍一下关于激活函数的选择问题，作者进行了对比如下：</p> <img src="/2019/07/22/PNN-new/16.jpg"><p>从图中看出，anh在某些方面要优于relu，但作者采用的是relu，relu的作用： 1、稀疏的激活函数（负数会被丢失）；2、有效的梯度传播（缓解梯度消失和梯度爆炸）；3、有效的计算（仅有加法、乘法、比较操作）。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;今天第二篇，还是之前的经典论文（PNN）还是基于DNN的深度模型用于预测点击率，不过相比于FNN提出了不少新的idea，一起来看下吧。&lt;/p&gt;
&lt;p&gt;原论文：《 Product-based Neural Networks for User Response Prediction 》&lt;/p&gt;
    
    </summary>
    
      <category term="ctr" scheme="https://jesse-csj.github.io/categories/ctr/"/>
    
    
      <category term="推荐系统" scheme="https://jesse-csj.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
      <category term="DNN" scheme="https://jesse-csj.github.io/tags/DNN/"/>
    
  </entry>
  
  <entry>
    <title>（读论文）推荐系统之ctr预估-FNN模型解析</title>
    <link href="https://jesse-csj.github.io/2019/07/21/FNN/"/>
    <id>https://jesse-csj.github.io/2019/07/21/FNN/</id>
    <published>2019-07-21T08:01:40.000Z</published>
    <updated>2019-08-27T09:01:00.740Z</updated>
    
    <content type="html"><![CDATA[<h1 id><a href="#" class="headerlink" title=" "></a> </h1><p>今天要介绍的论文也是之前看到的一篇经典的推荐相关的论文（FNN），最近要快点更新啊，要赶上最新看的进度。</p><p>原论文：《Deep learning over multi-field categorical data》</p><a id="more"></a><p>地址：<a href="https://arxiv.org/pdf/1601.02376.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1601.02376.pdf</a></p><h1 id="1、问题由来"><a href="#1、问题由来" class="headerlink" title="1、问题由来"></a><strong>1、问题由来</strong></h1><p>基于传统机器学习模型（如LR、FM等）的CTR预测方案又被称为基于浅层模型的方案，其优点是模型简单，预测性能较好，可解释性强；缺点主要在于很难自动提取高阶组合特征携带的信息，目前一般通过特征工程来手动的提取高阶组合特征。而随着深度学习在计算机视觉、语音识别、自然语言处理等领域取得巨大成功，其在探索特征间高阶隐含信息的能力也被应用到了CTR预测中。较早有影响力的基于深度学习模型的CTR预测方案是在2016年提出的基于因子分解机的神经网络(Factorization Machine supported Neural Network, FNN)模型，就是我们今天要分享的内容，一起来看下。</p><h1 id="2、模型"><a href="#2、模型" class="headerlink" title="2、模型"></a><strong>2、模型</strong></h1><p> FNN模型如下图所示：</p><img src="/2019/07/21/FNN/1.png"><img src="/2019/07/21/FNN/4.png"><p>（FM的详细解释可看我上一篇<a href="https://zhuanlan.zhihu.com/p/74337279" target="_blank" rel="noopener">文章</a>）：</p><img src="/2019/07/21/FNN/2.png"><p>我们可以看出这个模型有着十分显著的特点：<br>　　1. 采用FM预训练得到的隐含层及其权重作为神经网络的第一层的初始值，之后再不断堆叠全连接层，最终输出预测的点击率。 　　　<br>　　2. 可以将FNN理解成一种特殊的embedding+MLP，其要求第一层嵌入后的各领域特征维度一致，并且嵌入权重的初始化是FM预训练好的。 　　　　<br>　　3. 这不是一个端到端的训练过程，有贪心训练的思路。而且如果不考虑预训练过程，模型网络结构也没有考虑低阶特征组合。</p><p>　  为了方便理解，如下图所示，FNN = FM + MLP <strong>，相当于用FM模型得到了每一维特征的嵌入向量，做了一次特征工程，得到特征送入分类器，不是端到端的思路，有贪心训练的思路。</strong></p><img src="/2019/07/21/FNN/3.png"><h1 id="3、FNN的优缺点"><a href="#3、FNN的优缺点" class="headerlink" title="3、FNN的优缺点"></a><strong>3、FNN的优缺点</strong></h1><p><strong>优点</strong>：每个特征的嵌入向量是预先采用FM模型训练的，因此在学习DNN模型时，训练开销降低，模型能够更快达到收敛。</p><p><strong>缺点：</strong></p><ol><li>Embedding 的参数受 FM 的影响，不一定准确。</li><li>预训练阶段增加了计算复杂度，训练效率低。</li><li>FNN 只能学习到高阶的组合特征；模型中没有对低阶特征建模。</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h1 id&gt;&lt;a href=&quot;#&quot; class=&quot;headerlink&quot; title=&quot; &quot;&gt;&lt;/a&gt; &lt;/h1&gt;&lt;p&gt;今天要介绍的论文也是之前看到的一篇经典的推荐相关的论文（FNN），最近要快点更新啊，要赶上最新看的进度。&lt;/p&gt;
&lt;p&gt;原论文：《Deep learning over multi-field categorical data》&lt;/p&gt;
    
    </summary>
    
      <category term="ctr" scheme="https://jesse-csj.github.io/categories/ctr/"/>
    
    
      <category term="推荐系统" scheme="https://jesse-csj.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
      <category term="DNN" scheme="https://jesse-csj.github.io/tags/DNN/"/>
    
  </entry>
  
  <entry>
    <title>（读论文）推荐系统之ctr预估-FM算法解析</title>
    <link href="https://jesse-csj.github.io/2019/07/20/%EF%BC%88%E8%AF%BB%E8%AE%BA%E6%96%87%EF%BC%89%E8%AE%A1%E7%AE%97%E5%B9%BF%E5%91%8A%E4%B9%8BCTR%E9%A2%84%E4%BC%B0-FM%E7%AE%97%E6%B3%95%E8%A7%A3%E6%9E%90/"/>
    <id>https://jesse-csj.github.io/2019/07/20/（读论文）计算广告之CTR预估-FM算法解析/</id>
    <published>2019-07-20T08:01:40.000Z</published>
    <updated>2019-08-27T09:00:38.209Z</updated>
    
    <content type="html"><![CDATA[<p> 大家好，我是csj，这是我的第一篇个人博客，以一篇经典的论文FM开始吧：<br>  原文：《Factorization Machines》</p><a id="more"></a><p>  地址：<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.393.8529&amp;rep=rep1&amp;type=pdf" target="_blank" rel="noopener">http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.393.8529&amp;rep=rep1&amp;type=pdf</a></p><h1 id="一、问题由来"><a href="#一、问题由来" class="headerlink" title="一、问题由来"></a><strong>一、问题由来</strong></h1><p>在计算广告和推荐系统中，CTR预估(click-through rate)是非常重要的一个环节，判断一个商品的是否进行推荐需要根据CTR预估的点击率来进行。传统的逻辑回归模型是一种广义线性模型，非常容易实现大规模实时并行处理，因此在工业界获得了广泛应用，但是线性模型的学习能力有限，不能捕获高阶特征(非线性信息)，而在进行CTR预估时，除了单特征外，往往要对特征进行组合。对于特征组合来说，业界现在通用的做法主要有两大类：FM系列与DNN系列。今天，我们就来分享下FM算法。</p><h1 id="二、为什么需要FM"><a href="#二、为什么需要FM" class="headerlink" title="二、为什么需要FM"></a><strong>二、为什么需要FM</strong></h1><p>1、特征组合是许多机器学习建模过程中遇到的问题，如果对特征直接建模，很有可能会忽略掉特征与特征之间的关联信息，因此，可以通过构建新的交叉特征这一特征组合方式提高模型的效果。</p><p>2、高维的稀疏矩阵是实际工程中常见的问题，并直接会导致计算量过大，特征权值更新缓慢。试想一个10000<em>100的表，每一列都有8种元素，经过one-hot独热编码之后，会产生一个10000</em>800的表。因此表中每行元素只有100个值为1，700个值为0。特征空间急剧变大，以淘宝上的item为例，将item进行one-hot编码以后，样本空间有一个categorical变为了百万维的数值特征，特征空间一下子暴增一百万。所以大厂动不动上亿维度，就是这么来的。</p><p>而FM的优势就在于对这两方面问题的处理。首先是特征组合，通过对两两特征组合，引入交叉项特征，提高模型得分；其次是高维灾难，通过引入隐向量（对参数矩阵进行矩阵分解），完成对特征的参数估计。</p><h1 id="三、原理及求解"><a href="#三、原理及求解" class="headerlink" title="三、原理及求解"></a><strong>三、原理及求解</strong></h1><p>在看FM算法前，我们先回顾一下最常见的线性表达式：<br>                   <img src="/2019/07/20/（读论文）计算广告之CTR预估-FM算法解析/1.png"><br>其中w0 为初始权值，或者理解为偏置项，wi 为每个特征xi 对应的权值。可以看到，这种线性表达式只描述了每个特征与输出的关系。</p><p>FM的表达式如下，可观察到，只是在线性表达式后面加入了新的交叉项特征及对应的权值。</p><p><img src="//jesse-csj.github.io/2019/07/20/（读论文）计算广告之CTR预估-FM算法解析/1473228-20180904202908102-215826983.png" alt></p><p>  <strong>求解过程 ：　</strong></p><p>从上面的式子可以很容易看出，组合部分的特征相关参数共有n(n−1)/2个。但是如第二部分所分析，在数据很稀疏的情况下，满足xi,xj都不为0的情况非常少，这样将导致ωij无法通过训练得出。</p><p>为了求出ωij，我们对每一个特征分量xi引入辅助向量Vi=(vi1,vi2,⋯,vik)。然后，利用vivj^T对ωij进行求解：</p><p><img src="//jesse-csj.github.io/2019/07/20/（读论文）计算广告之CTR预估-FM算法解析/1630213-20190710174305361-1336985179.png" alt></p><p>那么ωij组成的矩阵可以表示为:</p><p><img src="//jesse-csj.github.io/2019/07/20/（读论文）计算广告之CTR预估-FM算法解析/1630213-20190710174342426-2098941787.png" alt></p><p>那么，如何求解vi和vj呢？主要采用了公式：</p><p><img src="//jesse-csj.github.io/2019/07/20/（读论文）计算广告之CTR预估-FM算法解析/1630213-20190710174408535-2047182456.png" alt></p><p>具体推导过程如下：（重要的化简过程）</p><p><img src="//jesse-csj.github.io/2019/07/20/（读论文）计算广告之CTR预估-FM算法解析/1630213-20190710174437368-117236465.png" alt></p><h1 id="四、参数求解"><a href="#四、参数求解" class="headerlink" title="四、参数求解"></a><strong>四、参数求解</strong></h1><p>利用梯度下降法，通过求损失函数对特征（输入项）的导数计算出梯度，从而更新权值。设m为样本个数，θ为权值。</p><img src="/2019/07/20/（读论文）计算广告之CTR预估-FM算法解析/7.png" title="更新参数"><p>每个梯度都可以在O(1)时间内求得，整体的参数更新的时间为O(kn)。</p><p>第一篇博客就到此结束啦~ 之后会继续分享计算广告相关的论文和知识。<br>有关FM或其他推荐模型的小demo可以在我的<a href="https://github.com/Jesse-csj/TensorFlow_Practice" target="_blank" rel="noopener">github</a>上找到，欢迎大家star~</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt; 大家好，我是csj，这是我的第一篇个人博客，以一篇经典的论文FM开始吧：&lt;br&gt;  原文：《Factorization Machines》&lt;/p&gt;
    
    </summary>
    
      <category term="ctr" scheme="https://jesse-csj.github.io/categories/ctr/"/>
    
    
      <category term="推荐系统" scheme="https://jesse-csj.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
      <category term="FM" scheme="https://jesse-csj.github.io/tags/FM/"/>
    
  </entry>
  
</feed>
