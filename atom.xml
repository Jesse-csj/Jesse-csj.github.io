<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Jesse&#39;s Blog</title>
  
  <subtitle>直落夜深花睡去，临风春华便思君。</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://jesse-csj.github.io/"/>
  <updated>2019-08-27T03:31:31.363Z</updated>
  <id>https://jesse-csj.github.io/</id>
  
  <author>
    <name>Jesse_jia</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>（读论文）推荐系统之ctr预估-MLR(LS-PLM)模型解析</title>
    <link href="https://jesse-csj.github.io/2019/08/12/MLR/"/>
    <id>https://jesse-csj.github.io/2019/08/12/MLR/</id>
    <published>2019-08-12T02:11:19.000Z</published>
    <updated>2019-08-27T03:31:31.363Z</updated>
    
    <content type="html"><![CDATA[<p>本人才疏学浅，不足之处欢迎大家指出和交流。</p><p>文有配图… 迟更几天，今天继续带来传统模型之MLR算法模型，这是一篇来自阿里盖坤团队的方案（LS-PLM），发表于2017年，但实际在2012年就已经提出并应用于实际业务中（膜拜ing），当时主流仍然是我们上一篇提到过的的LR模型，而本文作者创新性地提出了MLR(mixed logistic regression, 混合逻辑斯特回归)算法，引领了广告领域CTR预估算法的全新升级。总的来说，MLR算法创新地提出并实现了直接在原始空间学习特征之间的非线性关系，基于数据自动发掘可推广的模式，相比于人工来说效率和精度均有了大幅提升。下面我们一起来学习下吧。</p><p>原文：《Learning Piece-wise Linear Models from Large Scale Data for Ad Click Prediction》</p><p>地址：<a href="https://arxiv.org/abs/1704.05194" target="_blank" rel="noopener">https://arxiv.org/abs/1704.05194</a></p><h1 id="1、背景"><a href="#1、背景" class="headerlink" title="1、背景"></a>1、背景</h1><p><br></p><p>CTR预估(click-through-rate prediction)是广告行业比较常见的问题，根据用户的历史行为来判断用户对广告点击的可能性。在常见工业场景中，该问题的输入往往是数以万计的稀疏特征向量，在进行特征交叉后会维数会更高，比较常见的就是采用逻辑回归模型加一些正则化，因为逻辑回归模型计算开销小且容易实现并行。之前提到的facebook的一篇论文<a href="https://zhuanlan.zhihu.com/p/76794626" target="_blank" rel="noopener">（LR+GBDT）</a>中先用树模型做分类之后再加一个逻辑回归模型，最后得出效果出奇的好，应该也是工业界比较常用的方法，同时树模型的选择或者说是再构造特征的特性也逐渐被大家所关注。另一种比较有效的就是因子分解模型系列，包括FM及其的其他变种，它们的主要思想就是构造交叉特征或者是二阶的特征来一起进行训练。</p><p>这篇文章中，作者主要提出了一种piece-wise的线性模型，并且给出了其在大规模数据上的训练算法，称之为LS-PLM(Large Scale Piecewise Linear Model)，LS-PLM采用了分治的思想，先分成几个局部再用线性模型拟合，这两部都采用监督学习的方式，来优化总体的预测误差，总的来说有以下优势：</p><ul><li><p><strong>端到端的非线性学习</strong>： 从模型端自动挖掘数据中蕴藏的非线性模式，省去了大量的人工特征设计，这 使得MLR算法可以端到端地完成训练，在不同场景中的迁移和应用非常轻松。通过分区来达到拟合非线性函数的效果；</p></li><li><p><strong>可伸缩性（scalability）</strong>：与逻辑回归模型相似，都可以很好的处理复杂的样本与高维的特征，并且做到了分布式并行；</p></li><li><p><strong>稀疏性</strong>: 对于在线学习系统，模型的稀疏性较为重要，所以采用了$L_{1}$和\(L_{2,1}\)正则化，模型的学习和在线预测性能更好。当然，目标函数非凸非光滑为算法优带来了新的挑战。</p></li></ul><h1 id="2、MLR总览"><a href="#2、MLR总览" class="headerlink" title="2、MLR总览"></a>2、MLR总览</h1><p><br></p><p>MLR就像它的名字一样，由很多个LR模型组合而成。用分片线性模式来拟合高维空间的非线性模式，形式化表述如下：</p><img src="/2019/08/12/MLR/1.png"> <p>给定样本x，模型的预测p(y|x)分为两部分：首先根据<br>\(\sigma（u_{j}^{T}x）\)分割特征空间为m部分，其中m为给定的超参数，然后对于各部分计算\(\eta（w_{j}^{T}x）\)作为各部分的预测。函数g(⋅)确保了我们的模型满足概率函数的定义。当我们将softmax函数作为分割函数σ(x)，将sigmoid函数作为拟合函数η(x)的时候，该模型为：</p><img src="/2019/08/12/MLR/2.png"> <p>此时我们的混合模型可以看做一个FOE模型：</p><img src="/2019/08/12/MLR/3.png"> <p>目标损失函数为：</p><img src="/2019/08/12/MLR/4.png"> <p>论文中一个直观的例子，<strong>如下图，LR不能拟合非线性数据，MLR可以拟合非线性数据，因为划分-训练模式</strong>。</p><img src="/2019/08/12/MLR/5.png"> <p>这种菱形分界面（非线性数据）其实非常难学，但MLR在其中表现出色。通过控制分片数量m，可以平衡模型的拟合能力和过拟合。上图m=4。论文中m=12得到了不错的效果。</p><p>理论上来说，增大m可以带来无限制的非线性拟合能力，但是同样会增加计算、存储的开销，同时会带来过拟合的风险。具体如何选取m要结合实际情况取舍；</p><p>同时MLR还引入了<strong>结构化先验、分组稀疏、线性偏置、模型级联、增量训练、Common Feature Trick</strong>来提升模型性能。</p><p>针对MLR上面提到的各种特性，下面我们一一来介绍细节：</p><h1 id="3、模型细节"><a href="#3、模型细节" class="headerlink" title="3、模型细节"></a>3、模型细节</h1><p><br></p><h2 id="3-1-结构化先验"><a href="#3-1-结构化先验" class="headerlink" title="3.1 结构化先验"></a>3.1 结构化先验</h2><p>MLR中非常重要的就是如何划分原始特征空间。</p><p>通过引入结构化先验，我们使用用户特征来划分特征空间，使用广告特征来进行基分类器的训练，减小了模型的探索空间，收敛更容易。</p><p>同时，这也是符合我们认知的：不同的人群具有聚类特性，同一类人群具有类似的广告点击偏好。</p><h2 id="3-2-线性偏置"><a href="#3-2-线性偏置" class="headerlink" title="3.2 线性偏置"></a>3.2 线性偏置</h2><p>针对CTR预估问题中存在的两种偏置：</p><ul><li><p><strong>Position Bias</strong>：排名第1位和第5位的样本，点击率天然存在差异。宝贝展示的页面、位置影响点击率</p></li><li><p><strong>Sample Bias</strong>：PC和Mobile上的样本，点击率天然存在差异。</p></li></ul><p>在原来宝贝特征x的基础上，增加偏移向量y(场景、页数、位置等)。如果直接学习联合概率P(X,Y)面临问题：学习联合概率一定需要x和y的大部分组合，但是实际情况，并不是所有的x，y的组合都能有采样。针对这个问题，提出了带偏移MLR算法，形式化表述如下：</p><img src="/2019/08/12/MLR/6.png"> <p>而且，大规模非线性CTR预估和偏移变量的分解一起优化。并且，只需要很少的一些x，y组合就可以了。从论文给出的数据中，AUC提高了2-8个百分点。</p><h2 id="3-3-模型级联"><a href="#3-3-模型级联" class="headerlink" title="3.3 模型级联"></a>3.3 模型级联</h2><p>盖坤在PPT讲解到，MLR支持与LR的级联式训练。有点类似于Wide &amp; Deep，一些强Feature配置成级联形式能够提高模型的收敛性。例如典型的应用方法是：以统计反馈类特征构建第一层模型，输出FBctr级联到第二级大规模稀疏ID特征中去，能得到更好的提升效果。</p><img src="/2019/08/12/MLR/7.png"> <p>反馈特征常用的如反馈CTR，是指系统上线一段时间之后得到的历史CTR值。</p><h2 id="3-4-增量训练"><a href="#3-4-增量训练" class="headerlink" title="3.4 增量训练"></a>3.4 增量训练</h2><p>实验证明，MLR利用结构先验（用户特征进行聚类，广告特征进行分类）进行pretrain，然后再增量进行全空间参数寻优训练，会使得收敛步数更少，收敛更稳定。</p><img src="/2019/08/12/MLR/8.jpg"> <h1 id="4、一些trick"><a href="#4、一些trick" class="headerlink" title="4、一些trick"></a>4、一些trick</h1><p><br></p><p>论文的idea简单有效的，重点是工程中出来的论文，对工程实现上的优化细节都很详细，下面我们来看下：</p><h2 id="4-1-并行化"><a href="#4-1-并行化" class="headerlink" title="4.1 并行化"></a>4.1 并行化</h2><p>论文里的实现基于分布式，包括两个维度的并行化，模型并行化，数据并行化。每一个计算节点中都包含两种角色：<strong>Server Node, Worker Node</strong>，这样做的好处有两点：</p><ul><li><p>最大化利用CPU计算资源。之前大多数Server Node单独放到一台服务器上，造成CPU资源的极大浪费。</p></li><li><p>最大化利用Memory资源。</p></li></ul><img src="/2019/08/12/MLR/9.jpg"> <h2 id="4-2-Common-Feature-Trick"><a href="#4-2-Common-Feature-Trick" class="headerlink" title="4.2 Common Feature Trick"></a>4.2 Common Feature Trick</h2><img src="/2019/08/12/MLR/10.jpg"> <p>一个用户在一次pageview中会看到多个广告，每个广告都组成一条样本。所以这些样本之间很多特征都是重复的。这些特征包括：用户特征（年龄、性别等）、用户的历史访问信息（之前购买的物品、喜欢的店铺等）。那么我们对于向量内积的计算分成两部分：<strong>common和non-common parts:</strong></p><img src="/2019/08/12/MLR/11.jpg"> <p>利用<strong>Common Feature Trick</strong>可以从三个方面来优化并行化：</p><ul><li><p>对于有Common Feature的样本作为一组一起训练，并保证在存储在一个worker上</p></li><li><p>对于Common Feature仅仅保存一次，以便来节省内存</p></li><li><p>对于Common Feature的loss和梯度更新只需要一次即可</p></li></ul><p>下面是实验结果：</p><img src="/2019/08/12/MLR/12.jpg"> <p>可以看到Common Feature Trick效果还是非常明显的。</p><h1 id="五、总结"><a href="#五、总结" class="headerlink" title="五、总结"></a>五、总结</h1><p><br></p><p>文末说LS-PLM在2012年就应用于阿里巴巴的ctr预估，到2017年才发表论文。不出意料，现在的模型应该已经不再是MLR这么简单了。另外，从MLR和LR进行级联，以便加强强特征来看，MLR还是有很大的局限性。个人感觉模型理论上来说确实非常棒，利用分片线性来模型高维非线性，但是虽然取得了非常不错的成绩，但是带来的挑战也不小：比如初值问题、非凸问题的局部极值、虽然MLR比LR好，但不知道和全局最优相比还有多远；第二，在初值的Pre-train方面需要改进和优化模型函数等等；第三，目前规模化能力方面也需要能够吞吐更多特征和数据，比如采用更快的收敛算法等等；最后，整体的MLR算法的抽象能力也需进一步得到强化。</p><p><br><br><br><br>实现MLR的一个Demo，感兴趣的童鞋可以看下我的<a href="https://link.zhihu.com/?target=https%253A//github.com/Jesse-csj/TensorFlow_Practice" target="_blank" rel="noopener">[github]</a><br><br></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本人才疏学浅，不足之处欢迎大家指出和交流。&lt;/p&gt;
&lt;p&gt;文有配图… 迟更几天，今天继续带来传统模型之MLR算法模型，这是一篇来自阿里盖坤团队的方案（LS-PLM），发表于2017年，但实际在2012年就已经提出并应用于实际业务中（膜拜ing），当时主流仍然是我们上一篇提到
      
    
    </summary>
    
      <category term="ctr" scheme="https://jesse-csj.github.io/categories/ctr/"/>
    
    
      <category term="推荐系统" scheme="https://jesse-csj.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>（读论文）推荐系统之ctr预估-LR与GBDT+LR模型解析</title>
    <link href="https://jesse-csj.github.io/2019/08/05/LR/"/>
    <id>https://jesse-csj.github.io/2019/08/05/LR/</id>
    <published>2019-08-05T08:18:19.000Z</published>
    <updated>2019-08-07T07:00:32.303Z</updated>
    
    <content type="html"><![CDATA[<p>本人才疏学浅，不足之处欢迎大家指出和交流。</p><p>本系列接下来几天要分享的文章是关于线性模型的，在15年后深度模型在ctr预估领域百花齐放之时，仍觉得传统的LR模型有着基石的作用，于是有了今天的分享。话不多说，今天会介绍的是经典的LR模型及Facebook在2014年提出的GBDT+LR（重点）。当时深度学习几乎还没有应用到到计算广告领域，Facebook提出利用GBDT的叶节点编号作为非线性特征的表示，或者说是组合特征的一种方式，可以自动实现特征工程，下面我们一起来看看吧。</p><p>两篇原文：</p><ol><li><p><a href="http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=51313D6762FBE110533EDFDD7A47A8D9?doi=10.1.1.134.395&amp;rep=rep1&amp;type=pdf" target="_blank" rel="noopener">Predicting Clicks: Estimating the Click-Through Rate for New Ads</a></p></li><li><p><a href="http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=A54CCA7D4A8F05B6636C9D64316BCF96?doi=10.1.1.718.9050&amp;rep=rep1&amp;type=pdf" target="_blank" rel="noopener">Practical Lessons from Predicting Clicks on Ads at Facebook</a></p></li></ol><a id="more"></a><h1 id="1、背景"><a href="#1、背景" class="headerlink" title="1、背景"></a>1、背景</h1><p><br><br>这里是微软研究院在当时提出LR模型时的商业背景：搜索引擎主要靠商业广告收入，在广告位上面打广告，用户点击，之后广告商付费。在通用搜索引擎，通常广告位置是在搜索结果之前，或者在搜索结果右边，由此为查询选择正确的广告及其展示顺序会极大地影响用户看到并点击每个广告的概率。此排名对搜索引擎从广告中获得的收入产生了很大影响。此外，向用户展示他们喜欢点击的广告也会提高用户满意度。出于这些原因，能够准确估计系统中广告的点击率非常重要。（朴素的想法即是放用户可能点击的广告，并且放每次点击广告商付费多的广告）<br>于是就归结到点击率预估的<strong>模型选择和特征工程</strong>问题。<br><br></p><h1 id="2、LR-Model"><a href="#2、LR-Model" class="headerlink" title="2、LR Model"></a>2、LR Model</h1><h2 id="2-1-LR的数学基础"><a href="#2-1-LR的数学基础" class="headerlink" title="2.1 LR的数学基础"></a>2.1 LR的数学基础</h2><p><br><br>为何在2012年之前LR模型占据了计算广告领域的极大部分市场呢，我们可以从数学角度稍作分析：<br>逻辑回归（logistics regression）作为广义线性模型的一种，它的假设是因变量y服从<strong>伯努利分布</strong>。那么在点击率预估这个问题上，“点击”这个事件是否发生就是模型的因变量y。而用户是否点击广告这个问题是一个经典的掷偏心硬币（二分类）问题，因此CTR模型的因变量显然应该服从伯努利分布。所以采用LR作为CTR模型是符合“点击”这一事件的物理意义的。<br>在了解这一基础假设的情况下，再来看LR的数学形式就极具解释性了：<br><img src="/2019/08/05/LR/3.png"><br>其中x是输入向量，θ 是我们要学习的参数向量。结合CTR模型的问题来说，x就是输入的特征向量，h(x)就是我们最终希望得到的点击率。</p><h2 id="2-2、朴素的直觉和可解释性"><a href="#2-2、朴素的直觉和可解释性" class="headerlink" title="2.2、朴素的直觉和可解释性"></a>2.2、朴素的直觉和可解释性</h2><p><br><br>直观来讲，LR模型目标函数的形式就是各特征的加权和，最后再加以sigmoid函数。忽略其数学基础，仅靠我们的直觉认知也可以一定程度上得出使用LR作为CTR模型的合理性：<br><strong>使用各特征的加权和是为了综合不同特征对CTR的影响，而由于不同特征的重要程度不一样，所以为不同特征指定不同的权重来代表不同特征的重要程度。最后要加上sigmoid函数，是希望其值能够映射到0-1之间，使其符合CTR的概率意义。</strong></p><p>对LR的这一直观（或是主观）认识的另一好处就是模型具有极强的<strong>可解释性</strong>，算法工程师们可以轻易的解释哪些特征比较重要，在CTR模型的预测有偏差的时候，也可以轻易找到哪些因素影响了最后的结果。</p><h2 id="2-3、工程化的需要"><a href="#2-3、工程化的需要" class="headerlink" title="2.3、工程化的需要"></a>2.3、工程化的需要</h2><p><br><br>在工业界每天动辄TB级别的数据面前，模型的训练开销就异常重要了。在GPU尚未流行开来的2012年之前，LR模型也凭借其易于并行化、模型简单、训练开销小等特点占据着工程领域的主流。囿于工程团队的限制，即使其他复杂模型的效果有所提升，在没有明显beat LR之前，公司也不会贸然加大计算资源的投入升级CTR模型，这是LR持续流行的另一重要原因。<br><br></p><h1 id="3、GBDT-LR-Model"><a href="#3、GBDT-LR-Model" class="headerlink" title="3、GBDT+LR Model"></a>3、GBDT+LR Model</h1><p>这篇文章（原文2）主要介绍了CTR预估模型LR(Logistic Regression)+GBDT。当时深度学习还没有应用到计算广告领域，而在此之前为探索特征交叉而提出的FM（见本系列第一篇）和FFM（本系列忘写了…）虽然能够较好地解决数据稀疏性的问题，但他们仍停留在二阶交叉的情况。如果要继续提高特征交叉的维度，不可避免的会发生组合爆炸和计算复杂度过高等问题。在此基础上，2014年Facebook提出了基于GBDT+LR组合模型的解决方案。简而言之，Facebook提出了一种利用GBDT（Gradient Boosting Decision Tree）自动进行特征筛选和组合，进而生成新的离散特征向量，再把该特征向量当作LR模型输入，预估CTR的模型结构。随后Kaggle竞赛也有实践此思路，GBDT与LR融合开始引起了业界关注。</p><p>LR+GBDT相比于单纯的LR或者GBDT带来了较大的性能提升，论文中给出数据为3%，这在CTR预估领域确实非常不错。除此之外，Facebook还在在线学习、Data freshness、学习速率、树模型参数、特征重要度等方面进行了探索（本文不做讨论，有兴趣可参阅原文）。<strong>Online Learning</strong>是本文的一个重点，接下来我们会提到。</p><h2 id="3-1、GBDT"><a href="#3-1、GBDT" class="headerlink" title="3.1、GBDT"></a>3.1、GBDT</h2><p>GBDT 由多棵 CART 树组成，本质是多颗回归树组成的森林。每一个节点按贪心分裂，最终生成的树包含多层，这就相当于一个特征组合的过程。根据规则，样本一定会落在一个叶子节点上，将这个叶子节点记为1，其他节点设为0，得到一个向量。论文中如下图所示：有两棵树，第一棵树有三个叶子节点，第二棵树有两个叶子节点。如果一个样本落在第一棵树的第二个叶子，将它编码成 [0, 1, 0]。在第二棵树落到第一个叶子，编码成 [1, 0]。所以，输入到 LR 模型中的向量就是 [0, 1, 0, 1, 0]。</p><p>需要注意的一点是：利用 GBDT 模型进行自动特征组合和筛选是一个独立（独立于LR训练）的过程，于是乎这种方法的优点在于两个模型在训练过程中是独立进行的，不需要进行联合训练，自然也就不存在如何将LR的梯度回传到GBDT这类复杂的问题。<br><img src="/2019/08/05/LR/1.png"></p><p>然而，还有两个重要问题是为什么建树采用GBDT以及为什么要使用集成的决策树模型，而不是单棵的决策树模型呢？</p><p>对于第二个问题，我们可以认为一棵树的表达能力很弱，不足以表达多个有区分性的特征组合，多棵树的表达能力更强一些，可以更好的发现有效的特征和特征组合。</p><p>对于第一个问题，牵涉到GBDT原理问题，这里不做过多讨论，请参考两篇网上写的较好的文章：</p><ol><li><a href="https://www.cnblogs.com/peizhe123/p/5086128.html" target="_blank" rel="noopener">GBDT详解</a>。</li><li><a href="https://www.cnblogs.com/ModifyRong/p/7744987.html" target="_blank" rel="noopener">机器学习算法GBDT</a>。</li></ol><h2 id="3-2、GBDT-LR总结"><a href="#3-2、GBDT-LR总结" class="headerlink" title="3.2、GBDT+LR总结"></a>3.2、GBDT+LR总结</h2><p>由于决策树的结构特点，事实上，决策树的深度就决定了特征交叉的维度。如果决策树的深度为4，通过三次节点分裂，最终的叶节点实际上是进行了3阶特征组合后的结果，如此强的特征组合能力显然是FM系的模型不具备的。但由于GBDT容易产生过拟合，以及GBDT这种特征转换方式实际上丢失了大量特征的数值信息，因此我们不能简单说GBDT由于特征交叉的能力更强，效果就比FM或FFM好（事实上FFM是2015年提出的）。在模型的选择和调试上，永远都是多种因素综合作用的结果。</p><p>GBDT+LR比FM重要的意义在于，它大大推进了特征工程模型化这一重要趋势，某种意义上来说，之后深度学习的各类网络结构，以及embedding技术的应用，都是这一趋势的延续。</p><h1 id="4、总结（具体的对比实验和实现细节等请参阅原论文）"><a href="#4、总结（具体的对比实验和实现细节等请参阅原论文）" class="headerlink" title="4、总结（具体的对比实验和实现细节等请参阅原论文）"></a>4、总结（具体的对比实验和实现细节等请参阅原论文）</h1><p>本次简要介绍了传统的LR模型，其优点和缺点都非常突出，但不可否认的是其基石般的作用和地位。同时我们也介绍了GBDT+LR的改进方案以解决特征组合的问题，但GBDT也存在很容易过拟合的问题，接下来这段话摘自于知乎：</p><img src="/2019/08/05/LR/2.png"><p>这里是引用阿里的盖坤大神的回答，他认为GBDT只是对历史的一个记忆罢了，没有推广性，或者说泛化能力。 番外：盖坤大神的团队在2017年提出了两个重要的用于CTR预估的模型，MLR（实际开始应用于2012年）和DIN，后面的线性模型和深度模型也会更新这两篇，下篇再见吧~</p><p>实现GBDT+LR的一个Demo，感兴趣的童鞋可以看下我的<a href="[https://link.zhihu.com/?target=https%3A//github.com/Jesse-csj/TensorFlow_Practice](https://link.zhihu.com/?target=https%3A//github.com/Jesse-csj/TensorFlow_Practice">github</a>)。<br><br><br><strong>巨人的肩膀</strong>：<a href="https://blog.csdn.net/shenziheng1/article/details/89737467" target="_blank" rel="noopener">https://blog.csdn.net/shenziheng1/article/details/89737467</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本人才疏学浅，不足之处欢迎大家指出和交流。&lt;/p&gt;
&lt;p&gt;本系列接下来几天要分享的文章是关于线性模型的，在15年后深度模型在ctr预估领域百花齐放之时，仍觉得传统的LR模型有着基石的作用，于是有了今天的分享。话不多说，今天会介绍的是经典的LR模型及Facebook在2014年提出的GBDT+LR（重点）。当时深度学习几乎还没有应用到到计算广告领域，Facebook提出利用GBDT的叶节点编号作为非线性特征的表示，或者说是组合特征的一种方式，可以自动实现特征工程，下面我们一起来看看吧。&lt;/p&gt;
&lt;p&gt;两篇原文：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=51313D6762FBE110533EDFDD7A47A8D9?doi=10.1.1.134.395&amp;amp;rep=rep1&amp;amp;type=pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Predicting Clicks: Estimating the Click-Through Rate for New Ads&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=A54CCA7D4A8F05B6636C9D64316BCF96?doi=10.1.1.718.9050&amp;amp;rep=rep1&amp;amp;type=pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Practical Lessons from Predicting Clicks on Ads at Facebook&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="ctr" scheme="https://jesse-csj.github.io/categories/ctr/"/>
    
    
      <category term="推荐系统" scheme="https://jesse-csj.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>（读论文）推荐系统之ctr预估-XDeepFM模型解析</title>
    <link href="https://jesse-csj.github.io/2019/08/01/XDeepFM/"/>
    <id>https://jesse-csj.github.io/2019/08/01/XDeepFM/</id>
    <published>2019-08-01T08:18:19.000Z</published>
    <updated>2019-08-01T09:31:39.385Z</updated>
    
    <content type="html"><![CDATA[<p>本人才疏学浅，不足之处欢迎大家指出和交流。</p><p>抱歉迟了一天更新…今天要分享的是极深因子分解机模型（XDeepFM)。未闻其声先见其名：<br>可能我们现在有两个问题，一是为何它叫极深因子分解机（深在哪），二是和DeepFM有何关系？下面我们一起走进模型细节来看看吧。</p><p>原文：xDeepFM: Combining Explicit and Implicit Feature Interactions for Recommender Systems</p><a id="more"></a><p>地址：<a href="https://arxiv.org/pdf/1803.05170.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1803.05170.pdf</a></p><h1 id="1、背景"><a href="#1、背景" class="headerlink" title="1、背景"></a>1、背景</h1><p><br></p><p>数据特点就不再说明了，有兴趣的同学翻下专栏前面的文章。首先看到XDeepFM这个名字很容易联想到17年华为提出的DeepFM。但正如上一篇所提到的，这篇论文其实是针对DCN进行改进的，详情请参阅:</p><p>前面我们已经提到，DCN 的Cross层接在Embedding层之后，虽然可以显示自动构造高阶特征，但特征交互是发生在元素级（bit-wise）而非特征向量级（vector-wise）。这里先简单介绍下bit-wise与vector-wise；显式特征交互和隐式特征交互两组概念：<br>假设隐向量的维度为3维，如果两个特征(对应的向量分别为(a1,b1,c1) (a1,b1,c1)(a1,b1,c1)和(a2,b2,c2) (a2,b2,c2)(a2,b2,c2)的话）在进行交互时，交互的形式类似于<br>f(w1∗a1∗a2,w2∗b1∗b2,w3∗c1∗c2) f(w1 <em> a1 </em> a2,w2 <em> b1 </em> b2 ,w3 <em> c1 </em> c2)<br>f(w1∗a1∗a2,w2∗b1∗b2,w3∗c1∗c2)<br>的话，此时我们认为特征交互是发生在元素级（bit-wise）上。如果特征交互形式类似于<br>f(w∗(a1∗a2,b1∗b2,c1∗c2)) f(w <em> (a1 </em> a2 ,b1<em> b2,c1 </em> c2))<br>f(w∗(a1∗a2,b1∗b2,c1∗c2))<br>的话，那么我们认为特征交互是发生在特征向量级（vector-wise）。</p><p>显式的特征交互和隐式的特征交互。以两个特征为例xi和xj，在经过一系列变换后，我们可以表示成 wij∗(xi∗xj) wij <em>(xi </em> xj)wij∗(xi∗xj)的形式，就可以认为是显式特征交互，否则的话，是隐式的特征交互。</p><p>例如，Age Field对应嵌入向量<a1,b1,c1>，Occupation Field对应嵌入向量<a2,b2,c2>，在Cross层，a1,b1,c1,a2,b2,c2会拼接后直接作为输入，即它意识不到Field vector的概念。Cross 以嵌入向量中的单个bit为最细粒度，而FM是以向量为最细粒度学习相关性，即<strong>vector-wise</strong>。<strong>xDeepFM的动机，正是将FM的vector-wise的思想引入Cross部分</strong>。</a2,b2,c2></a1,b1,c1></p><h1 id="2、NFM"><a href="#2、NFM" class="headerlink" title="2、NFM"></a>2、NFM</h1><h2 id="2-1-NFM-Model"><a href="#2-1-NFM-Model" class="headerlink" title="2.1 NFM Model"></a>2.1 NFM Model</h2><p>与FM（因式分解机）相似，NFM使用实值特征向量。给定一个稀疏向量x∈Rn作为输入，其中特征值为xi=0表示第i个特征不存在，NFM预估的目标为:<br><br>其中第一项和第二项是线性回归部分，与FM相似，FM模拟数据的全局偏差和特征权重。第三项f(x)是NFM的核心组成部分,用于建模特征交互。它是一个多层前馈神经网络。如图2所示，接下来，我们一层一层地阐述f(x)的设计。<br>模型整体结构图如下所示：<br></p><h3 id="2-1-1-Embedding-Layer"><a href="#2-1-1-Embedding-Layer" class="headerlink" title="2.1.1 Embedding Layer"></a>2.1.1 Embedding Layer</h3><p>和其他的DNN模型处理稀疏输入一样，Embedding将输入转换到低维度的稠密的嵌入空间中进行处理。这里做稍微不同的处理是，使用原始的特征值乘以Embedding vector，使得模型也可以处理real valued feature。</p><h3 id="2-1-2-Bi-Interaction-Layer"><a href="#2-1-2-Bi-Interaction-Layer" class="headerlink" title="2.1.2 Bi-Interaction Layer"></a>2.1.2 Bi-Interaction Layer</h3><p>Bi是Bi-linear的缩写，这一层其实是一个pooling层操作，它把很多个向量转换成一个向量，形式化如下：<br></p><p>fbi的输入是整个的嵌入向量，xi ，xj是特征取值，vi， vj是特征对应的嵌入向量。中间的操作表示对应位置相乘。所以原始的嵌入向量任意两个都进行组合，对应位置相乘结果得到一个新向量；然后把这些新向量相加，就得到了Bi-Interaction的输出。这个输出只有一个向量。</p><p>注：Bi-Interaction并没有引入额外的参数，而且它的计算复杂度也是线性的，参考FM的优化方法，化简如下：<br></p><h3 id="2-1-3-Hidden-Layer"><a href="#2-1-3-Hidden-Layer" class="headerlink" title="2.1.3 Hidden Layer"></a>2.1.3 Hidden Layer</h3><p>这个跟其他的模型基本一样，堆积隐藏层以期来学习高阶组合特征。一般选用constant的效果要好一些。</p><h3 id="2-1-4-Prediction-Layer"><a href="#2-1-4-Prediction-Layer" class="headerlink" title="2.1.4 Prediction Layer"></a>2.1.4 Prediction Layer</h3><p>最后一层隐藏层Zl到输出层最后预测结果形式化如下：<br><br>其中h是中间的网络参数。考虑到前面的各层隐藏层权重矩阵，f(x)形式化如下：<br></p><p>这里相比于FM其实多出的参数其实就是隐藏层的参数，所以说FM也可以看做是一个神经网络架构，就是去掉隐藏层的NFM。</p><h2 id="2-2-NFM-vs-Wide-amp-Deep、DeepCross"><a href="#2-2-NFM-vs-Wide-amp-Deep、DeepCross" class="headerlink" title="2.2 NFM vs Wide&amp;Deep、DeepCross"></a>2.2 NFM vs Wide&amp;Deep、DeepCross</h2><p><strong>实质：</strong></p><p>NFM最重要的区别就在于Bi-Interaction Layer。Wide&amp;Deep和DeepCross都是用拼接操作(concatenation)替换了Bi-Interaction。</p><p>Concatenation操作的最大缺点就是它并没有考虑任何的特征组合信息，所以就全部依赖后面的MLP去学习特征组合，但是很不幸，MLP的学习优化非常困难。</p><p>使用Bi-Interaction考虑到了二阶特征组合，使得输入的表示包含更多的信息，减轻了后面MLP部分的学习压力，所以可以用更简单的模型（实验中只一层隐层），取得更好的效果。</p><h1 id="3、总结（具体的对比实验和实现细节等请参阅原论文）"><a href="#3、总结（具体的对比实验和实现细节等请参阅原论文）" class="headerlink" title="3、总结（具体的对比实验和实现细节等请参阅原论文）"></a>3、总结（具体的对比实验和实现细节等请参阅原论文）</h1><p><br></p><p>NFM主要的特点如下：</p><p>1. NFM核心就是在NN中引入了Bilinear Interaction(Bi-Interaction) pooling操作。基于此，NN可以在low level就学习到包含更多信息的组合特征。</p><p>2. 通过deepen FM来学习高阶的非线性的组合特征。</p><p>3. NFM相比于上面提到的DNN模型，模型结构更浅、更简单(shallower structure)，但是性能更好，训练和调整参数更加容易。</p><p>所以，依旧是FM+DNN的组合套路，不同之处在于如何处理Embedding向量，这也是各个模型重点关注的地方。现在来看业界就如何用DNN来处理高维稀疏的数据并没有一个统一普适的方法，依旧在摸索中。</p><p>实现DeepFM的一个Demo，感兴趣的童鞋可以看下我的<a href="https://link.zhihu.com/?target=https%3A//github.com/Jesse-csj/TensorFlow_Practice" target="_blank" rel="noopener">github</a>。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本人才疏学浅，不足之处欢迎大家指出和交流。&lt;/p&gt;
&lt;p&gt;抱歉迟了一天更新…今天要分享的是极深因子分解机模型（XDeepFM)。未闻其声先见其名：&lt;br&gt;可能我们现在有两个问题，一是为何它叫极深因子分解机（深在哪），二是和DeepFM有何关系？下面我们一起走进模型细节来看看吧。&lt;/p&gt;
&lt;p&gt;原文：xDeepFM: Combining Explicit and Implicit Feature Interactions for Recommender Systems&lt;/p&gt;
    
    </summary>
    
      <category term="ctr" scheme="https://jesse-csj.github.io/categories/ctr/"/>
    
    
      <category term="推荐系统" scheme="https://jesse-csj.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>（读论文）推荐系统之ctr预估-DCN模型解析</title>
    <link href="https://jesse-csj.github.io/2019/07/30/DCN/"/>
    <id>https://jesse-csj.github.io/2019/07/30/DCN/</id>
    <published>2019-07-30T11:19:37.000Z</published>
    <updated>2019-08-27T03:23:37.299Z</updated>
    
    <content type="html"><![CDATA[<p>本人才疏学浅，不足之处欢迎大家指出和交流。</p><p>今天要分享的是2017年斯坦福与Google联合提出的DCN模型，和明天要分享的XDeepFM是配套的，同时这篇论文是Google 对 Wide &amp; Deep工作的一个后续研究。本人写文章的顺序尽量严格按照时间顺序来的~话不多说，来看看今天分享的深度模型（串行结构）有哪些创新之处吧。</p><p>原文：Deep &amp; Cross Network for Ad Click Predictions<br><a id="more"></a></p><p>地址：<a href="https://arxiv.org/pdf/1708.05123.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1708.05123.pdf</a></p><h1 id="1、背景及相关工作"><a href="#1、背景及相关工作" class="headerlink" title="1、背景及相关工作"></a>1、背景及相关工作</h1><p><br></p><p>（每篇文章都重复下这些背景哈，希望大家别看烦，尽量换种方式）：传统的CTR预估模型需要大量的人工特征工程，耗时耗力；引入DNN之后，依靠神经网络强大的学习能力，可以一定程度上实现自动学习特征组合。但是DNN的缺点在于隐式的学习特征组合带来的不可解释性，以及低效率的学习(并不是所有的特征组合都是有用的)。这时交叉网络应运而生，同时联合DNN，发挥两者的共同优势。传统的CTR预估模型需要大量的人工特征工程，耗时耗力；引入DNN之后，依靠神经网络强大的学习能力，可以一定程度上实现自动学习特征组合。但是DNN的缺点在于隐式的学习特征组合带来的不可解释性，以及低效率的学习(并不是所有的特征组合都是有用的)。这时交叉网络应运而生，同时联合DNN，发挥两者的共同优势。</p><p><strong>相关工作：</strong>由于数据集规模和维数的急剧增加，之前已经提出了许多方法：</p><p>最开始FM使用隐向量的内积来建模组合特征；</p><p>FFM在此基础上引入field的概念，针对不同的field使用不同的隐向量。但是，这两者都是针对低阶（二阶，高阶会产生非常大的计算成本）的特征组合进行建模的；  随着DNN在计算机视觉、自然语言处理、语音识别等领域取得重要进展，DNN几乎无限的表达能力被广泛的研究。同样也尝试被用来解决web产品中输入数据高维高稀疏的问题。DNN可以对高维组合特征进行建模，但是DNN的不可解释性让DNN是否是目前最高效的针对此类问题的建模方式成为了一个问题；  另一方面，在Kaggle上的很多比赛中，大部分的获胜方案都是使用的人工特征工程，构造低阶的组合特征，这些特征意义明确且高效。而DNN学习到的特征都是隐式的、高度非线性的高阶组合特征，含义非常难以解释。这揭示了一个模型能够比通用的DNN设计更能够有效地学习的有界度特征的相互作用，那是否能设计一种DNN的特定网络结构来改善DNN，使得其学习起来更加高效呢？</p><p>Wide＆Deep是其中一个探索的例子，它以交叉特征作为一个线性模型的输入，与一个DNN模型一起训练，然而，W&amp;D网络的成功取决于正确的交叉特征的选择（仍依赖人工特征工程），这是一个至今还没有明确有效的方法解决的指数问题。</p><p>于是提出DCN进行进一步探索，将Wide部分替换为由特殊网络结构实现的Cross，<strong>自动构造有限高阶的交叉特征</strong>，并学习对应权重，告别了繁琐的人工叉乘。下面一起来看下细节：</p><h1 id="2、DEEP-amp-CROSS-NETWORK"><a href="#2、DEEP-amp-CROSS-NETWORK" class="headerlink" title="2、DEEP &amp; CROSS NETWORK"></a>2、DEEP &amp; CROSS NETWORK</h1><img src="/2019/07/30/DCN/1.png"> <p>DCN整体模型的架构图如上：底层是Embedding and stacking layer，然后是并行的Cross Network和Deep Network，最后是Combination Layer把Cross NetworkDeep Network的结果stack得到Output。</p><h2 id="2-1-Embedding-and-stacking-layer"><a href="#2-1-Embedding-and-stacking-layer" class="headerlink" title="2.1 Embedding and stacking layer"></a>2.1 Embedding and stacking layer</h2><p>DCN底层的两个功能是Embed和Stack。</p><p><strong>Embed</strong>：<br>在web-scale的推荐系统比如CTR预估中，输入的大部分特征都是类别型特征，通常的处理办</p><p>法就是编码为one-hot向量，对于实际应用中维度会非常高且稀疏，因此使用：<br><img src="/2019/07/30/DCN/2.png"><br>来将这些离散特征转换成实数值的稠密向量。</p><p><strong>Stack</strong>：<br>处理完了类别型特征，还有连续型特征需要处理。所以我们把连续型特征规范化之后，和嵌入向量stacking（堆叠）到一起形成一个向量，就得到了原始的输入：<br><img src="/2019/07/30/DCN/3.png"></p><h2 id="2-2-Cross-network"><a href="#2-2-Cross-network" class="headerlink" title="2.2 Cross network"></a>2.2 Cross network</h2><p>Cross Network是这个模型的核心，它被设计来<strong>高效地应用</strong>显式的交叉特征，关键在于如何高效地进行<strong>feature crossing</strong>。对于每层的计算，使用下述公式：<br><img src="/2019/07/30/DCN/4.png"></p><p>其中xl和xl+1 分别是第l层和第l+1层cross layer的输出（的列向量），wl和bl是这两层之间的连接参数。注意上式中所有的变量均是<strong>列向量</strong>，W也是列向量，并不是矩阵。</p><p><strong>理解</strong>：<br>这其实应用了残差网络的思想，<strong>xl+1 = f(xl, wl, bl) + xl：</strong>每一层的输出，都是上一层的输出加上<strong>feature crossing</strong> <strong>f</strong>。而<strong>f</strong>就是在<strong>拟合该层输出和上一层输出的残差（</strong>xl+1​−xl​<strong>）。残差网络有很多优点，其中一点是处理梯度退化/消失的问题，使神经网络可以“更深”.</strong>一层交叉层的可视化如下图所示：<br><img src="/2019/07/30/DCN/5.png"></p><p><strong>High-degree Interaction Across Features:</strong><br>Cross Network特殊的网络结构使得cross feature的阶数随着layer depth的增加而增加。相对于输入x0来说，一个l层的cross network的cross feature的阶数为l+1。</p><p><strong>复杂度分析：</strong><br>假设一共有Lc层cross layer，起始输入x0的维度为d。那么整个cross network的参数个数为:<br><img src="/2019/07/30/DCN/6.png"><br>因为每一层的W和b都是d维的。从上式可以发现，复杂度是输入维度d的线性函数，所以相比于deep network，cross network引入的复杂度微不足道。这样就保证了DCN的复杂度和DNN是一个级别的。论文中分析Cross Network的这种效率是因为x0 * xT的秩为1，使得我们不用计算并存储整个的矩阵就可以得到所有的cross terms。<br>但是，正是因为cross network的参数比较少导致它的表达能力受限，为了能够学习高阶非线性的组合特征，DCN并行的引入了Deep Network。</p><h2 id="2-3-Deep-network"><a href="#2-3-Deep-network" class="headerlink" title="2.3 Deep network"></a>2.3 Deep network</h2><p>深度网络就是一个全连接的前馈神经网络，每个深度层具有如下公式:<br><img src="/2019/07/30/DCN/7.png"><br>分析计算一下参数的数量来估计下复杂度。假设输入x0维度为d，一共有Lc层神经网络，每一层的神经元个数都是m个。那么总的参数或者复杂度为：<br><img src="/2019/07/30/DCN/8.png"></p><h2 id="2-4-Combination-layer"><a href="#2-4-Combination-layer" class="headerlink" title="2.4 Combination layer"></a>2.4 Combination layer</h2><p>Combination Layer把Cross Network和Deep Network的输出拼接起来，然后经过一个加权求和后得到logits，然后输入到标准的逻辑回归函数得到最终的预测概率。形式化如下：<br><img src="/2019/07/30/DCN/9.png"></p><p>p是最终的预测概率；XL1是d维的，表示Cross Network的最终输出；hL2是m维的，表示Deep Network的最终输出；Wlogits是Combination Layer的权重；最后经过sigmoid函数，得到最终预测概率。</p><p>损失函数使用带正则项的log loss，形式化如下：<br><img src="/2019/07/30/DCN/10.png"></p><p>此外，Cross Network和Deep Network，DCN是一起训练Cross Network和Deep Network的，这样网络可以知道另外一个网络的存在。</p><h1 id="3、CROSS-NETWORK-ANALYSIS"><a href="#3、CROSS-NETWORK-ANALYSIS" class="headerlink" title="3、CROSS NETWORK ANALYSIS"></a>3、CROSS NETWORK ANALYSIS</h1><p>本节是为了解释DCN的高效性,从三个角度：</p><p><strong>1、polynomial approximation；2、generalization to FMs；3、efficientprojection。</strong><br>（具体的分析请大家参阅原论文：这部分是非常重要的一部分，是论文中解释DCN为何高效的理论部分，然而本人还没有完全看懂，等之后再仔细看下再更新-  。-）</p><h1 id="4、总结（具体的对比实验和实现细节等请参阅原论文）"><a href="#4、总结（具体的对比实验和实现细节等请参阅原论文）" class="headerlink" title="4、总结（具体的对比实验和实现细节等请参阅原论文）"></a>4、总结（具体的对比实验和实现细节等请参阅原论文）</h1><p><br></p><p>DCN模型的特点：</p><p>1. 在cross network中，在每一层都应用feature crossing。高效的学习了bounded degree组合特征。不需要人工特征工程。</p><p>2. 网络结构简单且高效。多项式复杂度由layer depth决定。</p><p>3. 相比于DNN，DCN的logloss更低，而且参数的数量将近少了一个数量级。</p><p>4. 但是经过对cross network的分析如下，最终得到的输出就相当于X0 不断乘以一个数（标量），而且它们的特征交互是发生在元素级（bit-wise）。这种处理方式可能是存在问题的。<br><img src="/2019/07/30/DCN/11.png"></p><p>（当然，指出这些问题的就是我们下一篇论文XDeepFM啦，我们下篇再见）<br><br><br>实现DCN的一个Demo，感兴趣的童鞋可以看下我的<a href="https://link.zhihu.com/?target=https%3A//github.com/Jesse-csj/TensorFlow_Practice" target="_blank" rel="noopener">github</a>。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本人才疏学浅，不足之处欢迎大家指出和交流。&lt;/p&gt;
&lt;p&gt;今天要分享的是2017年斯坦福与Google联合提出的DCN模型，和明天要分享的XDeepFM是配套的，同时这篇论文是Google 对 Wide &amp;amp; Deep工作的一个后续研究。本人写文章的顺序尽量严格按照时间顺序来的~话不多说，来看看今天分享的深度模型（串行结构）有哪些创新之处吧。&lt;/p&gt;
&lt;p&gt;原文：Deep &amp;amp; Cross Network for Ad Click Predictions&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="ctr" scheme="https://jesse-csj.github.io/categories/ctr/"/>
    
    
      <category term="推荐系统" scheme="https://jesse-csj.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>（读论文）推荐系统之ctr预估-NFM模型解析</title>
    <link href="https://jesse-csj.github.io/2019/07/29/NFM/"/>
    <id>https://jesse-csj.github.io/2019/07/29/NFM/</id>
    <published>2019-07-29T12:18:19.000Z</published>
    <updated>2019-07-30T01:21:17.682Z</updated>
    
    <content type="html"><![CDATA[<p>本人才疏学浅，不足之处欢迎大家指出和交流。</p><p>周末没更新呀…今天要分享的是另一个Deep模型NFM(串行结构)。NFM也是用FM+DNN来对问题建模的，相比于之前提到的Wide&amp;Deep(Google)、DeepFM(华为+哈工大)、PNN(上交)和之后会分享的的DCN(Google)、DIN(阿里)等，NFM有什么优点呢，下面就走进模型我们一起来看看吧。</p><p>原文：Neural Factorization Machines for Sparse Predictive Analytics<br><a id="more"></a></p><p>地址：<a href="https://arxiv.org/pdf/1708.05027.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1708.05027.pdf</a></p><h1 id="1、问题由来"><a href="#1、问题由来" class="headerlink" title="1、问题由来"></a>1、问题由来</h1><p><br></p><p>老生常谈，再聊数据特点：对于广告中的大量的类别特征，特征组合也是非常多的。传统的做法是通过人工特征工程或者利用决策树来进行特征选择，选择出比较重要的特征。但是这样的做法都有一个缺点，就是：<strong>无法学习训练集中没有出现的特征组合</strong>。</p><p>最近几年，Embedding-based方法开始成为主流，通过把高维稀疏的输入_embed_到低维度的稠密的隐向量空间中，模型可以学习到训练集中没有出现过的特征组合。</p><p>Embedding-based大致可以分为两类：</p><p><strong>1.factorization machine-based linear models</strong></p><p><strong>2.neural network-based non-linear models</strong><a href="#_msocom_1"></a><br>（具体就不再展开了）</p><hr><p> <a href="#_msoanchor_1"></a>FM：以线性的方式学习二阶特征交互，对于捕获现实数据非线性和复杂的内在结构表达力不够；</p><p>深度网络：例如Wide&amp;Deep 和DeepCross，简单地拼接特征embedding向量不会考虑任何的特征之间的交互, 但是能够学习特征交互的非线性层的深层网络结构又很难训练优化；</p><p>而NFM摒弃了直接把嵌入向量拼接输入到神经网络的做法，在嵌入层之后增加了_Bi-Interaction_操作来对二阶组合特征进行建模。这使得low level的输入表达的信息更加的丰富，极大的提高了后面隐藏层学习高阶非线性组合特征的能力。</p><h1 id="2、NFM"><a href="#2、NFM" class="headerlink" title="2、NFM"></a>2、NFM</h1><h2 id="2-1-NFM-Model"><a href="#2-1-NFM-Model" class="headerlink" title="2.1 NFM Model"></a>2.1 NFM Model</h2><p>与FM（因式分解机）相似，NFM使用实值特征向量。给定一个稀疏向量x∈Rn作为输入，其中特征值为xi=0表示第i个特征不存在，NFM预估的目标为:<br><img src="/2019/07/29/NFM/1.png"><br>其中第一项和第二项是线性回归部分，与FM相似，FM模拟数据的全局偏差和特征权重。第三项f(x)是NFM的核心组成部分,用于建模特征交互。它是一个多层前馈神经网络。如图2所示，接下来，我们一层一层地阐述f(x)的设计。<br>模型整体结构图如下所示：<br><img src="/2019/07/29/NFM/2.png"></p><h3 id="2-1-1-Embedding-Layer"><a href="#2-1-1-Embedding-Layer" class="headerlink" title="2.1.1 Embedding Layer"></a>2.1.1 Embedding Layer</h3><p>和其他的DNN模型处理稀疏输入一样，Embedding将输入转换到低维度的稠密的嵌入空间中进行处理。这里做稍微不同的处理是，使用原始的特征值乘以Embedding vector，使得模型也可以处理real valued feature。</p><h3 id="2-1-2-Bi-Interaction-Layer"><a href="#2-1-2-Bi-Interaction-Layer" class="headerlink" title="2.1.2 Bi-Interaction Layer"></a>2.1.2 Bi-Interaction Layer</h3><p>Bi是Bi-linear的缩写，这一层其实是一个pooling层操作，它把很多个向量转换成一个向量，形式化如下：<br><img src="/2019/07/29/NFM/3.png"></p><p>fbi的输入是整个的嵌入向量，xi ，xj是特征取值，vi， vj是特征对应的嵌入向量。中间的操作表示对应位置相乘。所以原始的嵌入向量任意两个都进行组合，对应位置相乘结果得到一个新向量；然后把这些新向量相加，就得到了Bi-Interaction的输出。这个输出只有一个向量。</p><p>注：Bi-Interaction并没有引入额外的参数，而且它的计算复杂度也是线性的，参考FM的优化方法，化简如下：<br><img src="/2019/07/29/NFM/4.png"></p><h3 id="2-1-3-Hidden-Layer"><a href="#2-1-3-Hidden-Layer" class="headerlink" title="2.1.3 Hidden Layer"></a>2.1.3 Hidden Layer</h3><p>这个跟其他的模型基本一样，堆积隐藏层以期来学习高阶组合特征。一般选用constant的效果要好一些。</p><h3 id="2-1-4-Prediction-Layer"><a href="#2-1-4-Prediction-Layer" class="headerlink" title="2.1.4 Prediction Layer"></a>2.1.4 Prediction Layer</h3><p>最后一层隐藏层Zl到输出层最后预测结果形式化如下：<br><img src="/2019/07/29/NFM/5.png"><br>其中h是中间的网络参数。考虑到前面的各层隐藏层权重矩阵，f(x)形式化如下：<br><img src="/2019/07/29/NFM/6.png"></p><p>这里相比于FM其实多出的参数其实就是隐藏层的参数，所以说FM也可以看做是一个神经网络架构，就是去掉隐藏层的NFM。</p><h2 id="2-2-NFM-vs-Wide-amp-Deep、DeepCross"><a href="#2-2-NFM-vs-Wide-amp-Deep、DeepCross" class="headerlink" title="2.2 NFM vs Wide&amp;Deep、DeepCross"></a>2.2 NFM vs Wide&amp;Deep、DeepCross</h2><p><strong>实质：</strong></p><p>NFM最重要的区别就在于Bi-Interaction Layer。Wide&amp;Deep和DeepCross都是用拼接操作(concatenation)替换了Bi-Interaction。</p><p>Concatenation操作的最大缺点就是它并没有考虑任何的特征组合信息，所以就全部依赖后面的MLP去学习特征组合，但是很不幸，MLP的学习优化非常困难。</p><p>使用Bi-Interaction考虑到了二阶特征组合，使得输入的表示包含更多的信息，减轻了后面MLP部分的学习压力，所以可以用更简单的模型（实验中只一层隐层），取得更好的效果。</p><h1 id="3、总结（具体的对比实验和实现细节等请参阅原论文）"><a href="#3、总结（具体的对比实验和实现细节等请参阅原论文）" class="headerlink" title="3、总结（具体的对比实验和实现细节等请参阅原论文）"></a>3、总结（具体的对比实验和实现细节等请参阅原论文）</h1><p><br></p><p>NFM主要的特点如下：</p><p>1. NFM核心就是在NN中引入了Bilinear Interaction(Bi-Interaction) pooling操作。基于此，NN可以在low level就学习到包含更多信息的组合特征。</p><p>2. 通过deepen FM来学习高阶的非线性的组合特征。</p><p>3. NFM相比于上面提到的DNN模型，模型结构更浅、更简单(shallower structure)，但是性能更好，训练和调整参数更加容易。</p><p>所以，依旧是FM+DNN的组合套路，不同之处在于如何处理Embedding向量，这也是各个模型重点关注的地方。现在来看业界就如何用DNN来处理高维稀疏的数据并没有一个统一普适的方法，依旧在摸索中。</p><p>实现DeepFM的一个Demo，感兴趣的童鞋可以看下我的<a href="https://link.zhihu.com/?target=https%3A//github.com/Jesse-csj/TensorFlow_Practice" target="_blank" rel="noopener">github</a>。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本人才疏学浅，不足之处欢迎大家指出和交流。&lt;/p&gt;
&lt;p&gt;周末没更新呀…今天要分享的是另一个Deep模型NFM(串行结构)。NFM也是用FM+DNN来对问题建模的，相比于之前提到的Wide&amp;amp;Deep(Google)、DeepFM(华为+哈工大)、PNN(上交)和之后会分享的的DCN(Google)、DIN(阿里)等，NFM有什么优点呢，下面就走进模型我们一起来看看吧。&lt;/p&gt;
&lt;p&gt;原文：Neural Factorization Machines for Sparse Predictive Analytics&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="ctr" scheme="https://jesse-csj.github.io/categories/ctr/"/>
    
    
      <category term="推荐系统" scheme="https://jesse-csj.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>（读论文）推荐系统之ctr预估-DeepFM模型解析</title>
    <link href="https://jesse-csj.github.io/2019/07/24/DeepFM/"/>
    <id>https://jesse-csj.github.io/2019/07/24/DeepFM/</id>
    <published>2019-07-24T14:30:28.000Z</published>
    <updated>2019-07-29T13:38:04.754Z</updated>
    
    <content type="html"><![CDATA[<p>今天第二篇（最近更新的都是Deep模型，传统的线性模型会后面找个时间更新的哈）。本篇介绍华为的DeepFM模型 (2017年)，此模型在 Wide&amp;Deep 的基础上进行改进，成功解决了一些问题，具体的话下面一起来看下吧。</p><p>原文：Deepfm: a factorization-machine based neural network for ctr prediction<br><a id="more"></a></p><p>地址：<a href="http://www.ijcai.org/proceedings/2017/0239.pdf" target="_blank" rel="noopener">http://www.ijcai.org/proceedings/2017/0239.pdf</a></p><h1 id="1、问题由来"><a href="#1、问题由来" class="headerlink" title="1、问题由来"></a>1、问题由来</h1><h2 id="1-1、背景"><a href="#1-1、背景" class="headerlink" title="1.1、背景"></a>1.1、背景</h2><p>CTR 预估 数据特点：</p><ol><li><p>输入中包含类别型和连续型数据。类别型数据需要 one-hot, 连续型数据可以先离散化再 one-hot，也可以直接保留原值。</p></li><li><p>维度非常高。</p></li><li><p>数据非常稀疏。</p></li><li><p>特征按照 Field 分组。</p></li></ol><p>CTR 预估 重点 在于 学习组合特征。注意，组合特征包括二阶、三阶甚至更高阶的，阶数越高越复杂，越不容易学习。Google 的论文研究得出结论：高阶和低阶的组合特征都非常重要，同时学习到这两种组合特征的性能要比只考虑其中一种的性能要好。</p><p>那么关键问题转化成：如何高效的提取这些组合特征。一种办法就是引入领域知识人工进行特征工程。这样做的弊端是高阶组合特征非常难提取，会耗费极大的人力。而且，有些组合特征是隐藏在数据中的，即使是专家也不一定能提取出来，比如著名的“尿布与啤酒”问题。</p><p>在 DeepFM 提出之前，已有 LR（线性或广义线性模型后面更新），FM、FFM、FNN、PNN（以及三种变体：IPNN,OPNN,PNN*）,Wide&amp;Deep 模型，这些模型在 CTR 或者是推荐系统中被广泛使用。</p><h2 id="1-2、现有模型的问题"><a href="#1-2、现有模型的问题" class="headerlink" title="1.2、现有模型的问题"></a>1.2、现有模型的问题</h2><p><br></p><p>线性模型：最开始 CTR 或者是推荐系统领域，一些线性模型取得了不错的效果（线性模型LR简单、快速并且模型具有可解释，有着很好的拟合能力），但是LR模型是线性模型，表达能力有限，泛化能力较弱，需要做好特征工程，尤其需要交叉特征，才能取得一个良好的效果，然而在工业场景中，特征的数量会很多，可能达到成千上万，甚至数十万，这时特征工程就很难做，还不一定能取得更好的效果。</p><p>FM模型：线性模型差强人意，直接导致了 FM 模型应运而生（在 Kaggle 上打比赛提出来的，取得了第一名的成绩）。FM 通过隐向量 latent vector 做内积来表示组合特征，从理论上解决了低阶和高阶组合特征提取的问题。但是实际应用中受限于计算复杂度，一般也就只考虑到 2 阶交叉特征。后面又进行了改进，提出了 FFM，增加了 Field 的概念。</p><p>遇上深度学习：随着 DNN 在图像、语音、NLP 等领域取得突破，人们渐渐意识到 DNN 在特征表示上的天然优势。相继提出了使用 CNN 或 RNN 来做 CTR 预估的模型。但是，CNN 模型的缺点是：偏向于学习相邻特征的组合特征。  RNN 模型的缺点是：比较适用于有序列 (时序) 关系的数据。</p><p>FNN 的提出，应该算是一次非常不错的尝试：先使用预先训练好的 FM，得到隐向量，然后作为 DNN 的输入来训练模型。缺点在于：受限于 FM 预训练的效果。</p><p>随后提出了 PNN，PNN 为了捕获高阶组合特征，在embedding layer和first hidden layer之间增加了一个product layer。根据 product layer 使用内积、外积、混合分别衍生出IPNN, OPNN, PNN*三种类型。</p><p>但无论是 FNN 还是 PNN，他们都有一个绕不过去的缺点：对于低阶的组合特征，学习到的比较少。 而前面我们说过，低阶特征对于 CTR 也是非常重要的。</p><p>Google（上一篇） 意识到了这个问题，为了同时学习低阶和高阶组合特征，提出了 Wide&amp;Deep 模型。它混合了一个 线性模型（Wide part） 和 Deep 模型 (Deep part)。这两部分模型需要不同的输入，而 Wide part 部分的输入，依旧 依赖人工特征工程。</p><p>但是，这些模型普遍都存在两个问题：</p><ol><li><p>偏向于提取低阶或者高阶的组合特征。不能同时提取这两种类型的特征。</p></li><li><p>需要专业的领域知识来做特征工程。</p></li></ol><p>于是DeepFM 应运而生，成功解决了这两个问题，并做了一些改进，其 优点 如下：</p><ol><li><p>不需要预训练 FM 得到隐向量。</p></li><li><p>不需要人工特征工程。</p></li><li><p>能同时学习低阶和高阶的组合特征。</p></li><li><p>FM 模块和 Deep 模块共享 Feature Embedding 部分，可以更快的训练，以及更精确的训练学习。</p></li></ol><p>下面就一直来走进模型的细节。</p><h1 id="2、模型细节"><a href="#2、模型细节" class="headerlink" title="2、模型细节"></a>2、模型细节</h1><p><br></p><p>DeepFM主要做法如下：</p><ol><li><p>FM Component + Deep Component。FM 提取低阶组合特征，Deep 提取高阶组合特征。但是和 Wide&amp;Deep 不同的是，DeepFM 是端到端的训练，不需要人工特征工程。</p></li><li><p>共享 feature embedding。FM 和 Deep 共享输入和feature embedding不但使得训练更快，而且使得训练更加准确。相比之下，Wide&amp;Deep 中，input vector 非常大，里面包含了大量的人工设计的 pairwise 组合特征，增加了它的计算复杂度。</p></li></ol><p>模型整体结构图如下所示：<br><img src="/2019/07/24/DeepFM/1.png"></p><p>由上面网络结构图可以看到，DeepFM 包括 FM和 DNN两部分，所以模型最终的输出也由这两部分组成：</p><img src="/2019/07/24/DeepFM/2.png"><p>下面，把结构图进行拆分，分别来看这两部分。</p><h2 id="2-1、The-FM-Component"><a href="#2-1、The-FM-Component" class="headerlink" title="2.1、The FM Component"></a>2.1、The FM Component</h2><p>FM 部分的输出由两部分组成：一个 Addition Unit，多个 内积单元。</p><img src="/2019/07/24/DeepFM/4.png"><p>这里的 d 是输入 one-hot 之后的维度，我们一般称之为feature_size。对应的是 one-hot 之前的特征维度，我们称之为field_size。</p><img src="/2019/07/24/DeepFM/3.png"><p>架构图如上图所示：Addition Unit 反映的是 1 阶的特征。内积单元 反映的是 2 阶的组合特征对于预测结果的影响。</p><p>这里需要注意三点：</p><ol><li><p>这里的wij，也就是<vi,vj>，可以理解为DeepFM结构中计算embedding vector的权矩阵（并非是网上很多文章认为的vi是embedding vector）。</vi,vj></p></li><li><p>由于输入特征one-hot编码，所以embedding vector也就是输入层到Dense Embeddings层的权重。</p></li><li><p>Dense Embeddings层的神经元个数是由embedding vector和field_size共同确定，直观来说就是：神经元的个数为embedding vector*field_size。</p></li></ol><p><strong>FM Component 总结：</strong></p><ol><li><p>FM 模块实现了对于 1 阶和 2 阶组合特征的建模。</p></li><li><p>无须预训练。</p></li><li><p>没有人工特征工程。</p></li><li><p>embedding 矩阵的大小是：特征数量 * 嵌入维度。然后用一个 index 表示选择了哪个特征。</p></li></ol><p>需要训练的有两部分：</p><ol><li><p>input_vector 和 Addition Unit 相连的全连接层，也就是 1 阶的 Embedding 矩阵。</p></li><li><p>Sparse Feature 到 Dense Embedding 的 Embedding 矩阵，中间也是全连接的，要训练的是中间的权重矩阵，这个权重矩阵也就是隐向量 Vi。</p></li></ol><h2 id="2-2、The-Deep-Component"><a href="#2-2、The-Deep-Component" class="headerlink" title="2.2、The Deep Component"></a>2.2、The Deep Component</h2><p>Deep Component 架构图：</p><img src="/2019/07/24/DeepFM/5.png"><p>这里DNN的作用是构造高阶组合特征，网络里面黑色的线是全连接层，参数需要神经网络去学习。且有一个特点：DNN的输入也是embedding vector。所谓的权值共享指的就是这里。</p><p>关于DNN网络中的输入a处理方式采用前向传播，如下所示：</p><img src="/2019/07/24/DeepFM/6.png"><p>这里假设α(0)=(e1,e2,…em)表示 embedding层的输出，那么α(0)作为下一层 DNN隐藏层的输入，其前馈过程如下：</p><img src="/2019/07/24/DeepFM/7.png"><p>优点：</p><ol><li><p>模型可以从最原始的特征中，同时学习低阶和高阶组合特征</p></li><li><p>不再需要人工特征工程。Wide&amp;Deep 中低阶组合特征就是同过特征工程得到的。</p></li></ol><h1 id="3、总结（具体的对比实验和实现细节等请参阅原论文）"><a href="#3、总结（具体的对比实验和实现细节等请参阅原论文）" class="headerlink" title="3、总结（具体的对比实验和实现细节等请参阅原论文）"></a>3、总结（具体的对比实验和实现细节等请参阅原论文）</h1><p><br></p><p>DeepFM优点：</p><ol><li><p>没有用 FM 去预训练隐向量 Vi，并用 Vi去初始化神经网络。（相比之下 FNN 就需要预训练 FM 来初始化 DNN）。</p></li><li><p>FM 模块不是独立的，是跟整个模型一起训练学习得到的。（相比之下 Wide&amp;Deep 中的 Wide 和 Deep 部分是没有共享的）</p></li><li><p>不需要特征工程。（相比之下 Wide&amp;Deep 中的 Wide 部分需要特征工程）</p></li><li><p>训练效率高。（相比 PNN 没有那么多参数）</p></li></ol><p>其中最核心的：</p><ol><li><p>没有预训练（no pre-training）</p></li><li><p>共享 Feature Embedding，没有特征工程（no feature engineering）</p></li><li><p>同时学习低阶和高阶组合特征（capture both low-high-order interaction features）</p></li></ol><p>实现DeepFM的一个Demo，感兴趣的童鞋可以关注我的<a href="https://github.com/Jesse-csj/TensorFlow_Practice" target="_blank" rel="noopener">github</a>。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;今天第二篇（最近更新的都是Deep模型，传统的线性模型会后面找个时间更新的哈）。本篇介绍华为的DeepFM模型 (2017年)，此模型在 Wide&amp;amp;Deep 的基础上进行改进，成功解决了一些问题，具体的话下面一起来看下吧。&lt;/p&gt;
&lt;p&gt;原文：Deepfm: a factorization-machine based neural network for ctr prediction&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="ctr" scheme="https://jesse-csj.github.io/categories/ctr/"/>
    
    
      <category term="推荐系统" scheme="https://jesse-csj.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>（读论文）推荐系统之ctr预估-Wide＆Deep模型解析</title>
    <link href="https://jesse-csj.github.io/2019/07/24/Wide-Deep/"/>
    <id>https://jesse-csj.github.io/2019/07/24/Wide-Deep/</id>
    <published>2019-07-23T23:33:22.000Z</published>
    <updated>2019-07-24T03:17:22.029Z</updated>
    
    <content type="html"><![CDATA[<p>在读了FM和FNN/PNN的论文后，来学习一下16年的一篇Google的论文，文章将传统的LR和DNN组合构成一个wide&amp;deep模型（并行结构），既保留了LR的拟合能力，又具有DNN的泛化能力，并且不需要单独训练模型，可以方便模型的迭代，一起来看下吧。<br>原文：Wide &amp; Deep Learning for Recommender Systems<br><a id="more"></a></p><p>地址： <a href="https://arxiv.org/pdf/1606.07792.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1606.07792.pdf</a></p><h1 id="1、问题由来"><a href="#1、问题由来" class="headerlink" title="1、问题由来"></a><strong>1、问题由来</strong></h1><h2 id="1-1、背景"><a href="#1-1、背景" class="headerlink" title="1.1、背景"></a><strong>1.1、背景</strong></h2><p>本文提出时是针对推荐系统中应用的，当然也可以应用在ctr预估中。<br>首先介绍论文中通篇出现的两个名词：</p><ul><li>memorization（暂且翻译为记忆）：即从历史数据中发现item或者特征之间的相关性。</li><li>generalization（暂且翻译为泛化）：即相关性的传递，发现在历史数据中很少或者没有出现的新的特征组合。</li></ul><p>举个例子来解释下：在人类的认知学习过程中演化过程中，人类的大脑很复杂，它可以记忆(memorize)下每天发生的事情（麻雀可以飞，鸽子可以飞）然后泛化(generalize)这些知识到之前没有看到过的东西（有翅膀的动物都能飞）。<br>但是泛化的规则有时候不是特别的准确，有时候会出错（有翅膀的动物都能飞吗）。这时候就需要记忆(memorization)来修正泛化的规则(generalized rules)，叫做特例（企鹅有翅膀，但是不能飞）。这就是Memorization和Generalization的来由或者说含义。</p><h2 id="1-2、现有模型的问题"><a href="#1-2、现有模型的问题" class="headerlink" title="1.2、现有模型的问题"></a><strong>1.2、现有模型的问题</strong></h2><ul><li><p>线性模型LR简单、快速并且模型具有可解释，有着很好的拟合能力，但是LR模型是线性模型，表达能力有限，泛化能力较弱，需要做好特征工程，尤其需要交叉特征，才能取得一个良好的效果，然而在工业场景中，特征的数量会很多，可能达到成千上万，甚至数十万，这时特征工程就很难做，还不一定能取得更好的效果。 </p></li><li><p>DNN模型不需要做太精细的特征工程，就可以取得很好的效果，DNN可以自动交叉特征，学习到特征之间的相互作用，尤其是可以学到高阶特征交互，具有很好的泛化能力。另外，DNN通过增加embedding层，可以有效的解决稀疏数据特征的问题，防止特征爆炸。推荐系统中的泛化能力是很重要的，可以提高推荐物品的多样性，但是DNN在拟合数据上相比较LR会较弱。 </p></li><li><p>总结一下：</p><ol><li>线性模型无法学习到训练集中未出现的组合特征；</li><li>FM或DNN通过学习embedding vector虽然可以学习到训练集中未出现的组合特征，但是会过度泛化。</li></ol></li></ul><p>为了提高推荐系统的拟合性和泛化性，可以将LR和DNN结合起来，同时增强拟合能力和泛化能力，wide&amp;deep就是将LR和DNN组合起来，wide部分就是LR，deep部分就是DNN，将两者的结果组合进行输出。</p><h1 id="2、模型细节"><a href="#2、模型细节" class="headerlink" title="2、模型细节"></a><strong>2、模型细节</strong></h1><p>再简单介绍下两个名词的实现：<br><strong>Memorization：</strong>之前大规模稀疏输入的处理是：通过线性模型 + 特征交叉。所带来的Memorization以及记忆能力非常有效和可解释。但是Generalization（泛化能力）需要更多的人工特征工程。</p><p><strong>Generalization：</strong>相比之下，DNN几乎不需要特征工程。通过对低纬度的dense embedding进行组合可以学习到更深层次的隐藏特征。但是，缺点是有点over-generalize（过度泛化）。推荐系统中表现为：会给用户推荐不是那么相关的物品，尤其是user-item矩阵比较稀疏并且是high-rank（高秩矩阵）</p><p><strong>两者区别：</strong>Memorization趋向于更加保守，推荐用户之前有过行为的items。相比之下，generalization更加趋向于提高推荐系统的多样性（diversity）。</p><h2 id="2-1、Wide-和-Deep"><a href="#2-1、Wide-和-Deep" class="headerlink" title="2.1、Wide 和 Deep"></a><strong>2.1、Wide 和 Deep</strong></h2><p><strong>Wide &amp; Deep:</strong><br>Wide &amp; Deep包括两部分：线性模型 + DNN部分。结合上面两者的优点，平衡memorization和generalization。<br>原因：综合memorization和generalizatio的优点，服务于推荐系统。在本文的实验中相比于wide-only和deep-only的模型，wide &amp; deep提升显著。下图是模型整体结构：<br><img src="/2019/07/24/Wide-Deep/1.png"><br>可以看出，Wide也是一种特殊的神经网络，他的输入直接和输出相连，属于广义线性模型的范畴。Deep就是指Deep Neural Network，这个很好理解。Wide Linear Model用于memorization；Deep Neural Network用于generalization。<br>左侧是Wide-only，右侧是Deep-only，中间是Wide &amp; Deep。</p><h2 id="2-2、Cross-product-transformation"><a href="#2-2、Cross-product-transformation" class="headerlink" title="2.2、Cross-product transformation"></a><strong>2.2、Cross-product transformation</strong></h2><p>论文Wide中不断提到这样一种变换用来生成组合特征，这里很重要。它的定义如下：<br><img src="/2019/07/24/Wide-Deep/2.png"><br>其中k表示第k个组合特征。i表示输入X的第i维特征。C_ki表示这个第i维度特征是否要参与第k个组合特征的构造。d表示输入X的维度。到底有哪些维度特征要参与构造组合特征，这个是人工设定的（这也就是说需要人工特征工程），在公式中没有体现。</p><p>其实这么一个复杂的公式，就是我们之前一直在说的one-hot之后的组合特征：仅仅在输入样本X中的特征gender=female和特征language=en同时为1，新的组合特征AND(gender=female, language=en)才为1。所以只要把两个特征的值相乘就可以了。<br>（这样Cross-product transformation 可以在二值特征中学习到组合特征，并且为模型增加非线性）</p><h2 id="2-3、The-Wide-Component"><a href="#2-3、The-Wide-Component" class="headerlink" title="2.3、The Wide Component"></a>2.3、<strong>The Wide Component</strong></h2><p>如上面所说Wide Part其实是一个广义的线性模型。使用特征包括：</p><ul><li><p>raw input： 原始特征</p></li><li><p>cross-product transformation ：上面提到的组合特征</p></li></ul><p>用同一个例子来说明：你给model一个query（你想吃的美食），model返回给你一个美食，然后你购买/消费了这个推荐。 也就是说，推荐系统其实要学习的是这样一个条件概率： P(consumption | query, item)。<br>Wide Part可以对一些特例进行memorization。比如AND(query=”fried chicken”, item=”chicken fried rice”)虽然从字符角度来看很接近，但是实际上完全不同的东西，那么Wide就可以记住这个组合是不好的，是一个特例，下次当你再点炸鸡的时候，就不会推荐给你鸡肉炒米饭了。</p><h2 id="2-4、The-Deep-Component"><a href="#2-4、The-Deep-Component" class="headerlink" title="2.4、The Deep Component"></a>2.4、<strong>The Deep Component</strong></h2><p>如模型右边所示：Deep Part通过学习一个低纬度的dense representation（也叫做embedding vector）对于每一个query和item，来<strong>泛化</strong>给你推荐一些字符上看起来不那么相关，但是你可能也是需要的。比如说：你想要炸鸡，Embedding Space中，炸鸡和汉堡很接近，所以也会给你推荐汉堡。</p><p>Embedding vectors被随机初始化，并根据最终的loss来反向训练更新。这些低维度的dense embedding vectors被作为第一个隐藏层的输入。隐藏层的激活函数通常使用ReLU。</p><h1 id="3、模型训练"><a href="#3、模型训练" class="headerlink" title="3、模型训练"></a><strong>3、模型训练</strong></h1><p>训练中原始的稀疏特征，在两个组件中都会用到，比如query=”fried chicken” item=”chicken fried rice”:<br><img src="/2019/07/24/Wide-Deep/3.png"></p><p>在训练的时候，根据最终的loss计算出gradient，反向传播到Wide和Deep两部分中，分别训练自己的参数。也就是说，<strong>两个模块是一起训练的</strong>（也就是论文中的联合训练），注意这不是模型融合。</p><ul><li><p>Wide部分中的组合特征可以<strong>记住</strong>那些稀疏的，特定的rules</p></li><li><p>Deep部分通过Embedding来<strong>泛化</strong>推荐一些相似的items</p></li></ul><p>Wide模块通过组合特征可以很效率的学习一些特定的组合，但是这也导致了他并不能学习到训练集中没有出现的组合特征。所幸，Deep模块弥补了这个缺点。<br>另外，因为是一起训练的，wide和deep的size都减小了。wide组件只需要填补deep组件的不足就行了，所以需要比较少的cross-product feature transformations，而不是full-size wide Model。<br>具体的训练方法和实验请参考原论文。</p><h1 id="4、总结"><a href="#4、总结" class="headerlink" title="4、总结"></a><strong>4、总结</strong></h1><p>缺点：Wide部分还是需要人工特征工程。<br>优点：实现了对memorization和generalization的统一建模。能同时学习低阶和高阶组合特征</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在读了FM和FNN/PNN的论文后，来学习一下16年的一篇Google的论文，文章将传统的LR和DNN组合构成一个wide&amp;amp;deep模型（并行结构），既保留了LR的拟合能力，又具有DNN的泛化能力，并且不需要单独训练模型，可以方便模型的迭代，一起来看下吧。&lt;br&gt;原文：Wide &amp;amp; Deep Learning for Recommender Systems&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="ctr" scheme="https://jesse-csj.github.io/categories/ctr/"/>
    
    
      <category term="推荐系统" scheme="https://jesse-csj.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>（读论文）推荐系统之ctr预估-PNN模型解析</title>
    <link href="https://jesse-csj.github.io/2019/07/22/PNN/"/>
    <id>https://jesse-csj.github.io/2019/07/22/PNN/</id>
    <published>2019-07-22T08:01:40.000Z</published>
    <updated>2019-07-23T13:25:04.358Z</updated>
    
    <content type="html"><![CDATA[<p>今天第二篇，还是之前的经典论文（PNN）还是基于DNN的深度模型用于预测点击率，不过相比于FNN提出了不少新的idea，一起来看下吧。</p><p>原论文：Product-based Neural Networks for User Response Prediction ：2016</p><a id="more"></a><p>地址：<a href="https://arxiv.org/pdf/1611.00144.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1611.00144.pdf</a></p><h1 id="1、原理"><a href="#1、原理" class="headerlink" title="1、原理"></a><strong>1、原理</strong></h1><p>给大家举例一个直观的场景：比如现在有一个凤凰网站，网站上面有一个迪斯尼广告，那我们现在想知道用户进入这个网站之后会不会有兴趣点击这个广告，类似这种用户点击率预测在信息检索领域就是一个非常核心的问题。普遍的做法就是通过不同的域来描述这个事件然后预测用户的点击行为，而这个域可以有很多。那么什么样的用户会点击这个广告呢？我们可能猜想：目前在上海的年轻的用户可能会有需求，如果今天是周五，看到这个广告，可能会点击这个广告为周末做活动参考。那可能的特征会是：[Weekday=Friday, occupation=Student, City=Shanghai]，当这些特征同时出现时，我们认为这个用户点击这个迪斯尼广告的概率会比较大。</p><p>传统的做法是应用One-Hot Binary的编码方式去处理这类数据，例如现在有三个域的数据X=[Weekday=Wednesday, Gender=Male, City=Shanghai],其中 Weekday有7个取值，我们就把它编译为7维的二进制向量，其中只有Wednesday是1，其他都是0，因为它只有一个特征值；Gender有两维，其中一维是1；如果有一万个城市的话，那City就有一万维，只有上海这个取值是1，其他是0。</p><p>那最终就会得到一个高维稀疏向量。但是这个数据集不能直接用神经网络训练：如果直接用One-Hot Binary进行编码，那输入特征至少有一百万，第一层至少需要500个节点，那么第一层我们就需要训练5亿个参数，那就需要20亿或是50亿的数据集，而要获得如此大的数据集基本上是很困难的事情。</p><p><strong>回顾FM、FNN模型</strong></p><p>因为上述原因需要将非常大的特征向量嵌入到低维向量空间中来减小模型复杂度，而FM（Factorisation machine）是很有效的embedding model：<br>                              <img src="/2019/07/22/PNN/1.jpg"></p><p>第一部分仍然为Logistic Regression，第二部分是通过两两向量之间的点积来判断特征向量之间和目标变量之间的关系。比如上述的迪斯尼广告，occupation=Student和City=Shanghai这两个向量之间的角度应该小于90，它们之间的点积应该大于0，说明和迪斯尼广告的点击率是正相关的。这种算法在推荐系统领域应用比较广泛。</p><p>那就基于这个模型来考虑神经网络模型，其实这个模型本质上就是一个三层网络：</p><p><img src="https://pic2.zhimg.com/80/v2-b046c93f125596de5265f7c6342542f6_hd.png" alt></p><p>它在第二层对向量做了乘积处理（比如上图蓝色节点直接为两个向量乘积，其连接边上没有参数需要学习），每个field都只会被映射到一个low-dimensional vector，field和field之间没有相互影响，那么第一层就被大量降维，之后就可以在此基础上应用神经网络模型。</p><p>如用FM算法对底层field进行embeddding，在此基础上面建模就是FNN(Factorisation-machinesupported Neural Networks)模型:</p><p><img src="https://pic2.zhimg.com/80/v2-9fb5ca82bca2cd35f3751aca3ca5ac45_hd.png" alt></p><p>那现在进一步考虑FNN与一般的神经网络的区别是什么？大部分的神经网络模型对向量之间的处理都是采用加法操作，而FM 则是通过向量之间的乘法来衡量两者之间的关系。我们知道乘法关系其实相当于逻辑“且”的关系，拿上述例子来说，只有特征是学生而且在上海的人才有更大的概率去点击迪斯尼广告。但是加法仅相当于逻辑中“或”的关系，显然“且”比“或”更能严格区分目标变量。（加法就是正常拼接后作为输出，这里就是先做乘积再拼接作为DNN的输入）</p><p>所以我们接下来的工作就是对乘法关系建模。可以对两个向量做内积和外积的乘法操作，在此基础之上我们搭建的神经网络PNN：提出了一种product layer的思想，既基于乘法的运算来体现特征交叉的DNN网络结构，如下图：</p> <img src="/2019/07/22/PNN/2.jpg"><p>按照论文的思路，从上往下来看这个网络结构：</p><p>输出层 输出层很简单，将上一层的网络输出通过一个全链接层，经过sigmoid函数转换后映射到(0,1)的区间中，得到我们的点击率的预测值：</p><p>​                <img src="/2019/07/22/PNN/3.jpg"><br><strong>l2层</strong></p><p>根据l1层的输出，经一个全链接层 ，并使用relu进行激活，得到我们l2的输出结果：</p> <img src="/2019/07/22/PNN/4.jpg"><p><strong>l1层</strong> </p><p>l1层的输出由如下的公式计算：</p> <img src="/2019/07/22/PNN/5.jpg"><p>可以看到在得到l1层输出时，这里输入了三部分，分别是lz，lp 和 b1，b1是偏置项，这里可以先不管。lz和lp的计算就是PNN的重点所在了。</p><p><strong>Product Layer</strong></p><p>product思想来源于，在ctr预估中，认为特征之间的关系更多是一种and“且”的关系，而非add”或”的关系。例如，性别为男且喜欢游戏的人群，比起性别男和喜欢游戏的人群，前者的组合比后者更能体现特征交叉的意义。</p><p>product layer可以分成两个部分，一部分是线性部分lz，一部分是非线性部分lp。二者的形式如下：</p> <img src="/2019/07/22/PNN/6.jpg"><p>在这里，我们要使用到论文中所定义的一种运算方式，其实就是矩阵的点乘（对应位置相乘然后求和，最终得到的是一个标量）:</p> <img src="/2019/07/22/PNN/7.jpg"><p>这里先继续介绍网络结构，下一章中详细介绍Product Layer。</p><p><strong>Embedding Layer</strong></p><p>Embedding Layer跟DeepFM中相同，将每一个field的特征转换成同样长度的向量，这里用f来表示。</p><p><img src="https://pic2.zhimg.com/80/v2-b46b0d082980847e97b4e4448be2c5ba_hd.png" alt></p><p><strong>损失函数</strong> </p><p>损失函数使用交叉熵：</p> <img src="/2019/07/22/PNN/8.jpg"><h1 id="2、Product-Layer详细介绍"><a href="#2、Product-Layer详细介绍" class="headerlink" title="2、Product Layer详细介绍"></a>2、Product Layer详细介绍</h1><p>前面提到了，product layer可以分成两个部分，一部分是线性部分lz，一部分是非线性部分lp。它们同维度，其具体形式如下：</p><p><img src="https://pic2.zhimg.com/80/v2-37e77e50b3a109ed916b6396046b55c0_hd.png" alt></p><p>其中z,p为信号向量，z为线性信号向量，p为二次信号向量,</p><script type="math/tex; mode=display">其中W_{z}^{i},W_{p}^{i}为权重矩阵。</script><p>(权重矩阵与z,p同维，经过定义的这种点乘运算后都得到一个标量作为DNN的输入）看上面的公式，我们首先需要知道z和p，这都是由我们的embedding层得到的，其中z是线性信号向量，因此我们直接用embedding层得到：</p><p>看上面的公式，我们首先需要知道z和p，这都是由我们的embedding层得到的，其中z是线性信号向量，因此我们直接用embedding层得到：</p> <img src="/2019/07/22/PNN/9.jpg"><p>论文中使用的等号加一个三角形，其实就是相等的意思，可以认为z就是embedding层的复制。</p><p>对于p来说，这里需要一个公式进行映射：</p> <img src="/2019/07/22/PNN/10.jpg"><p>不同的g函数的选择使得我们有了两种PNN的计算方法，一种叫做Inner PNN，简称IPNN，一种叫做Outer PNN，简称OPNN。</p><p>接下来分别来具体介绍这两种形式的PNN模型，由于涉及到复杂度的分析，所以我们这里先定义Embedding的大小为M，field的大小为N，而lz和lp的长度为D1。</p><h2 id="2-1-IPNN"><a href="#2-1-IPNN" class="headerlink" title="2.1 IPNN"></a>2.1 IPNN</h2><p>IPNN中p的计算方式如下，即使用内积来代表pij：</p> <img src="/2019/07/22/PNN/11.jpg"><p>所以，pij其实是一个数（标量），得到一个pij的时间复杂度为M，p的大小为N_N，因此计算得到p的时间复杂度为N_N_M。而再由p得到lp的时间复杂度是N_N_D1。因此 对于IPNN来说，总的时间复杂度为N_N(D1+M)。文章对这一结构进行了优化，可以看到，我们的p是一个对称矩阵，因此我们的权重也是一个对称矩阵，对这个对称矩阵进行如下的分解：</p><p><img src="https://pic2.zhimg.com/80/v2-10fdbd46cde76ec299fbf090d89161b8_hd.png" alt></p><p>因此：</p> <img src="/2019/07/22/PNN/12.jpg"><p>因此：</p> <img src="/2019/07/22/PNN/13.jpg"><h2 id="2-2-OPNN"><a href="#2-2-OPNN" class="headerlink" title="2.2 OPNN"></a>2.2 OPNN</h2><p>OPNN中p的计算方式如下：</p><p><img src="https://pic2.zhimg.com/80/v2-6d6e9313125ef15bfcbefa0b42596f86_hd.png" alt></p><p>此时pij为M_M的矩阵，计算一个pij的时间复杂度为M_M，而p是N_N_M_M的矩阵，因此计算p的事件复杂度为N_N_M_M。从而计算lp的时间复杂度变为D1 _ N_N_M_M。这个显然代价很高的。为了减少复杂度，论文使用了叠加的思想，它重新定义了p矩阵：</p><p>通过元素相乘的叠加，也就是先叠加N个field的Embedding向量，然后做乘法，可以大幅减少时间复杂度，定义p为：</p> <img src="/2019/07/22/PNN/14.jpg"><p>这里计算p的时间复杂度就变为了D1_M_(M+N)。</p><h1 id="3、Discussion"><a href="#3、Discussion" class="headerlink" title="3、Discussion"></a>3、Discussion</h1><p>和FNN相比，PNN多了一个product层，和FM相比，PNN多了隐层，并且输出不是简单的叠加；在训练部分，可以单独训练FNN或者FM部分作为初始化，然后BP算法应用整个网络，那么至少效果不会差于FNN和FM；</p><h1 id="4、Experiments"><a href="#4、Experiments" class="headerlink" title="4、Experiments"></a>4、Experiments</h1><p>使用Criteo和iPinYou的数据集，并用SGD算法比较了7种模型：LR、FM、FNN、CCPM、IPNN、OPNN、PNN（拼接内积和外积层），正则化部分（L2和Dropout）；</p><p>实验结果如下图所示：</p> <img src="/2019/07/22/PNN/15.jpg"><p>结果表明PNN提升还是蛮大的；这里介绍一下关于激活函数的选择问题，作者进行了对比如下：</p> <img src="/2019/07/22/PNN/16.jpg"><p>从图中看出，anh在某些方面要优于relu，但作者采用的是relu，relu的作用： 1、稀疏的激活函数（负数会被丢失）；2、有效的梯度传播（缓解梯度消失和梯度爆炸）；3、有效的计算（仅有加法、乘法、比较操作）。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;今天第二篇，还是之前的经典论文（PNN）还是基于DNN的深度模型用于预测点击率，不过相比于FNN提出了不少新的idea，一起来看下吧。&lt;/p&gt;
&lt;p&gt;原论文：Product-based Neural Networks for User Response Prediction ：2016&lt;/p&gt;
    
    </summary>
    
      <category term="ctr" scheme="https://jesse-csj.github.io/categories/ctr/"/>
    
    
      <category term="推荐系统" scheme="https://jesse-csj.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>（读论文）推荐系统之ctr预估-FNN模型解析</title>
    <link href="https://jesse-csj.github.io/2019/07/21/FNN/"/>
    <id>https://jesse-csj.github.io/2019/07/21/FNN/</id>
    <published>2019-07-21T08:01:40.000Z</published>
    <updated>2019-07-25T06:53:40.753Z</updated>
    
    <content type="html"><![CDATA[<h1 id><a href="#" class="headerlink" title=" "></a> </h1><p>今天要介绍的论文也是之前看到的一篇经典的推荐相关的论文（FNN），最近要快点更新啊，要赶上最新看的进度。</p><p>原论文：Deep learning over multi-field categorical data</p><a id="more"></a><p>地址：<a href="https://arxiv.org/pdf/1601.02376.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1601.02376.pdf</a></p><h1 id="1、问题由来"><a href="#1、问题由来" class="headerlink" title="1、问题由来"></a><strong>1、问题由来</strong></h1><p>　　基于传统机器学习模型（如LR、FM等）的CTR预测方案又被称为基于浅层模型的方案，其优点是模型简单，预测性能较好，可解释性强；缺点主要在于很难自动提取高阶组合特征携带的信息，目前一般通过特征工程来手动的提取高阶组合特征。而随着深度学习在计算机视觉、语音识别、自然语言处理等领域取得巨大成功，其在探索特征间高阶隐含信息的能力也被应用到了CTR预测中。较早有影响力的基于深度学习模型的CTR预测方案是在2016年提出的基于因子分解机的神经网络(Factorization Machine supported Neural Network, FNN)模型，就是我们今天要分享的内容，一起来看下。</p><h1 id="2、模型"><a href="#2、模型" class="headerlink" title="2、模型"></a><strong>2、模型</strong></h1><p> FNN模型如下图所示：</p><img src="/2019/07/21/FNN/1.png"><img src="/2019/07/21/FNN/4.png"><p>（FM的详细解释可看我上一篇<a href="https://zhuanlan.zhihu.com/p/74337279" target="_blank" rel="noopener">文章</a>）：</p><img src="/2019/07/21/FNN/2.png"><p>我们可以看出这个模型有着十分显著的特点：<br>　　1. 采用FM预训练得到的隐含层及其权重作为神经网络的第一层的初始值，之后再不断堆叠全连接层，最终输出预测的点击率。 　　　<br>　　2. 可以将FNN理解成一种特殊的embedding+MLP，其要求第一层嵌入后的各领域特征维度一致，并且嵌入权重的初始化是FM预训练好的。 　　　　<br>　　3. 这不是一个端到端的训练过程，有贪心训练的思路。而且如果不考虑预训练过程，模型网络结构也没有考虑低阶特征组合。</p><p>　  为了方便理解，如下图所示，FNN = FM + MLP <strong>，相当于用FM模型得到了每一维特征的嵌入向量，做了一次特征工程，得到特征送入分类器，不是端到端的思路，有贪心训练的思路。</strong></p><img src="/2019/07/21/FNN/3.png"><h1 id="3、FNN的优缺点"><a href="#3、FNN的优缺点" class="headerlink" title="3、FNN的优缺点"></a><strong>3、FNN的优缺点</strong></h1><p><strong>优点</strong>：每个特征的嵌入向量是预先采用FM模型训练的，因此在学习DNN模型时，训练开销降低，模型能够更快达到收敛。</p><p><strong>缺点：</strong></p><ol><li>Embedding 的参数受 FM 的影响，不一定准确。</li><li>预训练阶段增加了计算复杂度，训练效率低。</li><li>FNN 只能学习到高阶的组合特征；模型中没有对低阶特征建模。</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h1 id&gt;&lt;a href=&quot;#&quot; class=&quot;headerlink&quot; title=&quot; &quot;&gt;&lt;/a&gt; &lt;/h1&gt;&lt;p&gt;今天要介绍的论文也是之前看到的一篇经典的推荐相关的论文（FNN），最近要快点更新啊，要赶上最新看的进度。&lt;/p&gt;
&lt;p&gt;原论文：Deep learning over multi-field categorical data&lt;/p&gt;
    
    </summary>
    
      <category term="ctr" scheme="https://jesse-csj.github.io/categories/ctr/"/>
    
    
      <category term="推荐系统" scheme="https://jesse-csj.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>（读论文）推荐系统之ctr预估-FM算法解析</title>
    <link href="https://jesse-csj.github.io/2019/07/20/%EF%BC%88%E8%AF%BB%E8%AE%BA%E6%96%87%EF%BC%89%E8%AE%A1%E7%AE%97%E5%B9%BF%E5%91%8A%E4%B9%8BCTR%E9%A2%84%E4%BC%B0-FM%E7%AE%97%E6%B3%95%E8%A7%A3%E6%9E%90/"/>
    <id>https://jesse-csj.github.io/2019/07/20/（读论文）计算广告之CTR预估-FM算法解析/</id>
    <published>2019-07-20T08:01:40.000Z</published>
    <updated>2019-07-25T07:04:03.709Z</updated>
    
    <content type="html"><![CDATA[<p> 大家好，我是csj，这是我的第一篇个人博客，以一篇经典的论文FM开始吧：<br>  原文：Factorization Machines</p><a id="more"></a><p>  地址：<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.393.8529&amp;rep=rep1&amp;type=pdf" target="_blank" rel="noopener">http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.393.8529&amp;rep=rep1&amp;type=pdf</a></p><h1 id="一、问题由来"><a href="#一、问题由来" class="headerlink" title="一、问题由来"></a><strong>一、问题由来</strong></h1><p>在计算广告和推荐系统中，CTR预估(click-through rate)是非常重要的一个环节，判断一个商品的是否进行推荐需要根据CTR预估的点击率来进行。传统的逻辑回归模型是一种广义线性模型，非常容易实现大规模实时并行处理，因此在工业界获得了广泛应用，但是线性模型的学习能力有限，不能捕获高阶特征(非线性信息)，而在进行CTR预估时，除了单特征外，往往要对特征进行组合。对于特征组合来说，业界现在通用的做法主要有两大类：FM系列与DNN系列。今天，我们就来分享下FM算法。</p><h1 id="二、为什么需要FM"><a href="#二、为什么需要FM" class="headerlink" title="二、为什么需要FM"></a><strong>二、为什么需要FM</strong></h1><p>1、特征组合是许多机器学习建模过程中遇到的问题，如果对特征直接建模，很有可能会忽略掉特征与特征之间的关联信息，因此，可以通过构建新的交叉特征这一特征组合方式提高模型的效果。</p><p>2、高维的稀疏矩阵是实际工程中常见的问题，并直接会导致计算量过大，特征权值更新缓慢。试想一个10000<em>100的表，每一列都有8种元素，经过one-hot独热编码之后，会产生一个10000</em>800的表。因此表中每行元素只有100个值为1，700个值为0。特征空间急剧变大，以淘宝上的item为例，将item进行one-hot编码以后，样本空间有一个categorical变为了百万维的数值特征，特征空间一下子暴增一百万。所以大厂动不动上亿维度，就是这么来的。</p><p>而FM的优势就在于对这两方面问题的处理。首先是特征组合，通过对两两特征组合，引入交叉项特征，提高模型得分；其次是高维灾难，通过引入隐向量（对参数矩阵进行矩阵分解），完成对特征的参数估计。</p><h1 id="三、原理及求解"><a href="#三、原理及求解" class="headerlink" title="三、原理及求解"></a><strong>三、原理及求解</strong></h1><p>在看FM算法前，我们先回顾一下最常见的线性表达式：<br>                   <img src="/2019/07/20/（读论文）计算广告之CTR预估-FM算法解析/1.png"><br>其中w0 为初始权值，或者理解为偏置项，wi 为每个特征xi 对应的权值。可以看到，这种线性表达式只描述了每个特征与输出的关系。</p><p>FM的表达式如下，可观察到，只是在线性表达式后面加入了新的交叉项特征及对应的权值。</p><p><img src="//jesse-csj.github.io/2019/07/20/（读论文）计算广告之CTR预估-FM算法解析/1473228-20180904202908102-215826983.png" alt></p><p>  <strong>求解过程 ：　</strong></p><p>从上面的式子可以很容易看出，组合部分的特征相关参数共有n(n−1)/2个。但是如第二部分所分析，在数据很稀疏的情况下，满足xi,xj都不为0的情况非常少，这样将导致ωij无法通过训练得出。</p><p>为了求出ωij，我们对每一个特征分量xi引入辅助向量Vi=(vi1,vi2,⋯,vik)。然后，利用vivj^T对ωij进行求解：</p><p><img src="//jesse-csj.github.io/2019/07/20/（读论文）计算广告之CTR预估-FM算法解析/1630213-20190710174305361-1336985179.png" alt></p><p>那么ωij组成的矩阵可以表示为:</p><p><img src="//jesse-csj.github.io/2019/07/20/（读论文）计算广告之CTR预估-FM算法解析/1630213-20190710174342426-2098941787.png" alt></p><p>那么，如何求解vi和vj呢？主要采用了公式：</p><p><img src="//jesse-csj.github.io/2019/07/20/（读论文）计算广告之CTR预估-FM算法解析/1630213-20190710174408535-2047182456.png" alt></p><p>具体推导过程如下：（重要的化简过程）</p><p><img src="//jesse-csj.github.io/2019/07/20/（读论文）计算广告之CTR预估-FM算法解析/1630213-20190710174437368-117236465.png" alt></p><h1 id="四、参数求解"><a href="#四、参数求解" class="headerlink" title="四、参数求解"></a><strong>四、参数求解</strong></h1><p>利用梯度下降法，通过求损失函数对特征（输入项）的导数计算出梯度，从而更新权值。设m为样本个数，θ为权值。</p><img src="/2019/07/20/（读论文）计算广告之CTR预估-FM算法解析/7.png" title="更新参数"><p>每个梯度都可以在O(1)时间内求得，整体的参数更新的时间为O(kn)。</p><p>第一篇博客就到此结束啦~ 之后会继续分享计算广告相关的论文和知识。<br>有关FM或其他推荐模型的小demo可以在我的<a href="https://github.com/Jesse-csj/TensorFlow_Practice" target="_blank" rel="noopener">github</a>上找到，欢迎大家star~</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt; 大家好，我是csj，这是我的第一篇个人博客，以一篇经典的论文FM开始吧：&lt;br&gt;  原文：Factorization Machines&lt;/p&gt;
    
    </summary>
    
      <category term="ctr" scheme="https://jesse-csj.github.io/categories/ctr/"/>
    
    
      <category term="推荐系统" scheme="https://jesse-csj.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
</feed>
