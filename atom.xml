<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Jesse&#39;s Blog</title>
  
  <subtitle>直落夜深花睡去，临风春华便思君。</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://jesse-csj.github.io/"/>
  <updated>2019-07-24T03:13:29.275Z</updated>
  <id>https://jesse-csj.github.io/</id>
  
  <author>
    <name>Jesse_jia</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>（读论文）推荐系统之ctr预估-Wide＆Deep模型解析</title>
    <link href="https://jesse-csj.github.io/2019/07/24/Wide-Deep/"/>
    <id>https://jesse-csj.github.io/2019/07/24/Wide-Deep/</id>
    <published>2019-07-23T23:33:22.000Z</published>
    <updated>2019-07-24T03:13:29.275Z</updated>
    
    <content type="html"><![CDATA[<p>在读了FM和FNN/PNN的论文后，来学习一下16年的一篇Google的论文，文章将传统的LR和DNN组合构成一个wide&amp;deep模型（并行结构），既保留了LR的拟合能力，又具有DNN的泛化能力，并且不需要单独训练模型，可以方便模型的迭代，一起来看下吧。<br>原文：Wide &amp; Deep Learning for Recommender Systems<br>地址： <a href="https://arxiv.org/pdf/1606.07792.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1606.07792.pdf</a></p><h1 id="1、问题由来"><a href="#1、问题由来" class="headerlink" title="1、问题由来"></a><strong>1、问题由来</strong></h1><h2 id="1-1、背景"><a href="#1-1、背景" class="headerlink" title="1.1、背景"></a><strong>1.1、背景</strong></h2><p>本文提出时是针对推荐系统中应用的，当然也可以应用在ctr预估中。<br>首先介绍论文中通篇出现的两个名词：</p><ul><li>memorization（暂且翻译为记忆）：即从历史数据中发现item或者特征之间的相关性。</li><li>generalization（暂且翻译为泛化）：即相关性的传递，发现在历史数据中很少或者没有出现的新的特征组合。</li></ul><p>举个例子来解释下：在人类的认知学习过程中演化过程中，人类的大脑很复杂，它可以记忆(memorize)下每天发生的事情（麻雀可以飞，鸽子可以飞）然后泛化(generalize)这些知识到之前没有看到过的东西（有翅膀的动物都能飞）。<br>但是泛化的规则有时候不是特别的准确，有时候会出错（有翅膀的动物都能飞吗）。这时候就需要记忆(memorization)来修正泛化的规则(generalized rules)，叫做特例（企鹅有翅膀，但是不能飞）。这就是Memorization和Generalization的来由或者说含义。</p><h2 id="1-2、现有模型的问题"><a href="#1-2、现有模型的问题" class="headerlink" title="1.2、现有模型的问题"></a><strong>1.2、现有模型的问题</strong></h2><ul><li><p>线性模型LR简单、快速并且模型具有可解释，有着很好的拟合能力，但是LR模型是线性模型，表达能力有限，泛化能力较弱，需要做好特征工程，尤其需要交叉特征，才能取得一个良好的效果，然而在工业场景中，特征的数量会很多，可能达到成千上万，甚至数十万，这时特征工程就很难做，还不一定能取得更好的效果。 </p></li><li><p>DNN模型不需要做太精细的特征工程，就可以取得很好的效果，DNN可以自动交叉特征，学习到特征之间的相互作用，尤其是可以学到高阶特征交互，具有很好的泛化能力。另外，DNN通过增加embedding层，可以有效的解决稀疏数据特征的问题，防止特征爆炸。推荐系统中的泛化能力是很重要的，可以提高推荐物品的多样性，但是DNN在拟合数据上相比较LR会较弱。 </p></li><li><p>总结一下：</p><ol><li>线性模型无法学习到训练集中未出现的组合特征；</li><li>FM或DNN通过学习embedding vector虽然可以学习到训练集中未出现的组合特征，但是会过度泛化。</li></ol></li></ul><p>为了提高推荐系统的拟合性和泛化性，可以将LR和DNN结合起来，同时增强拟合能力和泛化能力，wide&amp;deep就是将LR和DNN组合起来，wide部分就是LR，deep部分就是DNN，将两者的结果组合进行输出。</p><h1 id="2、模型细节"><a href="#2、模型细节" class="headerlink" title="2、模型细节"></a><strong>2、模型细节</strong></h1><p>再简单介绍下两个名词的实现：<br><strong>Memorization：</strong>之前大规模稀疏输入的处理是：通过线性模型 + 特征交叉。所带来的Memorization以及记忆能力非常有效和可解释。但是Generalization（泛化能力）需要更多的人工特征工程。</p><p><strong>Generalization：</strong>相比之下，DNN几乎不需要特征工程。通过对低纬度的dense embedding进行组合可以学习到更深层次的隐藏特征。但是，缺点是有点over-generalize（过度泛化）。推荐系统中表现为：会给用户推荐不是那么相关的物品，尤其是user-item矩阵比较稀疏并且是high-rank（高秩矩阵）</p><p><strong>两者区别：</strong>Memorization趋向于更加保守，推荐用户之前有过行为的items。相比之下，generalization更加趋向于提高推荐系统的多样性（diversity）。</p><h2 id="2-1、Wide-和-Deep"><a href="#2-1、Wide-和-Deep" class="headerlink" title="2.1、Wide 和 Deep"></a><strong>2.1、Wide 和 Deep</strong></h2><p><strong>Wide &amp; Deep:</strong><br>Wide &amp; Deep包括两部分：线性模型 + DNN部分。结合上面两者的优点，平衡memorization和generalization。<br>原因：综合memorization和generalizatio的优点，服务于推荐系统。在本文的实验中相比于wide-only和deep-only的模型，wide &amp; deep提升显著。下图是模型整体结构：</p><img src="/2019/07/24/Wide-Deep/1.png"><p>可以看出，Wide也是一种特殊的神经网络，他的输入直接和输出相连，属于广义线性模型的范畴。Deep就是指Deep Neural Network，这个很好理解。Wide Linear Model用于memorization；Deep Neural Network用于generalization。<br>左侧是Wide-only，右侧是Deep-only，中间是Wide &amp; Deep。</p><h2 id="2-2、Cross-product-transformation"><a href="#2-2、Cross-product-transformation" class="headerlink" title="2.2、Cross-product transformation"></a><strong>2.2、Cross-product transformation</strong></h2><p>论文Wide中不断提到这样一种变换用来生成组合特征，这里很重要。它的定义如下：</p><img src="/2019/07/24/Wide-Deep/2.png"><p>其中k表示第k个组合特征。i表示输入X的第i维特征。C_ki表示这个第i维度特征是否要参与第k个组合特征的构造。d表示输入X的维度。到底有哪些维度特征要参与构造组合特征，这个是人工设定的（这也就是说需要人工特征工程），在公式中没有体现。</p><p>其实这么一个复杂的公式，就是我们之前一直在说的one-hot之后的组合特征：仅仅在输入样本X中的特征gender=female和特征language=en同时为1，新的组合特征AND(gender=female, language=en)才为1。所以只要把两个特征的值相乘就可以了。<br>（这样Cross-product transformation 可以在二值特征中学习到组合特征，并且为模型增加非线性）</p><h2 id="2-3、The-Wide-Component"><a href="#2-3、The-Wide-Component" class="headerlink" title="2.3、The Wide Component"></a>2.3、<strong>The Wide Component</strong></h2><p>如上面所说Wide Part其实是一个广义的线性模型。使用特征包括：</p><ul><li><p>raw input： 原始特征</p></li><li><p>cross-product transformation ：上面提到的组合特征</p></li></ul><p>用同一个例子来说明：你给model一个query（你想吃的美食），model返回给你一个美食，然后你购买/消费了这个推荐。 也就是说，推荐系统其实要学习的是这样一个条件概率： P(consumption | query, item)。<br>Wide Part可以对一些特例进行memorization。比如AND(query=”fried chicken”, item=”chicken fried rice”)虽然从字符角度来看很接近，但是实际上完全不同的东西，那么Wide就可以记住这个组合是不好的，是一个特例，下次当你再点炸鸡的时候，就不会推荐给你鸡肉炒米饭了。</p><h2 id="2-4、The-Deep-Component"><a href="#2-4、The-Deep-Component" class="headerlink" title="2.4、The Deep Component"></a>2.4、<strong>The Deep Component</strong></h2><p>如模型右边所示：Deep Part通过学习一个低纬度的dense representation（也叫做embedding vector）对于每一个query和item，来<strong>泛化</strong>给你推荐一些字符上看起来不那么相关，但是你可能也是需要的。比如说：你想要炸鸡，Embedding Space中，炸鸡和汉堡很接近，所以也会给你推荐汉堡。</p><p>Embedding vectors被随机初始化，并根据最终的loss来反向训练更新。这些低维度的dense embedding vectors被作为第一个隐藏层的输入。隐藏层的激活函数通常使用ReLU。</p><h1 id="3、模型训练"><a href="#3、模型训练" class="headerlink" title="3、模型训练"></a><strong>3、模型训练</strong></h1><p>训练中原始的稀疏特征，在两个组件中都会用到，比如query=”fried chicken” item=”chicken fried rice”:</p><img src="/2019/07/24/Wide-Deep/2.png"><p>在训练的时候，根据最终的loss计算出gradient，反向传播到Wide和Deep两部分中，分别训练自己的参数。也就是说，<strong>两个模块是一起训练的</strong>（也就是论文中的联合训练），注意这不是模型融合。</p><ul><li><p>Wide部分中的组合特征可以<strong>记住</strong>那些稀疏的，特定的rules</p></li><li><p>Deep部分通过Embedding来<strong>泛化</strong>推荐一些相似的items</p></li></ul><p>Wide模块通过组合特征可以很效率的学习一些特定的组合，但是这也导致了他并不能学习到训练集中没有出现的组合特征。所幸，Deep模块弥补了这个缺点。<br>另外，因为是一起训练的，wide和deep的size都减小了。wide组件只需要填补deep组件的不足就行了，所以需要比较少的cross-product feature transformations，而不是full-size wide Model。<br>具体的训练方法和实验请参考原论文。</p><h1 id="4、总结"><a href="#4、总结" class="headerlink" title="4、总结"></a><strong>4、总结</strong></h1><p>缺点：Wide部分还是需要人工特征工程。<br>优点：实现了对memorization和generalization的统一建模。能同时学习低阶和高阶组合特征</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在读了FM和FNN/PNN的论文后，来学习一下16年的一篇Google的论文，文章将传统的LR和DNN组合构成一个wide&amp;amp;deep模型（并行结构），既保留了LR的拟合能力，又具有DNN的泛化能力，并且不需要单独训练模型，可以方便模型的迭代，一起来看下吧。&lt;br&gt;原
      
    
    </summary>
    
      <category term="ctr" scheme="https://jesse-csj.github.io/categories/ctr/"/>
    
    
      <category term="推荐系统" scheme="https://jesse-csj.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>（读论文）推荐系统之ctr预估-PNN模型解析</title>
    <link href="https://jesse-csj.github.io/2019/07/22/PNN/"/>
    <id>https://jesse-csj.github.io/2019/07/22/PNN/</id>
    <published>2019-07-22T08:01:40.000Z</published>
    <updated>2019-07-23T13:25:04.358Z</updated>
    
    <content type="html"><![CDATA[<p>今天第二篇，还是之前的经典论文（PNN）还是基于DNN的深度模型用于预测点击率，不过相比于FNN提出了不少新的idea，一起来看下吧。</p><p>原论文：Product-based Neural Networks for User Response Prediction ：2016</p><a id="more"></a><p>地址：<a href="https://arxiv.org/pdf/1611.00144.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1611.00144.pdf</a></p><h1 id="1、原理"><a href="#1、原理" class="headerlink" title="1、原理"></a><strong>1、原理</strong></h1><p>给大家举例一个直观的场景：比如现在有一个凤凰网站，网站上面有一个迪斯尼广告，那我们现在想知道用户进入这个网站之后会不会有兴趣点击这个广告，类似这种用户点击率预测在信息检索领域就是一个非常核心的问题。普遍的做法就是通过不同的域来描述这个事件然后预测用户的点击行为，而这个域可以有很多。那么什么样的用户会点击这个广告呢？我们可能猜想：目前在上海的年轻的用户可能会有需求，如果今天是周五，看到这个广告，可能会点击这个广告为周末做活动参考。那可能的特征会是：[Weekday=Friday, occupation=Student, City=Shanghai]，当这些特征同时出现时，我们认为这个用户点击这个迪斯尼广告的概率会比较大。</p><p>传统的做法是应用One-Hot Binary的编码方式去处理这类数据，例如现在有三个域的数据X=[Weekday=Wednesday, Gender=Male, City=Shanghai],其中 Weekday有7个取值，我们就把它编译为7维的二进制向量，其中只有Wednesday是1，其他都是0，因为它只有一个特征值；Gender有两维，其中一维是1；如果有一万个城市的话，那City就有一万维，只有上海这个取值是1，其他是0。</p><p>那最终就会得到一个高维稀疏向量。但是这个数据集不能直接用神经网络训练：如果直接用One-Hot Binary进行编码，那输入特征至少有一百万，第一层至少需要500个节点，那么第一层我们就需要训练5亿个参数，那就需要20亿或是50亿的数据集，而要获得如此大的数据集基本上是很困难的事情。</p><p><strong>回顾FM、FNN模型</strong></p><p>因为上述原因需要将非常大的特征向量嵌入到低维向量空间中来减小模型复杂度，而FM（Factorisation machine）是很有效的embedding model：<br>                              <img src="/2019/07/22/PNN/1.jpg"></p><p>第一部分仍然为Logistic Regression，第二部分是通过两两向量之间的点积来判断特征向量之间和目标变量之间的关系。比如上述的迪斯尼广告，occupation=Student和City=Shanghai这两个向量之间的角度应该小于90，它们之间的点积应该大于0，说明和迪斯尼广告的点击率是正相关的。这种算法在推荐系统领域应用比较广泛。</p><p>那就基于这个模型来考虑神经网络模型，其实这个模型本质上就是一个三层网络：</p><p><img src="https://pic2.zhimg.com/80/v2-b046c93f125596de5265f7c6342542f6_hd.png" alt></p><p>它在第二层对向量做了乘积处理（比如上图蓝色节点直接为两个向量乘积，其连接边上没有参数需要学习），每个field都只会被映射到一个low-dimensional vector，field和field之间没有相互影响，那么第一层就被大量降维，之后就可以在此基础上应用神经网络模型。</p><p>如用FM算法对底层field进行embeddding，在此基础上面建模就是FNN(Factorisation-machinesupported Neural Networks)模型:</p><p><img src="https://pic2.zhimg.com/80/v2-9fb5ca82bca2cd35f3751aca3ca5ac45_hd.png" alt></p><p>那现在进一步考虑FNN与一般的神经网络的区别是什么？大部分的神经网络模型对向量之间的处理都是采用加法操作，而FM 则是通过向量之间的乘法来衡量两者之间的关系。我们知道乘法关系其实相当于逻辑“且”的关系，拿上述例子来说，只有特征是学生而且在上海的人才有更大的概率去点击迪斯尼广告。但是加法仅相当于逻辑中“或”的关系，显然“且”比“或”更能严格区分目标变量。（加法就是正常拼接后作为输出，这里就是先做乘积再拼接作为DNN的输入）</p><p>所以我们接下来的工作就是对乘法关系建模。可以对两个向量做内积和外积的乘法操作，在此基础之上我们搭建的神经网络PNN：提出了一种product layer的思想，既基于乘法的运算来体现特征交叉的DNN网络结构，如下图：</p> <img src="/2019/07/22/PNN/2.jpg"><p>按照论文的思路，从上往下来看这个网络结构：</p><p>输出层 输出层很简单，将上一层的网络输出通过一个全链接层，经过sigmoid函数转换后映射到(0,1)的区间中，得到我们的点击率的预测值：</p><p>​                <img src="/2019/07/22/PNN/3.jpg"><br><strong>l2层</strong></p><p>根据l1层的输出，经一个全链接层 ，并使用relu进行激活，得到我们l2的输出结果：</p> <img src="/2019/07/22/PNN/4.jpg"><p><strong>l1层</strong> </p><p>l1层的输出由如下的公式计算：</p> <img src="/2019/07/22/PNN/5.jpg"><p>可以看到在得到l1层输出时，这里输入了三部分，分别是lz，lp 和 b1，b1是偏置项，这里可以先不管。lz和lp的计算就是PNN的重点所在了。</p><p><strong>Product Layer</strong></p><p>product思想来源于，在ctr预估中，认为特征之间的关系更多是一种and“且”的关系，而非add”或”的关系。例如，性别为男且喜欢游戏的人群，比起性别男和喜欢游戏的人群，前者的组合比后者更能体现特征交叉的意义。</p><p>product layer可以分成两个部分，一部分是线性部分lz，一部分是非线性部分lp。二者的形式如下：</p> <img src="/2019/07/22/PNN/6.jpg"><p>在这里，我们要使用到论文中所定义的一种运算方式，其实就是矩阵的点乘（对应位置相乘然后求和，最终得到的是一个标量）:</p> <img src="/2019/07/22/PNN/7.jpg"><p>这里先继续介绍网络结构，下一章中详细介绍Product Layer。</p><p><strong>Embedding Layer</strong></p><p>Embedding Layer跟DeepFM中相同，将每一个field的特征转换成同样长度的向量，这里用f来表示。</p><p><img src="https://pic2.zhimg.com/80/v2-b46b0d082980847e97b4e4448be2c5ba_hd.png" alt></p><p><strong>损失函数</strong> </p><p>损失函数使用交叉熵：</p> <img src="/2019/07/22/PNN/8.jpg"><h1 id="2、Product-Layer详细介绍"><a href="#2、Product-Layer详细介绍" class="headerlink" title="2、Product Layer详细介绍"></a>2、Product Layer详细介绍</h1><p>前面提到了，product layer可以分成两个部分，一部分是线性部分lz，一部分是非线性部分lp。它们同维度，其具体形式如下：</p><p><img src="https://pic2.zhimg.com/80/v2-37e77e50b3a109ed916b6396046b55c0_hd.png" alt></p><p>其中z,p为信号向量，z为线性信号向量，p为二次信号向量,<br>$$<br>其中W_{z}^{i},W_{p}^{i}为权重矩阵。<br>$$<br>(权重矩阵与z,p同维，经过定义的这种点乘运算后都得到一个标量作为DNN的输入）看上面的公式，我们首先需要知道z和p，这都是由我们的embedding层得到的，其中z是线性信号向量，因此我们直接用embedding层得到：</p><p>看上面的公式，我们首先需要知道z和p，这都是由我们的embedding层得到的，其中z是线性信号向量，因此我们直接用embedding层得到：</p> <img src="/2019/07/22/PNN/9.jpg"><p>论文中使用的等号加一个三角形，其实就是相等的意思，可以认为z就是embedding层的复制。</p><p>对于p来说，这里需要一个公式进行映射：</p> <img src="/2019/07/22/PNN/10.jpg"><p>不同的g函数的选择使得我们有了两种PNN的计算方法，一种叫做Inner PNN，简称IPNN，一种叫做Outer PNN，简称OPNN。</p><p>接下来分别来具体介绍这两种形式的PNN模型，由于涉及到复杂度的分析，所以我们这里先定义Embedding的大小为M，field的大小为N，而lz和lp的长度为D1。</p><h2 id="2-1-IPNN"><a href="#2-1-IPNN" class="headerlink" title="2.1 IPNN"></a>2.1 IPNN</h2><p>IPNN中p的计算方式如下，即使用内积来代表pij：</p> <img src="/2019/07/22/PNN/11.jpg"><p>所以，pij其实是一个数（标量），得到一个pij的时间复杂度为M，p的大小为N_N，因此计算得到p的时间复杂度为N_N_M。而再由p得到lp的时间复杂度是N_N_D1。因此 对于IPNN来说，总的时间复杂度为N_N(D1+M)。文章对这一结构进行了优化，可以看到，我们的p是一个对称矩阵，因此我们的权重也是一个对称矩阵，对这个对称矩阵进行如下的分解：</p><p><img src="https://pic2.zhimg.com/80/v2-10fdbd46cde76ec299fbf090d89161b8_hd.png" alt></p><p>因此：</p> <img src="/2019/07/22/PNN/12.jpg"><p>因此：</p> <img src="/2019/07/22/PNN/13.jpg"><h2 id="2-2-OPNN"><a href="#2-2-OPNN" class="headerlink" title="2.2 OPNN"></a>2.2 OPNN</h2><p>OPNN中p的计算方式如下：</p><p><img src="https://pic2.zhimg.com/80/v2-6d6e9313125ef15bfcbefa0b42596f86_hd.png" alt></p><p>此时pij为M_M的矩阵，计算一个pij的时间复杂度为M_M，而p是N_N_M_M的矩阵，因此计算p的事件复杂度为N_N_M_M。从而计算lp的时间复杂度变为D1 _ N_N_M_M。这个显然代价很高的。为了减少复杂度，论文使用了叠加的思想，它重新定义了p矩阵：</p><p>通过元素相乘的叠加，也就是先叠加N个field的Embedding向量，然后做乘法，可以大幅减少时间复杂度，定义p为：</p> <img src="/2019/07/22/PNN/14.jpg"><p>这里计算p的时间复杂度就变为了D1_M_(M+N)。</p><h1 id="3、Discussion"><a href="#3、Discussion" class="headerlink" title="3、Discussion"></a>3、Discussion</h1><p>和FNN相比，PNN多了一个product层，和FM相比，PNN多了隐层，并且输出不是简单的叠加；在训练部分，可以单独训练FNN或者FM部分作为初始化，然后BP算法应用整个网络，那么至少效果不会差于FNN和FM；</p><h1 id="4、Experiments"><a href="#4、Experiments" class="headerlink" title="4、Experiments"></a>4、Experiments</h1><p>使用Criteo和iPinYou的数据集，并用SGD算法比较了7种模型：LR、FM、FNN、CCPM、IPNN、OPNN、PNN（拼接内积和外积层），正则化部分（L2和Dropout）；</p><p>实验结果如下图所示：</p> <img src="/2019/07/22/PNN/15.jpg"><p>结果表明PNN提升还是蛮大的；这里介绍一下关于激活函数的选择问题，作者进行了对比如下：</p> <img src="/2019/07/22/PNN/16.jpg"><p>从图中看出，anh在某些方面要优于relu，但作者采用的是relu，relu的作用： 1、稀疏的激活函数（负数会被丢失）；2、有效的梯度传播（缓解梯度消失和梯度爆炸）；3、有效的计算（仅有加法、乘法、比较操作）。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;今天第二篇，还是之前的经典论文（PNN）还是基于DNN的深度模型用于预测点击率，不过相比于FNN提出了不少新的idea，一起来看下吧。&lt;/p&gt;
&lt;p&gt;原论文：Product-based Neural Networks for User Response Prediction ：2016&lt;/p&gt;
    
    </summary>
    
      <category term="ctr" scheme="https://jesse-csj.github.io/categories/ctr/"/>
    
    
      <category term="推荐系统" scheme="https://jesse-csj.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>（读论文）推荐系统之ctr预估-FNN模型解析</title>
    <link href="https://jesse-csj.github.io/2019/07/21/FNN/"/>
    <id>https://jesse-csj.github.io/2019/07/21/FNN/</id>
    <published>2019-07-21T08:01:40.000Z</published>
    <updated>2019-07-23T13:19:05.022Z</updated>
    
    <content type="html"><![CDATA[<h1 id><a href="#" class="headerlink" title=" "></a> </h1><p>今天要介绍的论文也是之前看到的一篇经典的推荐相关的论文（FNN），最近要快点更新啊，要赶上最新看的进度。</p><p>原论文：Deep learning over multi-field categorical data</p><a id="more"></a><p>地址：<a href="https://arxiv.org/pdf/1601.02376.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1601.02376.pdf</a></p><h1 id="1、问题由来"><a href="#1、问题由来" class="headerlink" title="1、问题由来"></a><strong>1、问题由来</strong></h1><p>　　基于传统机器学习模型（如LR、FM等）的CTR预测方案又被称为基于浅层模型的方案，其优点是模型简单，预测性能较好，可解释性强；缺点主要在于很难自动提取高阶组合特征携带的信息，目前一般通过特征工程来手动的提取高阶组合特征。而随着深度学习在计算机视觉、语音识别、自然语言处理等领域取得巨大成功，其在探索特征间高阶隐含信息的能力也被应用到了CTR预测中。较早有影响力的基于深度学习模型的CTR预测方案是在2016年提出的基于因子分解机的神经网络(Factorization Machine supported Neural Network, FNN)模型，就是我们今天要分享的内容，一起来看下。</p><h1 id="2、模型"><a href="#2、模型" class="headerlink" title="2、模型"></a><strong>2、模型</strong></h1><p> FNN模型如下图所示：</p><p><img src="https://pic1.zhimg.com/v2-67e9af999f4bfbd54f49b84ebee082c8_b.png" alt="img"></p><p>FNN模型结构</p><p>​      对图中的一些变量进行一下解释：x是输入的特征，它是大规模离散稀疏的。它可以分成N个Field，每一个Field中，只有一个值为1，其余都为0（即one-hot）。Field i的则可以表示成 <img src="https://www.zhihu.com/equation?tex=X%5Bstart_%7Bi%7D%2Cend_%7Bi%7D%5D" alt="X[start_{i},end_{i}]">X[start_{i},end_{i}] , <img src="https://www.zhihu.com/equation?tex=W_%7B0%7D%5E%7Bi%7D" alt="W_{0}^{i}">W_{0}^{i} 为Field i的embedding矩阵。 <img src="https://www.zhihu.com/equation?tex=Z_%7Bi%7D" alt="Z_{i}">Z_{i} 为embedding后的向量。它由一次项 <img src="https://www.zhihu.com/equation?tex=W_%7Bi%7D" alt="W_{i}">W_{i} ，二次项 <img src="https://www.zhihu.com/equation?tex=V_%7Bi%7D%3D(V_%7Bi%7D%5E%7B1%7D%2CV_%7Bi%7D%5E%7B2%7D...V_%7Bi%7D%5E%7Bk%7D%2C)" alt="V_{i}=(V_{i}^{1},V_{i}^{2}...V_{i}^{k},)">V_{i}=(V_{i}^{1},V_{i}^{2}…V_{i}^{k},) 组成，其中K是FM中二次项的向量的维度。而后面的 <img src="https://www.zhihu.com/equation?tex=l_%7B1%7D%2Cl_%7B2%7D%2C" alt="l_{1},l_{2},">l_{1},l_{2}, 则为神经网络的全连接层的表示。</p><p>​      详细解释一下基于FM的预训练：嵌入后的向量 <img src="https://www.zhihu.com/equation?tex=Z_%7Bi%7D" alt="Z_{i}">Z_{i} = <img src="https://www.zhihu.com/equation?tex=(w_%7Bi%7D%2Cv_%7Bi%7D%5E%7B1%7D%2Cv_%7Bi%7D%5E%7B2%7D...v_%7Bi%7D%5E%7Bk%7D%2C)" alt="(w_{i},v_{i}^{1},v_{i}^{2}...v_{i}^{k},)">(w_{i},v_{i}^{1},v_{i}^{2}…v_{i}^{k},) ,其中 <img src="https://www.zhihu.com/equation?tex=w_%7Bi%7D" alt="w_{i}">w_{i} 就是FM里面的一次性系数，而 <img src="https://www.zhihu.com/equation?tex=v_%7Bi%7D%5E%7Bk%7D" alt="v_{i}^{k}">v_{i}^{k} 就是二次项的系数，可以详细对照看FM的公式如下（FM的详细解释可看我上一篇<a href="https://zhuanlan.zhihu.com/p/74337279" target="_blank" rel="noopener">文章</a>）：</p><p><img src="https://pic4.zhimg.com/v2-869f40f8d7d754d66f1bf5a371990dc3_b.png" alt="img"></p><p>基于FM的预训练得到嵌入向量</p><p>　  我们可以看出这个模型有着十分显著的特点：</p><p>　　　　1. 采用FM预训练得到的隐含层及其权重作为神经网络的第一层的初始值，之后再不断堆叠全连接层，最终输出预测的点击率。 　　　　2. 可以将FNN理解成一种特殊的embedding+MLP，其要求第一层嵌入后的各领域特征维度一致，并且嵌入权重的初始化是FM预训练好的。 　　　　3. 这不是一个端到端的训练过程，有贪心训练的思路。而且如果不考虑预训练过程，模型网络结构也没有考虑低阶特征组合。</p><p>　  为了方便理解，如下图所示，FNN = FM + MLP <strong>，相当于用FM模型得到了每一维特征的嵌入向量，做了一次特征工程，得到特征送入分类器，不是端到端的思路，有贪心训练的思路。</strong></p><p><img src="https://pic1.zhimg.com/v2-609c3a22419dfc2e8fce458505d448bc_b.png" alt="img"></p><h1 id="3、FNN的优缺点"><a href="#3、FNN的优缺点" class="headerlink" title="3、FNN的优缺点"></a><strong>3、FNN的优缺点</strong></h1><p><strong>优点</strong>：每个特征的嵌入向量是预先采用FM模型训练的，因此在学习DNN模型时，训练开销降低，模型能够更快达到收敛。</p><p><strong>缺点：</strong></p><ol><li>Embedding 的参数受 FM 的影响，不一定准确</li><li>预训练阶段增加了计算复杂度，训练效率低</li><li>FNN 只能学习到高阶的组合特征；模型中没有对低阶特征建模。</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h1 id&gt;&lt;a href=&quot;#&quot; class=&quot;headerlink&quot; title=&quot; &quot;&gt;&lt;/a&gt; &lt;/h1&gt;&lt;p&gt;今天要介绍的论文也是之前看到的一篇经典的推荐相关的论文（FNN），最近要快点更新啊，要赶上最新看的进度。&lt;/p&gt;
&lt;p&gt;原论文：Deep learning over multi-field categorical data&lt;/p&gt;
    
    </summary>
    
      <category term="ctr" scheme="https://jesse-csj.github.io/categories/ctr/"/>
    
    
      <category term="推荐系统" scheme="https://jesse-csj.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>（读论文）推荐系统之ctr预估-FM算法解析</title>
    <link href="https://jesse-csj.github.io/2019/07/20/%EF%BC%88%E8%AF%BB%E8%AE%BA%E6%96%87%EF%BC%89%E8%AE%A1%E7%AE%97%E5%B9%BF%E5%91%8A%E4%B9%8BCTR%E9%A2%84%E4%BC%B0-FM%E7%AE%97%E6%B3%95%E8%A7%A3%E6%9E%90/"/>
    <id>https://jesse-csj.github.io/2019/07/20/（读论文）计算广告之CTR预估-FM算法解析/</id>
    <published>2019-07-20T08:01:40.000Z</published>
    <updated>2019-07-23T13:35:41.761Z</updated>
    
    <content type="html"><![CDATA[<p> 大家好，我是csj，这是我的第一篇个人博客，以一篇经典的论文FM开始吧：<br>  原文：Factorization Machines</p><a id="more"></a><p>  地址：<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.393.8529&rep=rep1&type=pdf" target="_blank" rel="noopener">http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.393.8529&amp;rep=rep1&amp;type=pdf</a></p><h1 id="一、问题由来"><a href="#一、问题由来" class="headerlink" title="一、问题由来"></a><strong>一、问题由来</strong></h1><p>在计算广告和推荐系统中，CTR预估(click-through rate)是非常重要的一个环节，判断一个商品的是否进行推荐需要根据CTR预估的点击率来进行。传统的逻辑回归模型是一种广义线性模型，非常容易实现大规模实时并行处理，因此在工业界获得了广泛应用，但是线性模型的学习能力有限，不能捕获高阶特征(非线性信息)，而在进行CTR预估时，除了单特征外，往往要对特征进行组合。对于特征组合来说，业界现在通用的做法主要有两大类：FM系列与DNN系列。今天，我们就来分享下FM算法。</p><h1 id="二、为什么需要FM"><a href="#二、为什么需要FM" class="headerlink" title="二、为什么需要FM"></a><strong>二、为什么需要FM</strong></h1><p>1、特征组合是许多机器学习建模过程中遇到的问题，如果对特征直接建模，很有可能会忽略掉特征与特征之间的关联信息，因此，可以通过构建新的交叉特征这一特征组合方式提高模型的效果。</p><p>2、高维的稀疏矩阵是实际工程中常见的问题，并直接会导致计算量过大，特征权值更新缓慢。试想一个10000<em>100的表，每一列都有8种元素，经过one-hot独热编码之后，会产生一个10000</em>800的表。因此表中每行元素只有100个值为1，700个值为0。特征空间急剧变大，以淘宝上的item为例，将item进行one-hot编码以后，样本空间有一个categorical变为了百万维的数值特征，特征空间一下子暴增一百万。所以大厂动不动上亿维度，就是这么来的。</p><p>而FM的优势就在于对这两方面问题的处理。首先是特征组合，通过对两两特征组合，引入交叉项特征，提高模型得分；其次是高维灾难，通过引入隐向量（对参数矩阵进行矩阵分解），完成对特征的参数估计。</p><h1 id="三、原理及求解"><a href="#三、原理及求解" class="headerlink" title="三、原理及求解"></a><strong>三、原理及求解</strong></h1><p>在看FM算法前，我们先回顾一下最常见的线性表达式：<br>                   <img src="/2019/07/20/（读论文）计算广告之CTR预估-FM算法解析/1.png"><br>其中w0 为初始权值，或者理解为偏置项，wi 为每个特征xi 对应的权值。可以看到，这种线性表达式只描述了每个特征与输出的关系。</p><p>FM的表达式如下，可观察到，只是在线性表达式后面加入了新的交叉项特征及对应的权值。</p><p><img src="//jesse-csj.github.io/2019/07/20/（读论文）计算广告之CTR预估-FM算法解析/1473228-20180904202908102-215826983.png" alt></p><p>  *<em>求解过程 ：　*</em></p><p>从上面的式子可以很容易看出，组合部分的特征相关参数共有n(n−1)/2个。但是如第二部分所分析，在数据很稀疏的情况下，满足xi,xj都不为0的情况非常少，这样将导致ωij无法通过训练得出。</p><p>为了求出ωij，我们对每一个特征分量xi引入辅助向量Vi=(vi1,vi2,⋯,vik)。然后，利用vivj^T对ωij进行求解：</p><p><img src="//jesse-csj.github.io/2019/07/20/（读论文）计算广告之CTR预估-FM算法解析/1630213-20190710174305361-1336985179.png" alt></p><p>那么ωij组成的矩阵可以表示为:</p><p><img src="//jesse-csj.github.io/2019/07/20/（读论文）计算广告之CTR预估-FM算法解析/1630213-20190710174342426-2098941787.png" alt></p><p>那么，如何求解vi和vj呢？主要采用了公式：</p><p><img src="//jesse-csj.github.io/2019/07/20/（读论文）计算广告之CTR预估-FM算法解析/1630213-20190710174408535-2047182456.png" alt></p><p>具体推导过程如下：（重要的化简过程）</p><p><img src="//jesse-csj.github.io/2019/07/20/（读论文）计算广告之CTR预估-FM算法解析/1630213-20190710174437368-117236465.png" alt></p><h1 id="四、参数求解"><a href="#四、参数求解" class="headerlink" title="四、参数求解"></a><strong>四、参数求解</strong></h1><p>利用梯度下降法，通过求损失函数对特征（输入项）的导数计算出梯度，从而更新权值。设m为样本个数，θ为权值。</p><img src="/2019/07/20/（读论文）计算广告之CTR预估-FM算法解析/7.png" title="更新参数"><p>每个梯度都可以在O(1)时间内求得，整体的参数更新的时间为O(kn)。</p><p>第一篇博客就到此结束啦~ 之后会继续分享计算广告相关的论文和知识。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt; 大家好，我是csj，这是我的第一篇个人博客，以一篇经典的论文FM开始吧：&lt;br&gt;  原文：Factorization Machines&lt;/p&gt;
    
    </summary>
    
      <category term="ctr" scheme="https://jesse-csj.github.io/categories/ctr/"/>
    
    
      <category term="推荐系统" scheme="https://jesse-csj.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
</feed>
