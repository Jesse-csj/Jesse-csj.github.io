<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Jesse&#39;s Blog</title>
  
  <subtitle>直落夜深花睡去，临风春华便思君。</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://jesse-csj.github.io/"/>
  <updated>2019-07-23T12:14:08.315Z</updated>
  <id>https://jesse-csj.github.io/</id>
  
  <author>
    <name>Jesse_jia</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>（读论文）推荐系统之ctr预估-Wide＆Deep模型解析</title>
    <link href="https://jesse-csj.github.io/2019/07/23/Wide-Deep/"/>
    <id>https://jesse-csj.github.io/2019/07/23/Wide-Deep/</id>
    <published>2019-07-23T11:33:22.000Z</published>
    <updated>2019-07-23T12:14:08.315Z</updated>
    
    <content type="html"><![CDATA[<p>1111<br>测试图片1：</p><img src="/2019/07/23/Wide-Deep/test.jpg" title="This is an example image"><p>测试图片2：<img src="https://pic1.zhimg.com/v2-f35cc17069a9f09660d1690e5de0f500_r.jpg" alt></p><p>测试图片3：</p><p><img src="https://pic4.zhimg.com/v2-1ca29173890655a4c5ea8a75f6127887_b.png" alt></p><p>测试图片4：<br><img src="//jesse-csj.github.io/2019/07/23/Wide-Deep/2.jpg" alt="2"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;1111&lt;br&gt;测试图片1：&lt;/p&gt;
&lt;img src=&quot;/2019/07/23/Wide-Deep/test.jpg&quot; title=&quot;This is an example image&quot;&gt;


&lt;p&gt;测试图片2：&lt;img src=&quot;https://pic1.zhimg.co
      
    
    </summary>
    
      <category term="ctr" scheme="https://jesse-csj.github.io/categories/ctr/"/>
    
    
      <category term="推荐系统" scheme="https://jesse-csj.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>（读论文）推荐系统之ctr预估-PNN模型解析</title>
    <link href="https://jesse-csj.github.io/2019/07/22/PNN/"/>
    <id>https://jesse-csj.github.io/2019/07/22/PNN/</id>
    <published>2019-07-22T08:01:40.000Z</published>
    <updated>2019-07-23T09:38:27.974Z</updated>
    
    <content type="html"><![CDATA[<p>今天第二篇，还是之前的经典论文（PNN）还是基于DNN的深度模型用于预测点击率，不过相比于FNN提出了不少新的idea，一起来看下吧。</p><p>原论文：Product-based Neural Networks for User Response Prediction ：2016</p><a id="more"></a><p>地址：<a href="https://arxiv.org/pdf/1611.00144.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1611.00144.pdf</a></p><h1 id="1、原理"><a href="#1、原理" class="headerlink" title="1、原理"></a><strong>1、原理</strong></h1><p>给大家举例一个直观的场景：比如现在有一个凤凰网站，网站上面有一个迪斯尼广告，那我们现在想知道用户进入这个网站之后会不会有兴趣点击这个广告，类似这种用户点击率预测在信息检索领域就是一个非常核心的问题。普遍的做法就是通过不同的域来描述这个事件然后预测用户的点击行为，而这个域可以有很多。那么什么样的用户会点击这个广告呢？我们可能猜想：目前在上海的年轻的用户可能会有需求，如果今天是周五，看到这个广告，可能会点击这个广告为周末做活动参考。那可能的特征会是：[Weekday=Friday, occupation=Student, City=Shanghai]，当这些特征同时出现时，我们认为这个用户点击这个迪斯尼广告的概率会比较大。</p><p>传统的做法是应用One-Hot Binary的编码方式去处理这类数据，例如现在有三个域的数据X=[Weekday=Wednesday, Gender=Male, City=Shanghai],其中 Weekday有7个取值，我们就把它编译为7维的二进制向量，其中只有Wednesday是1，其他都是0，因为它只有一个特征值；Gender有两维，其中一维是1；如果有一万个城市的话，那City就有一万维，只有上海这个取值是1，其他是0。</p><p>那最终就会得到一个高维稀疏向量。但是这个数据集不能直接用神经网络训练：如果直接用One-Hot Binary进行编码，那输入特征至少有一百万，第一层至少需要500个节点，那么第一层我们就需要训练5亿个参数，那就需要20亿或是50亿的数据集，而要获得如此大的数据集基本上是很困难的事情。</p><p><strong>回顾FM、FNN模型</strong></p><p>因为上述原因需要将非常大的特征向量嵌入到低维向量空间中来减小模型复杂度，而FM（Factorisation machine）是很有效的embedding model：</p><p><img src="//jesse-csj.github.io/2019/07/22/PNN/C:%5CUsers%5C98019%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1563874697101.png" alt="1563874697101"></p><p>第一部分仍然为Logistic Regression，第二部分是通过两两向量之间的点积来判断特征向量之间和目标变量之间的关系。比如上述的迪斯尼广告，occupation=Student和City=Shanghai这两个向量之间的角度应该小于90，它们之间的点积应该大于0，说明和迪斯尼广告的点击率是正相关的。这种算法在推荐系统领域应用比较广泛。</p><p>那就基于这个模型来考虑神经网络模型，其实这个模型本质上就是一个三层网络：</p><p><img src="https://pic2.zhimg.com/80/v2-b046c93f125596de5265f7c6342542f6_hd.png" alt></p><p>它在第二层对向量做了乘积处理（比如上图蓝色节点直接为两个向量乘积，其连接边上没有参数需要学习），每个field都只会被映射到一个low-dimensional vector，field和field之间没有相互影响，那么第一层就被大量降维，之后就可以在此基础上应用神经网络模型。</p><p>如用FM算法对底层field进行embeddding，在此基础上面建模就是FNN(Factorisation-machinesupported Neural Networks)模型:</p><p><img src="https://pic2.zhimg.com/80/v2-9fb5ca82bca2cd35f3751aca3ca5ac45_hd.png" alt></p><p>那现在进一步考虑FNN与一般的神经网络的区别是什么？大部分的神经网络模型对向量之间的处理都是采用加法操作，而FM 则是通过向量之间的乘法来衡量两者之间的关系。我们知道乘法关系其实相当于逻辑“且”的关系，拿上述例子来说，只有特征是学生而且在上海的人才有更大的概率去点击迪斯尼广告。但是加法仅相当于逻辑中“或”的关系，显然“且”比“或”更能严格区分目标变量。（加法就是正常拼接后作为输出，这里就是先做乘积再拼接作为DNN的输入）</p><p>所以我们接下来的工作就是对乘法关系建模。可以对两个向量做内积和外积的乘法操作，在此基础之上我们搭建的神经网络PNN：提出了一种product layer的思想，既基于乘法的运算来体现特征交叉的DNN网络结构，如下图：</p><p><img src="https://pic3.zhimg.com/80/v2-807b29032ca8d6b89924dc5498d21140_hd.png" alt></p><p>按照论文的思路，从上往下来看这个网络结构：</p><p>输出层 输出层很简单，将上一层的网络输出通过一个全链接层，经过sigmoid函数转换后映射到(0,1)的区间中，得到我们的点击率的预测值：</p><p>​                                                <img src="https://pic3.zhimg.com/80/v2-1ca29173890655a4c5ea8a75f6127887_hd.png" alt><br><strong>l2层</strong></p><p>根据l1层的输出，经一个全链接层 ，并使用relu进行激活，得到我们l2的输出结果：</p><p><img src="https://pic1.zhimg.com/80/v2-351a6340a114e7abaa23f24381b717a8_hd.png" alt></p><p><strong>l1层</strong> </p><p>l1层的输出由如下的公式计算：</p><p><img src="https://pic4.zhimg.com/80/v2-c6f73780db40e5d667fc6941e876ca60_hd.png" alt></p><p>可以看到在得到l1层输出时，这里输入了三部分，分别是lz，lp 和 b1，b1是偏置项，这里可以先不管。lz和lp的计算就是PNN的重点所在了。</p><p><strong>Product Layer</strong></p><p>product思想来源于，在ctr预估中，认为特征之间的关系更多是一种and“且”的关系，而非add”或”的关系。例如，性别为男且喜欢游戏的人群，比起性别男和喜欢游戏的人群，前者的组合比后者更能体现特征交叉的意义。</p><p>product layer可以分成两个部分，一部分是线性部分lz，一部分是非线性部分lp。二者的形式如下：</p><p><img src="https://pic4.zhimg.com/80/v2-37e77e50b3a109ed916b6396046b55c0_hd.png" alt></p><p>在这里，我们要使用到论文中所定义的一种运算方式，其实就是矩阵的点乘（对应位置相乘然后求和，最终得到的是一个标量）:</p><p><img src="https://pic3.zhimg.com/80/v2-41eaa2a77669bb8200e20a7c24b7a5dd_hd.png" alt></p><p>这里先继续介绍网络结构，下一章中详细介绍Product Layer。</p><p><strong>Embedding Layer</strong></p><p>Embedding Layer跟DeepFM中相同，将每一个field的特征转换成同样长度的向量，这里用f来表示。</p><p><img src="https://pic2.zhimg.com/80/v2-b46b0d082980847e97b4e4448be2c5ba_hd.png" alt></p><p><strong>损失函数</strong> </p><p>损失函数使用交叉熵：</p><p><img src="https://pic1.zhimg.com/80/v2-8d567d10d305b2e9d8a2e5acc487fa2c_hd.png" alt></p><h1 id="2、Product-Layer详细介绍"><a href="#2、Product-Layer详细介绍" class="headerlink" title="2、Product Layer详细介绍"></a>2、Product Layer详细介绍</h1><p>前面提到了，product layer可以分成两个部分，一部分是线性部分lz，一部分是非线性部分lp。它们同维度，其具体形式如下：</p><p><img src="https://pic2.zhimg.com/80/v2-37e77e50b3a109ed916b6396046b55c0_hd.png" alt></p><p>其中z,p为信号向量，z为线性信号向量，p为二次信号向量， <img src="https://www.zhihu.com/equation?tex=W_%7Bz%7D%5E%7Bi%7D" alt="W\_{z}^{i}">   , <img src="https://www.zhihu.com/equation?tex=W_%7Bp%7D%5E%7Bi%7D" alt="W\_{p}^{i}">   为权重矩阵。（权重矩阵与z,p同维，经过定义的这种点乘运算后都得到一个标量作为DNN的输入）</p><p>看上面的公式，我们首先需要知道z和p，这都是由我们的embedding层得到的，其中z是线性信号向量，因此我们直接用embedding层得到：</p><p><img src="https://pic1.zhimg.com/80/v2-bf743a28a4fb4bbf2d86cc0a91de5f9d_hd.png" alt></p><p>论文中使用的等号加一个三角形，其实就是相等的意思，可以认为z就是embedding层的复制。</p><p>对于p来说，这里需要一个公式进行映射：</p><p><img src="https://pic4.zhimg.com/80/v2-b1fb4fc0989288cc9e18dded4ee1f1a4_hd.png" alt></p><p>不同的g函数的选择使得我们有了两种PNN的计算方法，一种叫做Inner PNN，简称IPNN，一种叫做Outer PNN，简称OPNN。</p><p>接下来分别来具体介绍这两种形式的PNN模型，由于涉及到复杂度的分析，所以我们这里先定义Embedding的大小为M，field的大小为N，而lz和lp的长度为D1。</p><h2 id="2-1-IPNN"><a href="#2-1-IPNN" class="headerlink" title="2.1 IPNN"></a>2.1 IPNN</h2><p>IPNN中p的计算方式如下，即使用内积来代表pij：</p><p><img src="https://pic1.zhimg.com/80/v2-731803947caf6411575279977b1a9841_hd.png" alt></p><p>所以，pij其实是一个数（标量），得到一个pij的时间复杂度为M，p的大小为N_N，因此计算得到p的时间复杂度为N_N_M。而再由p得到lp的时间复杂度是N_N_D1。因此 对于IPNN来说，总的时间复杂度为N_N(D1+M)。文章对这一结构进行了优化，可以看到，我们的p是一个对称矩阵，因此我们的权重也是一个对称矩阵，对这个对称矩阵进行如下的分解：</p><p><img src="https://pic2.zhimg.com/80/v2-10fdbd46cde76ec299fbf090d89161b8_hd.png" alt></p><p>因此：</p><p><img src="https://pic4.zhimg.com/80/v2-efe977966a70061095c3026407bbe0ac_hd.png" alt></p><p>因此：</p><p><img src="https://pic2.zhimg.com/80/v2-f0370a6c5e2562f5c4f3a2afd9dc09ad_hd.png" alt></p><h2 id="2-2-OPNN"><a href="#2-2-OPNN" class="headerlink" title="2.2 OPNN"></a>2.2 OPNN</h2><p>OPNN中p的计算方式如下：</p><p><img src="https://pic2.zhimg.com/80/v2-6d6e9313125ef15bfcbefa0b42596f86_hd.png" alt></p><p>此时pij为M_M的矩阵，计算一个pij的时间复杂度为M_M，而p是N_N_M_M的矩阵，因此计算p的事件复杂度为N_N_M_M。从而计算lp的时间复杂度变为D1 _ N_N_M_M。这个显然代价很高的。为了减少复杂度，论文使用了叠加的思想，它重新定义了p矩阵：</p><p>通过元素相乘的叠加，也就是先叠加N个field的Embedding向量，然后做乘法，可以大幅减少时间复杂度，定义p为：</p><p><img src="https://pic1.zhimg.com/80/v2-8e3fe7702bab617eb1b1b3a40018fb98_hd.png" alt></p><p>这里计算p的时间复杂度就变为了D1_M_(M+N)。</p><h1 id="3、Discussion"><a href="#3、Discussion" class="headerlink" title="3、Discussion"></a>3、Discussion</h1><p>和FNN相比，PNN多了一个product层，和FM相比，PNN多了隐层，并且输出不是简单的叠加；在训练部分，可以单独训练FNN或者FM部分作为初始化，然后BP算法应用整个网络，那么至少效果不会差于FNN和FM；</p><h1 id="4、Experiments"><a href="#4、Experiments" class="headerlink" title="4、Experiments"></a>4、Experiments</h1><p>使用Criteo和iPinYou的数据集，并用SGD算法比较了7种模型：LR、FM、FNN、CCPM、IPNN、OPNN、PNN（拼接内积和外积层），正则化部分（L2和Dropout）；</p><p>实验结果如下图所示：</p><p><img src="https://pic4.zhimg.com/80/v2-8736f30fa55d4afe8bb9624dafc44301_hd.png" alt></p><p>结果表明PNN提升还是蛮大的；这里介绍一下关于激活函数的选择问题，作者进行了对比如下：</p><p><img src="https://pic4.zhimg.com/80/v2-cf8d155b2332e3e287410f7f381e1db3_hd.png" alt></p><p>从图中看出，anh在某些方面要优于relu，但作者采用的是relu，relu的作用： 1、稀疏的激活函数（负数会被丢失）；2、有效的梯度传播（缓解梯度消失和梯度爆炸）；3、有效的计算（仅有加法、乘法、比较操作）.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;今天第二篇，还是之前的经典论文（PNN）还是基于DNN的深度模型用于预测点击率，不过相比于FNN提出了不少新的idea，一起来看下吧。&lt;/p&gt;
&lt;p&gt;原论文：Product-based Neural Networks for User Response Prediction ：2016&lt;/p&gt;
    
    </summary>
    
      <category term="ctr" scheme="https://jesse-csj.github.io/categories/ctr/"/>
    
    
      <category term="推荐系统" scheme="https://jesse-csj.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>（读论文）推荐系统之ctr预估-FNN模型解析</title>
    <link href="https://jesse-csj.github.io/2019/07/21/FNN/"/>
    <id>https://jesse-csj.github.io/2019/07/21/FNN/</id>
    <published>2019-07-21T08:01:40.000Z</published>
    <updated>2019-07-23T08:26:58.468Z</updated>
    
    <content type="html"><![CDATA[<h1 id><a href="#" class="headerlink" title=" "></a> </h1><p>今天要介绍的论文也是之前看到的一篇经典的推荐相关的论文（FNN），最近要快点更新啊，要赶上最新看的进度。</p><p>原论文：Deep learning over multi-field categorical data</p><a id="more"></a><p>地址：<a href="https://arxiv.org/pdf/1601.02376.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1601.02376.pdf</a></p><h1 id="1、问题由来"><a href="#1、问题由来" class="headerlink" title="1、问题由来"></a><strong>1、问题由来</strong></h1><p>　　基于传统机器学习模型（如LR、FM等）的CTR预测方案又被称为基于浅层模型的方案，其优点是模型简单，预测性能较好，可解释性强；缺点主要在于很难自动提取高阶组合特征携带的信息，目前一般通过特征工程来手动的提取高阶组合特征。而随着深度学习在计算机视觉、语音识别、自然语言处理等领域取得巨大成功，其在探索特征间高阶隐含信息的能力也被应用到了CTR预测中。较早有影响力的基于深度学习模型的CTR预测方案是在2016年提出的基于因子分解机的神经网络(Factorization Machine supported Neural Network, FNN)模型，就是我们今天要分享的内容，一起来看下。</p><h1 id="2、模型"><a href="#2、模型" class="headerlink" title="2、模型"></a><strong>2、模型</strong></h1><p> FNN模型如下图所示：</p><p><img src="https://pic1.zhimg.com/v2-67e9af999f4bfbd54f49b84ebee082c8_b.png" alt="img"></p><p>FNN模型结构</p><p>​      对图中的一些变量进行一下解释：x是输入的特征，它是大规模离散稀疏的。它可以分成N个Field，每一个Field中，只有一个值为1，其余都为0（即one-hot）。Field i的则可以表示成 <img src="https://www.zhihu.com/equation?tex=X%5Bstart_%7Bi%7D%2Cend_%7Bi%7D%5D" alt="X[start_{i},end_{i}]">X[start_{i},end_{i}] , <img src="https://www.zhihu.com/equation?tex=W_%7B0%7D%5E%7Bi%7D" alt="W_{0}^{i}">W_{0}^{i} 为Field i的embedding矩阵。 <img src="https://www.zhihu.com/equation?tex=Z_%7Bi%7D" alt="Z_{i}">Z_{i} 为embedding后的向量。它由一次项 <img src="https://www.zhihu.com/equation?tex=W_%7Bi%7D" alt="W_{i}">W_{i} ，二次项 <img src="https://www.zhihu.com/equation?tex=V_%7Bi%7D%3D(V_%7Bi%7D%5E%7B1%7D%2CV_%7Bi%7D%5E%7B2%7D...V_%7Bi%7D%5E%7Bk%7D%2C)" alt="V_{i}=(V_{i}^{1},V_{i}^{2}...V_{i}^{k},)">V_{i}=(V_{i}^{1},V_{i}^{2}…V_{i}^{k},) 组成，其中K是FM中二次项的向量的维度。而后面的 <img src="https://www.zhihu.com/equation?tex=l_%7B1%7D%2Cl_%7B2%7D%2C" alt="l_{1},l_{2},">l_{1},l_{2}, 则为神经网络的全连接层的表示。</p><p>​      详细解释一下基于FM的预训练：嵌入后的向量 <img src="https://www.zhihu.com/equation?tex=Z_%7Bi%7D" alt="Z_{i}">Z_{i} = <img src="https://www.zhihu.com/equation?tex=(w_%7Bi%7D%2Cv_%7Bi%7D%5E%7B1%7D%2Cv_%7Bi%7D%5E%7B2%7D...v_%7Bi%7D%5E%7Bk%7D%2C)" alt="(w_{i},v_{i}^{1},v_{i}^{2}...v_{i}^{k},)">(w_{i},v_{i}^{1},v_{i}^{2}…v_{i}^{k},) ,其中 <img src="https://www.zhihu.com/equation?tex=w_%7Bi%7D" alt="w_{i}">w_{i} 就是FM里面的一次性系数，而 <img src="https://www.zhihu.com/equation?tex=v_%7Bi%7D%5E%7Bk%7D" alt="v_{i}^{k}">v_{i}^{k} 就是二次项的系数，可以详细对照看FM的公式如下（FM的详细解释可看我上一篇<a href="https://zhuanlan.zhihu.com/p/74337279" target="_blank" rel="noopener">文章</a>）：</p><p><img src="https://pic4.zhimg.com/v2-869f40f8d7d754d66f1bf5a371990dc3_b.png" alt="img"></p><p>基于FM的预训练得到嵌入向量</p><p>　  我们可以看出这个模型有着十分显著的特点：</p><p>　　　　1. 采用FM预训练得到的隐含层及其权重作为神经网络的第一层的初始值，之后再不断堆叠全连接层，最终输出预测的点击率。 　　　　2. 可以将FNN理解成一种特殊的embedding+MLP，其要求第一层嵌入后的各领域特征维度一致，并且嵌入权重的初始化是FM预训练好的。 　　　　3. 这不是一个端到端的训练过程，有贪心训练的思路。而且如果不考虑预训练过程，模型网络结构也没有考虑低阶特征组合。</p><p>　  为了方便理解，如下图所示，FNN = FM + MLP <strong>，相当于用FM模型得到了每一维特征的嵌入向量，做了一次特征工程，得到特征送入分类器，不是端到端的思路，有贪心训练的思路。</strong></p><p><img src="https://pic1.zhimg.com/v2-609c3a22419dfc2e8fce458505d448bc_b.png" alt="img"></p><h1 id="3、FNN的优缺点"><a href="#3、FNN的优缺点" class="headerlink" title="3、FNN的优缺点"></a><strong>3、FNN的优缺点</strong></h1><p><strong>优点</strong>：每个特征的嵌入向量是预先采用FM模型训练的，因此在学习DNN模型时，训练开销降低，模型能够更快达到收敛。</p><p><strong>缺点：</strong></p><ol><li>Embedding 的参数受 FM 的影响，不一定准确</li><li>预训练阶段增加了计算复杂度，训练效率低</li><li>FNN 只能学习到高阶的组合特征；模型中没有对低阶特征建模。</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h1 id&gt;&lt;a href=&quot;#&quot; class=&quot;headerlink&quot; title=&quot; &quot;&gt;&lt;/a&gt; &lt;/h1&gt;&lt;p&gt;今天要介绍的论文也是之前看到的一篇经典的推荐相关的论文（FNN），最近要快点更新啊，要赶上最新看的进度。&lt;/p&gt;
&lt;p&gt;原论文：Deep learning over multi-field categorical data&lt;/p&gt;
    
    </summary>
    
      <category term="ctr" scheme="https://jesse-csj.github.io/categories/ctr/"/>
    
    
      <category term="推荐系统" scheme="https://jesse-csj.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>（读论文）推荐系统之ctr预估-FM算法解析</title>
    <link href="https://jesse-csj.github.io/2019/07/20/%EF%BC%88%E8%AF%BB%E8%AE%BA%E6%96%87%EF%BC%89%E8%AE%A1%E7%AE%97%E5%B9%BF%E5%91%8A%E4%B9%8BCTR%E9%A2%84%E4%BC%B0-FM%E7%AE%97%E6%B3%95%E8%A7%A3%E6%9E%90/"/>
    <id>https://jesse-csj.github.io/2019/07/20/（读论文）计算广告之CTR预估-FM算法解析/</id>
    <published>2019-07-20T08:01:40.000Z</published>
    <updated>2019-07-23T12:24:01.108Z</updated>
    
    <content type="html"><![CDATA[<p> 大家好，我是csj，这是我的第一篇个人博客，以一篇经典的论文FM开始吧：<br>  原文：Factorization Machines</p><a id="more"></a><p>  地址：<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.393.8529&rep=rep1&type=pdf" target="_blank" rel="noopener">http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.393.8529&amp;rep=rep1&amp;type=pdf</a></p><h1 id="一、问题由来"><a href="#一、问题由来" class="headerlink" title="一、问题由来"></a><strong>一、问题由来</strong></h1><p>  在计算广告和推荐系统中，CTR预估(click-through rate)是非常重要的一个环节，判断一个商品的是否进行推荐需要根据CTR预估的点击率来进行。传统的逻辑回归模型是一种广义线性模型，非常容易实现大规模实时并行处理，因此在工业界获得了广泛应用，但是线性模型的学习能力有限，不能捕获高阶特征(非线性信息)，而在进行CTR预估时，除了单特征外，往往要对特征进行组合。对于特征组合来说，业界现在通用的做法主要有两大类：FM系列与DNN系列。今天，我们就来分享下FM算法。</p><h1 id="二、为什么需要FM"><a href="#二、为什么需要FM" class="headerlink" title="二、为什么需要FM"></a><strong>二、为什么需要FM</strong></h1><p>  　　　　1、特征组合是许多机器学习建模过程中遇到的问题，如果对特征直接建模，很有可能会忽略掉特征与特征之间的关联信息，因此，可以通过构建新的交叉特征这一特征组合方式提高模型的效果。</p><p>  　　　　2、高维的稀疏矩阵是实际工程中常见的问题，并直接会导致计算量过大，特征权值更新缓慢。试想一个10000<em>100的表，每一列都有8种元素，经过one-hot独热编码之后，会产生一个10000</em>800的表。因此表中每行元素只有100个值为1，700个值为0。特征空间急剧变大，以淘宝上的item为例，将item进行one-hot编码以后，样本空间有一个categorical变为了百万维的数值特征，特征空间一下子暴增一百万。所以大厂动不动上亿维度，就是这么来的。</p><p>  　　　　而FM的优势就在于对这两方面问题的处理。首先是特征组合，通过对两两特征组合，引入交叉项特征，提高模型得分；其次是高维灾难，通过引入隐向量（对参数矩阵进行矩阵分解），完成对特征的参数估计。</p><h1 id="三、原理及求解"><a href="#三、原理及求解" class="headerlink" title="三、原理及求解"></a><strong>三、原理及求解</strong></h1><p>  　　　　在看FM算法前，我们先回顾一下最常见的线性表达式：<br>                   <img src="https://img2018.cnblogs.com/blog/1473228/201809/1473228-20180904202838541-2090182057.png" alt><br>  　　　　其中w0 为初始权值，或者理解为偏置项，wi 为每个特征xi 对应的权值。可以看到，这种线性表达式只描述了每个特征与输出的关系。</p><p>  　　　　FM的表达式如下，可观察到，只是在线性表达式后面加入了新的交叉项特征及对应的权值。</p><p><img src="//jesse-csj.github.io/2019/07/20/（读论文）计算广告之CTR预估-FM算法解析/1473228-20180904202908102-215826983.png" alt></p><p>  *<em>求解过程 ：　*</em></p><p>  　　　　从上面的式子可以很容易看出，组合部分的特征相关参数共有n(n−1)/2个。但是如第二部分所分析，在数据很稀疏的情况下，满足xi,xj都不为0的情况非常少，这样将导致ωij无法通过训练得出。</p><p>  为了求出ωij，我们对每一个特征分量xi引入辅助向量Vi=(vi1,vi2,⋯,vik)。然后，利用vivj^T对ωij进行求解：</p><p><img src="//jesse-csj.github.io/2019/07/20/（读论文）计算广告之CTR预估-FM算法解析/1630213-20190710174305361-1336985179.png" alt></p><p>  　　　　那么ωij组成的矩阵可以表示为:</p><p><img src="//jesse-csj.github.io/2019/07/20/（读论文）计算广告之CTR预估-FM算法解析/1630213-20190710174342426-2098941787.png" alt></p><p>  　　　　那么，如何求解vi和vj呢？主要采用了公式：</p><p><img src="//jesse-csj.github.io/2019/07/20/（读论文）计算广告之CTR预估-FM算法解析/1630213-20190710174408535-2047182456.png" alt></p><p>  　　　　具体推导过程如下：（重要的化简过程）</p><p><img src="//jesse-csj.github.io/2019/07/20/（读论文）计算广告之CTR预估-FM算法解析/1630213-20190710174437368-117236465.png" alt></p><h1 id="四、参数求解"><a href="#四、参数求解" class="headerlink" title="四、参数求解"></a><strong>四、参数求解</strong></h1><p>  　　　　利用梯度下降法，通过求损失函数对特征（输入项）的导数计算出梯度，从而更新权值。设m为样本个数，θ为权值。</p><img src="/2019/07/20/（读论文）计算广告之CTR预估-FM算法解析/7.png" title="This is an example image"><p> 其中，<br>$$<br>\sum_{j=1}^nv_{j,f}x_j<br>$$</p><p>是和i无关的，可以事先求出来。每个梯度都可以在O(1)时间内求得，整体的参数更新的时间为O(kn)。</p><p>  第一篇博客就到此结束啦~ 之后会继续分享计算广告相关的论文和知识。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt; 大家好，我是csj，这是我的第一篇个人博客，以一篇经典的论文FM开始吧：&lt;br&gt;  原文：Factorization Machines&lt;/p&gt;
    
    </summary>
    
      <category term="ctr" scheme="https://jesse-csj.github.io/categories/ctr/"/>
    
    
      <category term="推荐系统" scheme="https://jesse-csj.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
</feed>
