<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[（读论文）推荐系统之ctr预估-DeepFM模型解析]]></title>
    <url>%2F2019%2F07%2F24%2FDeepFM%2F</url>
    <content type="text"><![CDATA[今天第二篇（最近更新的都是Deep模型，传统的线性模型会后面找个时间更新的哈）。本篇介绍华为的DeepFM模型 (2017年)，此模型在 Wide&amp;Deep 的基础上进行改进，成功解决了一些问题，具体的话下面一起来看下吧。 原文：Deepfm: a factorization-machine based neural network for ctr prediction 地址：http://www.ijcai.org/proceedings/2017/0239.pdf ## 1、问题由来### 1.1、背景CTR 预估 数据特点： 输入中包含类别型和连续型数据。类别型数据需要 one-hot, 连续型数据可以先离散化再 one-hot，也可以直接保留原值。 维度非常高。 数据非常稀疏。 特征按照 Field 分组。 CTR 预估 重点 在于 学习组合特征。注意，组合特征包括二阶、三阶甚至更高阶的，阶数越高越复杂，越不容易学习。Google 的论文研究得出结论：高阶和低阶的组合特征都非常重要，同时学习到这两种组合特征的性能要比只考虑其中一种的性能要好。 那么关键问题转化成：如何高效的提取这些组合特征。一种办法就是引入领域知识人工进行特征工程。这样做的弊端是高阶组合特征非常难提取，会耗费极大的人力。而且，有些组合特征是隐藏在数据中的，即使是专家也不一定能提取出来，比如著名的“尿布与啤酒”问题。 在 DeepFM 提出之前，已有 LR（线性或广义线性模型后面更新），FM，FFM（基于因子分解机的特征域），FNN，PNN（以及三种变体：IPNN,OPNN,PNN*）,Wide&amp;Deep 模型，这些模型在 CTR 或者是推荐系统中被广泛使用。 ### 1.2、现有模型的问题线性模型：最开始 CTR 或者是推荐系统领域，一些线性模型取得了不错的效果（线性模型LR简单、快速并且模型具有可解释，有着很好的拟合能力），但是LR模型是线性模型，表达能力有限，泛化能力较弱，需要做好特征工程，尤其需要交叉特征，才能取得一个良好的效果，然而在工业场景中，特征的数量会很多，可能达到成千上万，甚至数十万，这时特征工程就很难做，还不一定能取得更好的效果。 FM模型：线性模型差强人意，直接导致了 FM 模型应运而生（在 Kaggle 上打比赛提出来的，取得了第一名的成绩）。FM 通过隐向量 latent vector 做内积来表示组合特征，从理论上解决了低阶和高阶组合特征提取的问题。但是实际应用中受限于计算复杂度，一般也就只考虑到 2 阶交叉特征。后面又进行了改进，提出了 FFM，增加了 Field 的概念。 遇上深度学习：随着 DNN 在图像、语音、NLP 等领域取得突破，人们渐渐意识到 DNN 在特征表示上的天然优势。相继提出了使用 CNN 或 RNN 来做 CTR 预估的模型。但是，CNN 模型的缺点是：偏向于学习相邻特征的组合特征。 RNN 模型的缺点是：比较适用于有序列 (时序) 关系的数据。 FNN 的提出，应该算是一次非常不错的尝试：先使用预先训练好的 FM，得到隐向量，然后作为 DNN 的输入来训练模型。缺点在于：受限于 FM 预训练的效果。 随后提出了 PNN，PNN 为了捕获高阶组合特征，在embedding layer和first hidden layer之间增加了一个product layer。根据 product layer 使用内积、外积、混合分别衍生出IPNN, OPNN, PNN*三种类型。 但无论是 FNN 还是 PNN，他们都有一个绕不过去的缺点：对于低阶的组合特征，学习到的比较少。 而前面我们说过，低阶特征对于 CTR 也是非常重要的。 Google（上一篇） 意识到了这个问题，为了同时学习低阶和高阶组合特征，提出了 Wide&amp;Deep 模型。它混合了一个 线性模型（Wide part） 和 Deep 模型 (Deep part)。这两部分模型需要不同的输入，而 Wide part 部分的输入，依旧 依赖人工特征工程。 但是，这些模型普遍都存在两个问题： 偏向于提取低阶或者高阶的组合特征。不能同时提取这两种类型的特征。 需要专业的领域知识来做特征工程。 于是DeepFM 应运而生，成功解决了这两个问题，并做了一些改进，其 优点 如下： 不需要预训练 FM 得到隐向量。 不需要人工特征工程。 能同时学习低阶和高阶的组合特征。 FM 模块和 Deep 模块共享 Feature Embedding 部分，可以更快的训练，以及更精确的训练学习。 下面就一直来走进模型的细节。 ## 2、模型细节DeepFM主要做法如下： FM Component + Deep Component。FM 提取低阶组合特征，Deep 提取高阶组合特征。但是和 Wide&amp;Deep 不同的是，DeepFM 是端到端的训练，不需要人工特征工程。 共享 feature embedding。FM 和 Deep 共享输入和feature embedding不但使得训练更快，而且使得训练更加准确。相比之下，Wide&amp;Deep 中，input vector 非常大，里面包含了大量的人工设计的 pairwise 组合特征，增加了它的计算复杂度。 模型整体结构图如下所示： 由上面网络结构图可以看到，DeepFM 包括 FM和 DNN两部分，所以模型最终的输出也由这两部分组成： 下面，把结构图进行拆分，分别来看这两部分。 ### 2.1、The FM ComponentFM 部分的输出由两部分组成：一个 Addition Unit，多个 内积单元。 这里的 d 是输入 one-hot 之后的维度，我们一般称之为feature_size。对应的是 one-hot 之前的特征维度，我们称之为field_size。 架构图如上图所示：Addition Unit 反映的是 1 阶的特征。内积单元 反映的是 2 阶的组合特征对于预测结果的影响。 这里需要注意三点： 这里的wij，也就是&lt;vi,vj&gt;，可以理解为DeepFM结构中计算embedding vector的权矩阵（并非是网上很多文章认为的vi是embedding vector）。 由于输入特征one-hot编码，所以embedding vector也就是输入层到Dense Embeddings层的权重。 Dense Embeddings层的神经元个数是由embedding vector和field_size共同确定，直观来说就是：神经元的个数为embedding vector*field_size。 FM Component 总结： FM 模块实现了对于 1 阶和 2 阶组合特征的建模。 无须预训练。 没有人工特征工程。 embedding 矩阵的大小是：特征数量 * 嵌入维度。然后用一个 index 表示选择了哪个特征。 需要训练的有两部分： input_vector 和 Addition Unit 相连的全连接层，也就是 1 阶的 Embedding 矩阵。 Sparse Feature 到 Dense Embedding 的 Embedding 矩阵，中间也是全连接的，要训练的是中间的权重矩阵，这个权重矩阵也就是隐向量 Vi。 ### 2.2、The Deep Component Deep Component 架构图： 这里DNN的作用是构造高阶组合特征，网络里面黑色的线是全连接层，参数需要神经网络去学习。且有一个特点：DNN的输入也是embedding vector。所谓的权值共享指的就是这里。 关于DNN网络中的输入a处理方式采用前向传播，如下所示： 这里假设α(0)=(e1,e2,…em)表示 embedding层的输出，那么α(0)作为下一层 DNN隐藏层的输入，其前馈过程如下： 优点： 模型可以从最原始的特征中，同时学习低阶和高阶组合特征 不再需要人工特征工程。Wide&amp;Deep 中低阶组合特征就是同过特征工程得到的。 ## 3 、总结（具体的对比实验和实现细节等请参阅原论文）DeepFM优点： 没有用 FM 去预训练隐向量 Vi，并用 Vi去初始化神经网络。（相比之下 FNN 就需要预训练 FM 来初始化 DNN）。 FM 模块不是独立的，是跟整个模型一起训练学习得到的。（相比之下 Wide&amp;Deep 中的 Wide 和 Deep 部分是没有共享的） 不需要特征工程。（相比之下 Wide&amp;Deep 中的 Wide 部分需要特征工程） 训练效率高。（相比 PNN 没有那么多参数） 其中最核心的： 没有预训练（no pre-training） 共享 Feature Embedding，没有特征工程（no feature engineering） 同时学习低阶和高阶组合特征（capture both low-high-order interaction features） 实现DeepFM的一个Demo，感兴趣的童鞋可以关注我的github。]]></content>
      <categories>
        <category>ctr</category>
      </categories>
      <tags>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[（读论文）推荐系统之ctr预估-Wide＆Deep模型解析]]></title>
    <url>%2F2019%2F07%2F24%2FWide-Deep%2F</url>
    <content type="text"><![CDATA[在读了FM和FNN/PNN的论文后，来学习一下16年的一篇Google的论文，文章将传统的LR和DNN组合构成一个wide&amp;deep模型（并行结构），既保留了LR的拟合能力，又具有DNN的泛化能力，并且不需要单独训练模型，可以方便模型的迭代，一起来看下吧。原文：Wide &amp; Deep Learning for Recommender Systems 地址： https://arxiv.org/pdf/1606.07792.pdf 1、问题由来1.1、背景本文提出时是针对推荐系统中应用的，当然也可以应用在ctr预估中。首先介绍论文中通篇出现的两个名词： memorization（暂且翻译为记忆）：即从历史数据中发现item或者特征之间的相关性。 generalization（暂且翻译为泛化）：即相关性的传递，发现在历史数据中很少或者没有出现的新的特征组合。 举个例子来解释下：在人类的认知学习过程中演化过程中，人类的大脑很复杂，它可以记忆(memorize)下每天发生的事情（麻雀可以飞，鸽子可以飞）然后泛化(generalize)这些知识到之前没有看到过的东西（有翅膀的动物都能飞）。但是泛化的规则有时候不是特别的准确，有时候会出错（有翅膀的动物都能飞吗）。这时候就需要记忆(memorization)来修正泛化的规则(generalized rules)，叫做特例（企鹅有翅膀，但是不能飞）。这就是Memorization和Generalization的来由或者说含义。 1.2、现有模型的问题 线性模型LR简单、快速并且模型具有可解释，有着很好的拟合能力，但是LR模型是线性模型，表达能力有限，泛化能力较弱，需要做好特征工程，尤其需要交叉特征，才能取得一个良好的效果，然而在工业场景中，特征的数量会很多，可能达到成千上万，甚至数十万，这时特征工程就很难做，还不一定能取得更好的效果。 DNN模型不需要做太精细的特征工程，就可以取得很好的效果，DNN可以自动交叉特征，学习到特征之间的相互作用，尤其是可以学到高阶特征交互，具有很好的泛化能力。另外，DNN通过增加embedding层，可以有效的解决稀疏数据特征的问题，防止特征爆炸。推荐系统中的泛化能力是很重要的，可以提高推荐物品的多样性，但是DNN在拟合数据上相比较LR会较弱。 总结一下： 线性模型无法学习到训练集中未出现的组合特征； FM或DNN通过学习embedding vector虽然可以学习到训练集中未出现的组合特征，但是会过度泛化。 为了提高推荐系统的拟合性和泛化性，可以将LR和DNN结合起来，同时增强拟合能力和泛化能力，wide&amp;deep就是将LR和DNN组合起来，wide部分就是LR，deep部分就是DNN，将两者的结果组合进行输出。 2、模型细节再简单介绍下两个名词的实现：Memorization：之前大规模稀疏输入的处理是：通过线性模型 + 特征交叉。所带来的Memorization以及记忆能力非常有效和可解释。但是Generalization（泛化能力）需要更多的人工特征工程。 Generalization：相比之下，DNN几乎不需要特征工程。通过对低纬度的dense embedding进行组合可以学习到更深层次的隐藏特征。但是，缺点是有点over-generalize（过度泛化）。推荐系统中表现为：会给用户推荐不是那么相关的物品，尤其是user-item矩阵比较稀疏并且是high-rank（高秩矩阵） 两者区别：Memorization趋向于更加保守，推荐用户之前有过行为的items。相比之下，generalization更加趋向于提高推荐系统的多样性（diversity）。 2.1、Wide 和 DeepWide &amp; Deep:Wide &amp; Deep包括两部分：线性模型 + DNN部分。结合上面两者的优点，平衡memorization和generalization。原因：综合memorization和generalizatio的优点，服务于推荐系统。在本文的实验中相比于wide-only和deep-only的模型，wide &amp; deep提升显著。下图是模型整体结构： 可以看出，Wide也是一种特殊的神经网络，他的输入直接和输出相连，属于广义线性模型的范畴。Deep就是指Deep Neural Network，这个很好理解。Wide Linear Model用于memorization；Deep Neural Network用于generalization。左侧是Wide-only，右侧是Deep-only，中间是Wide &amp; Deep。 2.2、Cross-product transformation论文Wide中不断提到这样一种变换用来生成组合特征，这里很重要。它的定义如下： 其中k表示第k个组合特征。i表示输入X的第i维特征。C_ki表示这个第i维度特征是否要参与第k个组合特征的构造。d表示输入X的维度。到底有哪些维度特征要参与构造组合特征，这个是人工设定的（这也就是说需要人工特征工程），在公式中没有体现。 其实这么一个复杂的公式，就是我们之前一直在说的one-hot之后的组合特征：仅仅在输入样本X中的特征gender=female和特征language=en同时为1，新的组合特征AND(gender=female, language=en)才为1。所以只要把两个特征的值相乘就可以了。（这样Cross-product transformation 可以在二值特征中学习到组合特征，并且为模型增加非线性） 2.3、The Wide Component如上面所说Wide Part其实是一个广义的线性模型。使用特征包括： raw input： 原始特征 cross-product transformation ：上面提到的组合特征 用同一个例子来说明：你给model一个query（你想吃的美食），model返回给你一个美食，然后你购买/消费了这个推荐。 也就是说，推荐系统其实要学习的是这样一个条件概率： P(consumption | query, item)。Wide Part可以对一些特例进行memorization。比如AND(query=”fried chicken”, item=”chicken fried rice”)虽然从字符角度来看很接近，但是实际上完全不同的东西，那么Wide就可以记住这个组合是不好的，是一个特例，下次当你再点炸鸡的时候，就不会推荐给你鸡肉炒米饭了。 2.4、The Deep Component如模型右边所示：Deep Part通过学习一个低纬度的dense representation（也叫做embedding vector）对于每一个query和item，来泛化给你推荐一些字符上看起来不那么相关，但是你可能也是需要的。比如说：你想要炸鸡，Embedding Space中，炸鸡和汉堡很接近，所以也会给你推荐汉堡。 Embedding vectors被随机初始化，并根据最终的loss来反向训练更新。这些低维度的dense embedding vectors被作为第一个隐藏层的输入。隐藏层的激活函数通常使用ReLU。 3、模型训练训练中原始的稀疏特征，在两个组件中都会用到，比如query=”fried chicken” item=”chicken fried rice”: 在训练的时候，根据最终的loss计算出gradient，反向传播到Wide和Deep两部分中，分别训练自己的参数。也就是说，两个模块是一起训练的（也就是论文中的联合训练），注意这不是模型融合。 Wide部分中的组合特征可以记住那些稀疏的，特定的rules Deep部分通过Embedding来泛化推荐一些相似的items Wide模块通过组合特征可以很效率的学习一些特定的组合，但是这也导致了他并不能学习到训练集中没有出现的组合特征。所幸，Deep模块弥补了这个缺点。另外，因为是一起训练的，wide和deep的size都减小了。wide组件只需要填补deep组件的不足就行了，所以需要比较少的cross-product feature transformations，而不是full-size wide Model。具体的训练方法和实验请参考原论文。 4、总结缺点：Wide部分还是需要人工特征工程。优点：实现了对memorization和generalization的统一建模。能同时学习低阶和高阶组合特征]]></content>
      <categories>
        <category>ctr</category>
      </categories>
      <tags>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[（读论文）推荐系统之ctr预估-PNN模型解析]]></title>
    <url>%2F2019%2F07%2F22%2FPNN%2F</url>
    <content type="text"><![CDATA[今天第二篇，还是之前的经典论文（PNN）还是基于DNN的深度模型用于预测点击率，不过相比于FNN提出了不少新的idea，一起来看下吧。 原论文：Product-based Neural Networks for User Response Prediction ：2016 地址：https://arxiv.org/pdf/1611.00144.pdf 1、原理给大家举例一个直观的场景：比如现在有一个凤凰网站，网站上面有一个迪斯尼广告，那我们现在想知道用户进入这个网站之后会不会有兴趣点击这个广告，类似这种用户点击率预测在信息检索领域就是一个非常核心的问题。普遍的做法就是通过不同的域来描述这个事件然后预测用户的点击行为，而这个域可以有很多。那么什么样的用户会点击这个广告呢？我们可能猜想：目前在上海的年轻的用户可能会有需求，如果今天是周五，看到这个广告，可能会点击这个广告为周末做活动参考。那可能的特征会是：[Weekday=Friday, occupation=Student, City=Shanghai]，当这些特征同时出现时，我们认为这个用户点击这个迪斯尼广告的概率会比较大。 传统的做法是应用One-Hot Binary的编码方式去处理这类数据，例如现在有三个域的数据X=[Weekday=Wednesday, Gender=Male, City=Shanghai],其中 Weekday有7个取值，我们就把它编译为7维的二进制向量，其中只有Wednesday是1，其他都是0，因为它只有一个特征值；Gender有两维，其中一维是1；如果有一万个城市的话，那City就有一万维，只有上海这个取值是1，其他是0。 那最终就会得到一个高维稀疏向量。但是这个数据集不能直接用神经网络训练：如果直接用One-Hot Binary进行编码，那输入特征至少有一百万，第一层至少需要500个节点，那么第一层我们就需要训练5亿个参数，那就需要20亿或是50亿的数据集，而要获得如此大的数据集基本上是很困难的事情。 回顾FM、FNN模型 因为上述原因需要将非常大的特征向量嵌入到低维向量空间中来减小模型复杂度，而FM（Factorisation machine）是很有效的embedding model： 第一部分仍然为Logistic Regression，第二部分是通过两两向量之间的点积来判断特征向量之间和目标变量之间的关系。比如上述的迪斯尼广告，occupation=Student和City=Shanghai这两个向量之间的角度应该小于90，它们之间的点积应该大于0，说明和迪斯尼广告的点击率是正相关的。这种算法在推荐系统领域应用比较广泛。 那就基于这个模型来考虑神经网络模型，其实这个模型本质上就是一个三层网络： 它在第二层对向量做了乘积处理（比如上图蓝色节点直接为两个向量乘积，其连接边上没有参数需要学习），每个field都只会被映射到一个low-dimensional vector，field和field之间没有相互影响，那么第一层就被大量降维，之后就可以在此基础上应用神经网络模型。 如用FM算法对底层field进行embeddding，在此基础上面建模就是FNN(Factorisation-machinesupported Neural Networks)模型: 那现在进一步考虑FNN与一般的神经网络的区别是什么？大部分的神经网络模型对向量之间的处理都是采用加法操作，而FM 则是通过向量之间的乘法来衡量两者之间的关系。我们知道乘法关系其实相当于逻辑“且”的关系，拿上述例子来说，只有特征是学生而且在上海的人才有更大的概率去点击迪斯尼广告。但是加法仅相当于逻辑中“或”的关系，显然“且”比“或”更能严格区分目标变量。（加法就是正常拼接后作为输出，这里就是先做乘积再拼接作为DNN的输入） 所以我们接下来的工作就是对乘法关系建模。可以对两个向量做内积和外积的乘法操作，在此基础之上我们搭建的神经网络PNN：提出了一种product layer的思想，既基于乘法的运算来体现特征交叉的DNN网络结构，如下图： 按照论文的思路，从上往下来看这个网络结构： 输出层 输出层很简单，将上一层的网络输出通过一个全链接层，经过sigmoid函数转换后映射到(0,1)的区间中，得到我们的点击率的预测值： ​ l2层 根据l1层的输出，经一个全链接层 ，并使用relu进行激活，得到我们l2的输出结果： l1层 l1层的输出由如下的公式计算： 可以看到在得到l1层输出时，这里输入了三部分，分别是lz，lp 和 b1，b1是偏置项，这里可以先不管。lz和lp的计算就是PNN的重点所在了。 Product Layer product思想来源于，在ctr预估中，认为特征之间的关系更多是一种and“且”的关系，而非add”或”的关系。例如，性别为男且喜欢游戏的人群，比起性别男和喜欢游戏的人群，前者的组合比后者更能体现特征交叉的意义。 product layer可以分成两个部分，一部分是线性部分lz，一部分是非线性部分lp。二者的形式如下： 在这里，我们要使用到论文中所定义的一种运算方式，其实就是矩阵的点乘（对应位置相乘然后求和，最终得到的是一个标量）: 这里先继续介绍网络结构，下一章中详细介绍Product Layer。 Embedding Layer Embedding Layer跟DeepFM中相同，将每一个field的特征转换成同样长度的向量，这里用f来表示。 损失函数 损失函数使用交叉熵： 2、Product Layer详细介绍前面提到了，product layer可以分成两个部分，一部分是线性部分lz，一部分是非线性部分lp。它们同维度，其具体形式如下： 其中z,p为信号向量，z为线性信号向量，p为二次信号向量,$$其中W_{z}^{i},W_{p}^{i}为权重矩阵。$$(权重矩阵与z,p同维，经过定义的这种点乘运算后都得到一个标量作为DNN的输入）看上面的公式，我们首先需要知道z和p，这都是由我们的embedding层得到的，其中z是线性信号向量，因此我们直接用embedding层得到： 看上面的公式，我们首先需要知道z和p，这都是由我们的embedding层得到的，其中z是线性信号向量，因此我们直接用embedding层得到： 论文中使用的等号加一个三角形，其实就是相等的意思，可以认为z就是embedding层的复制。 对于p来说，这里需要一个公式进行映射： 不同的g函数的选择使得我们有了两种PNN的计算方法，一种叫做Inner PNN，简称IPNN，一种叫做Outer PNN，简称OPNN。 接下来分别来具体介绍这两种形式的PNN模型，由于涉及到复杂度的分析，所以我们这里先定义Embedding的大小为M，field的大小为N，而lz和lp的长度为D1。 2.1 IPNNIPNN中p的计算方式如下，即使用内积来代表pij： 所以，pij其实是一个数（标量），得到一个pij的时间复杂度为M，p的大小为N_N，因此计算得到p的时间复杂度为N_N_M。而再由p得到lp的时间复杂度是N_N_D1。因此 对于IPNN来说，总的时间复杂度为N_N(D1+M)。文章对这一结构进行了优化，可以看到，我们的p是一个对称矩阵，因此我们的权重也是一个对称矩阵，对这个对称矩阵进行如下的分解： 因此： 因此： 2.2 OPNNOPNN中p的计算方式如下： 此时pij为M_M的矩阵，计算一个pij的时间复杂度为M_M，而p是N_N_M_M的矩阵，因此计算p的事件复杂度为N_N_M_M。从而计算lp的时间复杂度变为D1 _ N_N_M_M。这个显然代价很高的。为了减少复杂度，论文使用了叠加的思想，它重新定义了p矩阵： 通过元素相乘的叠加，也就是先叠加N个field的Embedding向量，然后做乘法，可以大幅减少时间复杂度，定义p为： 这里计算p的时间复杂度就变为了D1_M_(M+N)。 3、Discussion和FNN相比，PNN多了一个product层，和FM相比，PNN多了隐层，并且输出不是简单的叠加；在训练部分，可以单独训练FNN或者FM部分作为初始化，然后BP算法应用整个网络，那么至少效果不会差于FNN和FM； 4、Experiments使用Criteo和iPinYou的数据集，并用SGD算法比较了7种模型：LR、FM、FNN、CCPM、IPNN、OPNN、PNN（拼接内积和外积层），正则化部分（L2和Dropout）； 实验结果如下图所示： 结果表明PNN提升还是蛮大的；这里介绍一下关于激活函数的选择问题，作者进行了对比如下： 从图中看出，anh在某些方面要优于relu，但作者采用的是relu，relu的作用： 1、稀疏的激活函数（负数会被丢失）；2、有效的梯度传播（缓解梯度消失和梯度爆炸）；3、有效的计算（仅有加法、乘法、比较操作）。]]></content>
      <categories>
        <category>ctr</category>
      </categories>
      <tags>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[（读论文）推荐系统之ctr预估-FNN模型解析]]></title>
    <url>%2F2019%2F07%2F21%2FFNN%2F</url>
    <content type="text"><![CDATA[今天要介绍的论文也是之前看到的一篇经典的推荐相关的论文（FNN），最近要快点更新啊，要赶上最新看的进度。 原论文：Deep learning over multi-field categorical data 地址：https://arxiv.org/pdf/1601.02376.pdf 1、问题由来 基于传统机器学习模型（如LR、FM等）的CTR预测方案又被称为基于浅层模型的方案，其优点是模型简单，预测性能较好，可解释性强；缺点主要在于很难自动提取高阶组合特征携带的信息，目前一般通过特征工程来手动的提取高阶组合特征。而随着深度学习在计算机视觉、语音识别、自然语言处理等领域取得巨大成功，其在探索特征间高阶隐含信息的能力也被应用到了CTR预测中。较早有影响力的基于深度学习模型的CTR预测方案是在2016年提出的基于因子分解机的神经网络(Factorization Machine supported Neural Network, FNN)模型，就是我们今天要分享的内容，一起来看下。 2、模型 FNN模型如下图所示： （FM的详细解释可看我上一篇文章）： 我们可以看出这个模型有着十分显著的特点： 1. 采用FM预训练得到的隐含层及其权重作为神经网络的第一层的初始值，之后再不断堆叠全连接层，最终输出预测的点击率。 2. 可以将FNN理解成一种特殊的embedding+MLP，其要求第一层嵌入后的各领域特征维度一致，并且嵌入权重的初始化是FM预训练好的。 3. 这不是一个端到端的训练过程，有贪心训练的思路。而且如果不考虑预训练过程，模型网络结构也没有考虑低阶特征组合。 为了方便理解，如下图所示，FNN = FM + MLP ，相当于用FM模型得到了每一维特征的嵌入向量，做了一次特征工程，得到特征送入分类器，不是端到端的思路，有贪心训练的思路。 3、FNN的优缺点优点：每个特征的嵌入向量是预先采用FM模型训练的，因此在学习DNN模型时，训练开销降低，模型能够更快达到收敛。 缺点： Embedding 的参数受 FM 的影响，不一定准确。 预训练阶段增加了计算复杂度，训练效率低。 FNN 只能学习到高阶的组合特征；模型中没有对低阶特征建模。]]></content>
      <categories>
        <category>ctr</category>
      </categories>
      <tags>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[（读论文）推荐系统之ctr预估-FM算法解析]]></title>
    <url>%2F2019%2F07%2F20%2F%EF%BC%88%E8%AF%BB%E8%AE%BA%E6%96%87%EF%BC%89%E8%AE%A1%E7%AE%97%E5%B9%BF%E5%91%8A%E4%B9%8BCTR%E9%A2%84%E4%BC%B0-FM%E7%AE%97%E6%B3%95%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[大家好，我是csj，这是我的第一篇个人博客，以一篇经典的论文FM开始吧： 原文：Factorization Machines 地址：http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.393.8529&amp;rep=rep1&amp;type=pdf 一、问题由来在计算广告和推荐系统中，CTR预估(click-through rate)是非常重要的一个环节，判断一个商品的是否进行推荐需要根据CTR预估的点击率来进行。传统的逻辑回归模型是一种广义线性模型，非常容易实现大规模实时并行处理，因此在工业界获得了广泛应用，但是线性模型的学习能力有限，不能捕获高阶特征(非线性信息)，而在进行CTR预估时，除了单特征外，往往要对特征进行组合。对于特征组合来说，业界现在通用的做法主要有两大类：FM系列与DNN系列。今天，我们就来分享下FM算法。 二、为什么需要FM1、特征组合是许多机器学习建模过程中遇到的问题，如果对特征直接建模，很有可能会忽略掉特征与特征之间的关联信息，因此，可以通过构建新的交叉特征这一特征组合方式提高模型的效果。 2、高维的稀疏矩阵是实际工程中常见的问题，并直接会导致计算量过大，特征权值更新缓慢。试想一个10000100的表，每一列都有8种元素，经过one-hot独热编码之后，会产生一个10000800的表。因此表中每行元素只有100个值为1，700个值为0。特征空间急剧变大，以淘宝上的item为例，将item进行one-hot编码以后，样本空间有一个categorical变为了百万维的数值特征，特征空间一下子暴增一百万。所以大厂动不动上亿维度，就是这么来的。 而FM的优势就在于对这两方面问题的处理。首先是特征组合，通过对两两特征组合，引入交叉项特征，提高模型得分；其次是高维灾难，通过引入隐向量（对参数矩阵进行矩阵分解），完成对特征的参数估计。 三、原理及求解在看FM算法前，我们先回顾一下最常见的线性表达式： 其中w0 为初始权值，或者理解为偏置项，wi 为每个特征xi 对应的权值。可以看到，这种线性表达式只描述了每个特征与输出的关系。 FM的表达式如下，可观察到，只是在线性表达式后面加入了新的交叉项特征及对应的权值。 *求解过程 ： * 从上面的式子可以很容易看出，组合部分的特征相关参数共有n(n−1)/2个。但是如第二部分所分析，在数据很稀疏的情况下，满足xi,xj都不为0的情况非常少，这样将导致ωij无法通过训练得出。 为了求出ωij，我们对每一个特征分量xi引入辅助向量Vi=(vi1,vi2,⋯,vik)。然后，利用vivj^T对ωij进行求解： 那么ωij组成的矩阵可以表示为: 那么，如何求解vi和vj呢？主要采用了公式： 具体推导过程如下：（重要的化简过程） 四、参数求解利用梯度下降法，通过求损失函数对特征（输入项）的导数计算出梯度，从而更新权值。设m为样本个数，θ为权值。 每个梯度都可以在O(1)时间内求得，整体的参数更新的时间为O(kn)。 第一篇博客就到此结束啦~ 之后会继续分享计算广告相关的论文和知识。有关FM或其他推荐模型的小demo可以在我的github上找到，欢迎大家star~]]></content>
      <categories>
        <category>ctr</category>
      </categories>
      <tags>
        <tag>推荐系统</tag>
      </tags>
  </entry>
</search>
